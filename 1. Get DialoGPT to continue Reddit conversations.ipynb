{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce9e3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Can money buy happiness?\n",
      "input text: Can money buy happiness?<|endoftext|>\n",
      "input token ids: tensor([[ 6090,  1637,  2822, 12157,    30, 50256]])\n",
      "bot input ids: tensor([[ 6090,  1637,  2822, 12157,    30, 50256]])\n",
      "History + DialoGPT: Can money buy happiness?<|endoftext|>Money can buy happiness.<|endoftext|>\n",
      "DialoGPT: Money can buy happiness.\n",
      ">> User:How so?\n",
      "input text: How so?<|endoftext|>\n",
      "input token ids: tensor([[ 2437,   523,    30, 50256]])\n",
      "bot input ids: tensor([[ 6090,  1637,  2822, 12157,    30, 50256, 26788,   460,  2822, 12157,\n",
      "           764, 50256,  2437,   523,    30, 50256]])\n",
      "History + DialoGPT: Can money buy happiness?<|endoftext|>Money can buy happiness.<|endoftext|>How so?<|endoftext|>This is the most depressing thing I've read all day.<|endoftext|>\n",
      "DialoGPT: This is the most depressing thing I've read all day.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4876/2147850829.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mnew_user_input_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User:\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mnew_user_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_user_input_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input text:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_user_input_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_text = input(\">> User:\") + tokenizer.eos_token\n",
    "    new_user_input_ids = tokenizer.encode(new_user_input_text, return_tensors='pt')\n",
    "    print('input text:', new_user_input_text)\n",
    "    print('input token ids:', new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    print('bot input ids:', bot_input_ids)\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"History + DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:,:][0], skip_special_tokens=False)))\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e18f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53675d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6d20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_df = pd.read_csv('data/reddit_conversations/sample.csv', header=None)\n",
    "conv_df.columns = ['id', 'context', 'response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e06b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i thought the reddit admins promised no more ads with auto play sounds ? ? ?<|endoftext|>you're supposed to block ads .<|endoftext|>not for sites you like .<|endoftext|>yes , all sites . everywhere .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [72, 1807, 262, 18374, 44563, 8072, 645, 517, 9011, 351, 8295, 711, 5238, 5633, 5633, 5633, 50256, 5832, 821, 4385, 284, 2512, 9011, 764, 50256, 1662, 329, 5043, 345, 588, 764, 50256, 8505, 837, 477, 5043, 764, 8347, 764], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def preprocess_text(\n",
    "    txt,\n",
    "    dataset_eos_token = 'EOS',\n",
    "    tokenizer_eos_token = '<|endoftext|>'\n",
    "):\n",
    "    # remove \"title : \" prefixes\n",
    "    if txt[:8] == 'title : ':\n",
    "        txt = txt[8:]\n",
    "    \n",
    "    txt = str(txt).lower()\n",
    "    \n",
    "    # url and tag\n",
    "    words = []\n",
    "    for word in txt.split():\n",
    "        if word[0] == '#': # don't allow tag\n",
    "            continue\n",
    "        i = word.lower().find('http')\n",
    "        if i >= 0:\n",
    "            word = word[:i] + ' ' + '__url__'\n",
    "        words.append(word.strip())\n",
    "    txt = ' '.join(words)\n",
    "\n",
    "    # remove illegal char\n",
    "    txt = txt.replace(chr(92),'') # chr(92) = '\\'. as twitter has 'b\\/c' rather than 'b/c'\n",
    "    txt = txt.replace(\"b/c\",\"because\").replace('j/k','just kidding').replace('w/o','without').replace('w/','with')\n",
    "    txt = re.sub('__mention__','MENTION',txt)\n",
    "    txt = re.sub('__url__','URL',txt)\n",
    "    txt = re.sub(r\"[^A-Za-z0-9()\\[\\]:,.!?'“” ]\", \" \", txt)\n",
    "    txt = re.sub('MENTION','__mention__',txt)\n",
    "    txt = re.sub('URL','__url__',txt)\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=True)\n",
    "    txt = ' ' + ' '.join(tokenizer.tokenize(txt)) + ' '\n",
    "    \n",
    "    # remove un-necessary space\n",
    "    txt = ' '.join(txt.split())\n",
    "    \n",
    "    # replace 'EOS' with tokenizer's EOS\n",
    "    txt_utterance_split = txt.split(dataset_eos_token.lower())\n",
    "    txt = tokenizer_eos_token.join([s.strip() for s in txt_utterance_split])\n",
    "    \n",
    "    return txt\n",
    "\n",
    "\n",
    "\n",
    "prepro = preprocess_text(\"title : I thought the Reddit Admins Promised no more ads with auto play sounds ? ? ? EOS You're supposed to block ads . EOS Not for sites you like . EOS Yes , all sites . Everywhere .\")\n",
    "print(prepro)\n",
    "tokenizer(prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf73d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import DialogLMDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collate_fns import DialogCollate, DialogCollateExperimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66bccd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row: 0 of 252{'src': [72, 1807, 262, 18374, 44563, 8072, 645, 517, 9011, 351, 8295, 711, 5238, 5633, 5633, 5633, 50256, 5832, 821, 4385, 284, 2512, 9011, 764, 50256, 1662, 329, 5043, 345, 588, 764, 50256, 8505, 837, 477, 5043, 764, 8347, 764], 'tgt': [568, 345, 2138, 1414, 329, 262, 5043, 788, 4379, 9011, 5633]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"i thought the reddit admins promised no more ads with auto play sounds???<|endoftext|>you're supposed to block ads.<|endoftext|>not for sites you like.<|endoftext|>yes, all sites. everywhere.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train sets\n",
    "dataset_train = DialogLMDataset(\n",
    "    df = conv_df,\n",
    "    src_col = 'context',\n",
    "    src_eos_token = ' EOS ',\n",
    "    tokenizer = tokenizer,\n",
    "    tgt_col = 'response',\n",
    ")\n",
    "print(dataset_train[-1])\n",
    "\n",
    "tokenizer.decode(dataset_train[-1]['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab80847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      " tensor([  404,   259,   507,   319,   262,   285,  1415,   290,   308,    18,\n",
      "        36237,  5633, 50256,   270,  2936, 12178,    88,   290, 21873,   837,\n",
      "          290,  1312,   550,  5876,  9008,  6670,   326,   338,   780,   262,\n",
      "          936,   519, 46733, 22523,   837,   340,   338,  1464, 12178,    88,\n",
      "          290, 21873, 50256,  1169,   936,   519,   318,  9623,   319,   749,\n",
      "         3777,   764,  1312,   460, 16465,   680,   661,   379, 26160,  2837,\n",
      "          351,   281,   895,    70,   351,   340,   764,   655,   407,   351,\n",
      "          262,   285,  1415,   837,   329,   617,  1738,   764, 50256])\n",
      "position_ids:\n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77,  0])\n",
      "token_type_ids:\n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "target_ids:\n",
      " tensor([  404,   259,   507,   319,   262,   285,  1415,   290,   308,    18,\n",
      "        36237,  5633, 50256,   270,  2936, 12178,    88,   290, 21873,   837,\n",
      "          290,  1312,   550,  5876,  9008,  6670,   326,   338,   780,   262,\n",
      "          936,   519, 46733, 22523,   837,   340,   338,  1464, 12178,    88,\n",
      "          290, 21873, 50256,  1169,   936,   519,   318,  9623,   319,   749,\n",
      "         3777,   764,  1312,   460, 16465,   680,   661,   379, 26160,  2837,\n",
      "          351,   281,   895,    70,   351,   340,   764,   655,   407,   351,\n",
      "          262,   285,  1415,   837,   329,   617,  1738,   764, 50256])\n",
      "attention_masks:\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"opinions on the m14 and g3 optics?<|endoftext|>it felt floaty and inaccurate, and i had trouble hitting targets that's because the acog fuckin sucks, it's always floaty and inaccurate<|endoftext|>the acog is fantastic on most weapons. i can demolish people at sniper range with an smg with it. just not with the m14, for some reason.<|endoftext|>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collate object\n",
    "collate_fn = DialogCollateExperimental(\n",
    "    tokenizer = tokenizer,\n",
    "    max_len = 512,\n",
    "    _targets_ignore_index = -100,\n",
    "    _pad_token_id = tokenizer.eos_token_id  # 0\n",
    ")\n",
    "\n",
    "# dataloader\n",
    "loader_val = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size = 1,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "# example usage\n",
    "for i, batch in enumerate(loader_val):\n",
    "    if i == 3:\n",
    "        break\n",
    "\n",
    "example_id = 0\n",
    "print('input_ids:\\n',       batch['input_ids'][example_id])\n",
    "print('position_ids:\\n',    batch['position_ids'][example_id])\n",
    "print('token_type_ids:\\n',  batch['token_type_ids'][example_id])\n",
    "print('target_ids:\\n',      batch['target_ids'][example_id])\n",
    "print('attention_masks:\\n', batch['attention_masks'][example_id])\n",
    "\n",
    "tokenizer.decode(batch['input_ids'][example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "76ef8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 99 of 100"
     ]
    }
   ],
   "source": [
    "# generate responses to each example in training set\n",
    "\n",
    "# perhaps it's most convenient to have the random cutoffs during the dataset instantiation \n",
    "# and do a few runs through the dataset to generate random stuff\n",
    "\n",
    "n_samples_to_generate = 100\n",
    "n_valid_samples_so_far = 0\n",
    "samples = {\n",
    "    'context': [],\n",
    "    'response': []\n",
    "}\n",
    "index_of_example = 0\n",
    "epoch = 0  # how many times have we made a pass through the data?\n",
    "while n_valid_samples_so_far < n_samples_to_generate:\n",
    "    \n",
    "    print(f'\\rProcessing sample {n_valid_samples_so_far} of {n_samples_to_generate}', end='', flush=True)\n",
    "    \n",
    "    # grab the example\n",
    "    context = conv_df.iloc[index_of_example]['context'] + ' EOS ' + conv_df.iloc[index_of_example]['response']\n",
    "    context = preprocess_text(context)\n",
    "    context_split_utterances = [s.strip() for s in context.split('<|endoftext|>')]\n",
    "    \n",
    "    # randomly cut off conversation at one of the turns\n",
    "    cutoff_point = random.choice(range(len(context_split_utterances)+1))\n",
    "    context_split_utterances = context_split_utterances[:cutoff_point]\n",
    "    if len(context_split_utterances) == 0:  # do unconditional generation\n",
    "        context = ''\n",
    "        input_ids = None\n",
    "        len_input_ids = 0\n",
    "    else:\n",
    "        context = '<|endoftext|>'.join(context_split_utterances) + '<|endoftext|>'\n",
    "        input_ids = tokenizer(context, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        len_input_ids = input_ids.shape[-1]\n",
    "    \n",
    "    # model continues conversation\n",
    "    outputs = model.generate(\n",
    "        inputs = input_ids, \n",
    "        max_length = 1000,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        do_sample = True,  # do_sample = True; otherwise all questions will be identical\n",
    "        top_p = 0.95,      # nucleus sampling\n",
    "        top_k = 0          # deactivate top-k words sampling\n",
    "    ).cpu()\n",
    "    output = outputs[0]\n",
    "    output_bot_only = output[len_input_ids:]\n",
    "    output_bot_only_decoded = tokenizer.decode(output_bot_only, skip_special_tokens=True)\n",
    "    \n",
    "    # validate; skip if invalid\n",
    "    output_preproc = preprocess_text(output_bot_only_decoded).strip()\n",
    "    if len(output_preproc) == 0:  # put validation condition here\n",
    "        continue\n",
    "    \n",
    "    # reformat and save\n",
    "    if context[-13:] == '<|endoftext|>':\n",
    "        context = context[:-13]\n",
    "    context_reformat = context.split('<|endoftext|>')\n",
    "    context_reformat = ' EOS '.join(context_reformat)\n",
    "    response_reformat = output_preproc.strip('<|endoftext|>')\n",
    "    samples['context'].append(context_reformat)\n",
    "    samples['response'].append(response_reformat)\n",
    "    \n",
    "    # dump every 1000 samples\n",
    "    if n_valid_samples_so_far % 1000 == 0:  # dump every 1000 prompts\n",
    "        print(f'Saving {len(samples['context'])} samples to csv')\n",
    "        pd.DataFrame(samples).to_csv(f'red_lm_prompts/zero_shot/prompts_{int(n_valid_samples_so_far/1000)-1}000_{int(n_valid_samples_so_far/1000)}000.csv', index=False)\n",
    "        samples = {}\n",
    "    \n",
    "    # increment example\n",
    "    if index_of_example == len(conv_df)-1:\n",
    "        epoch += 1\n",
    "    index_of_example = (index_of_example + 1) % len(conv_df)\n",
    "    n_valid_samples_so_far += 1\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_deep_learning",
   "language": "python",
   "name": "advanced_deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
