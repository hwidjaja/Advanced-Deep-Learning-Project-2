{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KrNfVNueNhR"
   },
   "source": [
    "#Make your own Rick Sanchez (bot) with Transformers and DialoGPT fine-tuning\n",
    "\n",
    "by [Rostyslav Neskorozhenyi](https://www.linkedin.com/in/slanj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78afhsngLZMw"
   },
   "source": [
    "I am a big fan of 'Rick and Morty' and NLP (Natural Language Processing). After watching the last episode of a mad scientist's and his grandson's adventures I realized that it's not a coincidence and I have to apply my knowledge in NLP and Machine Learning to create my own Rick (in a form of chat-bot, at least for now).\n",
    "\n",
    "![alt text](https://media.giphy.com/media/ZdBnIqBeVWBYTobH8U/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)\n",
    "\n",
    "The path to creating virtual Rick was not easy. I tried many models and approaches but virtual Rick did not want to find a common language with me for a long time. Rick's first response to my greeting was \"I am not your buddy\" (to tell you the truth, in the beginning he knew only this phrase). But I did not give up and with the help of excellent Transformers library and Microsoft's Large-scale Pretrained Response Generation Model (DialoGPT) I managed to talk heart-to-heart with Rick. Here is one of our conversations:\n",
    "\n",
    "**User**:Hey, Rick, make me a time machine <br>\n",
    "**RickBot**: You're a monster! <br>\n",
    "**User**:No, I just want to travel in time <br>\n",
    "**RickBot**: To the past. <br>\n",
    "**User**:yes <br>\n",
    "**RickBot**: No time machine, please. <br>\n",
    "**User**:But I need it <br>\n",
    "**RickBot**: You don't need it, Morty. Just do it. <br>\n",
    "**User**:I want to see antient Greece <br>\n",
    "**RickBot**: Oh, geez. <br>\n",
    "\n",
    "![alt text](https://media.giphy.com/media/WVGSfyhUYBoiGLbZ7N/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGJHOZYOWZHR"
   },
   "source": [
    "## A bit of theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBd8PVGNWgfK"
   },
   "source": [
    "In this article I will tell you how to create a virtual character whose statements will be based on a transcript of my favorite animated science fiction sitcom. You can use characters and sitcoms of your choice. I added code to convert a regular text file with dialogs into a format that the model understands.\n",
    "\n",
    "As I already mentioned, [Transformers](https://huggingface.co/transformers/) library, which contains the latest NLP models (such as [BERT](https://huggingface.co/transformers/model_doc/bert.html), [XLNet](https://huggingface.co/transformers/model_doc/xlnet.html), [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)) will help us in our task. You can read more about transformers in a beautifully illustrated [article](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png) Image from[ http://jalammar.github.io](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "Not so long ago, Microsoftâ€™s [DialoGPT](https://huggingface.co/transformers/model_doc/dialogpt.html) was added to the Transformers model collection. DialoGPT is a GPT-2 model, trained on 147M multi-turn dialogue from Reddit discussion thread (you can learn more about GPT-2 [here](http://jalammar.github.io/illustrated-gpt2/)). This model is ideally suited for creating a virtual character for a fascinating conversation and even in the small implementation option it can maintain a coherent dialogue, which we will see now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hyTyW7Le2Sb"
   },
   "source": [
    "## First dialogue with DialoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEqpXb4ofFGd"
   },
   "source": [
    "We will conduct all our experiments in Google Colab, its resources are enough to train the small DialoGPT model. Firstly, we will connect to Google Drive and install the necessary modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVSTlysV2jaM"
   },
   "source": [
    "Try to chat with DialoGPT without fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "95ed0f145f494915b5870739603e7085",
      "6819f91aad5b4930ac1b43a74a57d852",
      "bbcfb9b4822e4dc9b9238bd8fe98bebf",
      "a6a5fdd6d4024dfc876270eb6a3d17e9",
      "2f18e981fde04466b609e54f50abff48",
      "07e561e746054cd59ef9b6c99c4c0b76",
      "a9798066c3d14283aee0876deb309ae6",
      "1431b3f013104ab99ae7f9a5a576a487",
      "a055d6391b5e4c9cb7fe3a88482d7d55",
      "fb73201b47dc446ca66cdefa01b81f7f",
      "3d8165ae581c4b1dacd1f7f55c7cab77",
      "aa29d0cf3f834e7c85527584958f8b0c",
      "f6957ad8de954698ada26fd880ccd711",
      "c6b866565b99437c80f887584b21f0d7",
      "88cc0f856ed54dffbed94cc2d22fab1f",
      "33d3beeaedeb469a92403db541551342",
      "27bbbc46cfad4733b7ede6b9242e73d2",
      "992dde81bdad49b9a03abb43a22caf3f",
      "27953c96d08846f9a919115a2d85006e",
      "c0292a6aba0e483a8388311f249ab35f",
      "317bc13466ac499ea0c696a153188e96",
      "c4021fb5d92c4211b369d5a5fe44d624",
      "167f7f54e2384c3d863bfff043dd3dec",
      "fb46b6aa2a6f4533a1f79a419ed17f14",
      "6bb27f2bf6cd4463b47bf92338a24c67",
      "b0bf7a14389241fe8dd662ad2650d2ef",
      "acfb2630da884e03809a47c581e5ea86",
      "7442f2a6fd734b88a67e7da5003f3e0a",
      "16b59273a5cb45ea92b2406eda686f2b",
      "b6777b193dc6464687b4b6975f72cc71",
      "928b31a5d3c041feab1f1abfbe83d386",
      "ea1abfd0d2c841499355edb67589b69d"
     ]
    },
    "id": "w6qrl7_SvPKg",
    "outputId": "6cfb1f7b-2346-48a2-ecf8-dc1c0e3b0e2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:878: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "CjZaN5ilgd-z",
    "outputId": "99190011-bb40-431c-f352-22a9b445453e"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14784/672516580.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnew_user_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User:\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# append the new user input tokens to the chat history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens    \n",
    "    chat_history_ids = model.generate(\n",
    "    bot_input_ids, max_length=1000,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True\n",
    "    )\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vbzm0BHOitsa"
   },
   "source": [
    "**User:**Hi Rick <br> \n",
    "**DialoGPT:** Hi Rick <br>\n",
    "**User:**How are you? <br>\n",
    "**DialoGPT:** I'm good, how are you? <br>\n",
    "**User:**I am fine. Where is Morty? <br>\n",
    "**DialoGPT:** He's in the basement. <br>\n",
    "**User:**Who is Morty? <br>\n",
    "**DialoGPT:** He's a Morty. <br>\n",
    "**User:**Who are you? <br>\n",
    "**DialoGPT:** I am a Morty. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAy2OBMW6QyD"
   },
   "source": [
    "![alt text](https://media.giphy.com/media/L3WevKXIKFDaZBvV8Q/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)\n",
    "\n",
    "Not bad but not too impressive. We will fix it with fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuzSROqxjUKM"
   },
   "source": [
    "## Model initial configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC3qNlfp30aU"
   },
   "source": [
    "Let's train our own Rick chatbot. For start, we will need basic configuration and a dataset.\n",
    "Configuration and training scripts are mostly based on this [script](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) from Huggingface and great [tutorial](https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html) from Nathan Cooper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g91QzdqU2haO"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    # AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "utprDGf06OVt"
   },
   "outputs": [],
   "source": [
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output-small'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 1\n",
    "        self.per_gpu_eval_batch_size = 1\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 0\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_qYqlTe9yx2"
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo718Zb-961e"
   },
   "source": [
    "Our dialogues dataset will be based on a dataset used in Andrada Olteanu's [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/) about Rick and Morty sentiment analysis. Big thanks to her work and also to Gabriel Hernandes, author of original [text dataset](https://github.com/ghhernandes/rickmorty-gan/tree/master/data)!\n",
    "\n",
    "![alt text](https://media.giphy.com/media/U6LOakQja88ImTnE6T/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)\n",
    "\n",
    "First of all we will use kaggle module to download needed dataset. You can read in more detail about module and how to get Kaggle API Token by this [link](https://github.com/Kaggle/kaggle-api). Or you can just download RickAndMortyScripts.csv file from [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/) and place this file in your working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>season no.</th>\n",
       "      <th>episode no.</th>\n",
       "      <th>episode name</th>\n",
       "      <th>name</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Rick</td>\n",
       "      <td>Morty! You gotta come on. Jus'... you gotta co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Morty</td>\n",
       "      <td>What, Rick? Whatâ€™s going on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Rick</td>\n",
       "      <td>I got a surprise for you, Morty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Morty</td>\n",
       "      <td>It's the middle of the night. What are you tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Rick</td>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Morty</td>\n",
       "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Rick</td>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Rick</td>\n",
       "      <td>What do you think of this... flying vehicle, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Morty</td>\n",
       "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Rick</td>\n",
       "      <td>Morty. I had to... I had to do it. I hadâ€” I ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  season no.  episode no. episode name   name  \\\n",
       "0      0           1            1        Pilot   Rick   \n",
       "1      1           1            1        Pilot  Morty   \n",
       "2      2           1            1        Pilot   Rick   \n",
       "3      3           1            1        Pilot  Morty   \n",
       "4      4           1            1        Pilot   Rick   \n",
       "5      5           1            1        Pilot  Morty   \n",
       "6      6           1            1        Pilot   Rick   \n",
       "7      7           1            1        Pilot   Rick   \n",
       "8      8           1            1        Pilot  Morty   \n",
       "9      9           1            1        Pilot   Rick   \n",
       "\n",
       "                                                line  \n",
       "0  Morty! You gotta come on. Jus'... you gotta co...  \n",
       "1                       What, Rick? Whatâ€™s going on?  \n",
       "2                   I got a surprise for you, Morty.  \n",
       "3  It's the middle of the night. What are you tal...  \n",
       "4  Come on, I got a surprise for you.  Come on, h...  \n",
       "5                Ow! Ow! You're tugging me too hard!  \n",
       "6  We gotta go, gotta get outta here, come on. Go...  \n",
       "7  What do you think of this... flying vehicle, M...  \n",
       "8  Yeah, Rick... I-it's great. Is this the surprise?  \n",
       "9  Morty. I had to... I had to do it. I hadâ€” I ha...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at original dataset\n",
    "all_rick = pd.read_csv('data/rick_and_morty_conversations/RickAndMortyScripts.csv')\n",
    "all_rick.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n51hsl2mAG8v"
   },
   "source": [
    "We will convert this dataset in a way that every responce row will contain **n** previous responces as a context. For our purposes seven previous responces will be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WyL7Dx1bbLRC"
   },
   "outputs": [],
   "source": [
    "# # Let's look at original dataset\n",
    "# all_rick = pd.read_csv('data/rick_and_morty_conversations/RickAndMortyScripts.csv')\n",
    "# all_rick.head(10)\n",
    "\n",
    "# # with data leakage\n",
    "# contexted = []\n",
    "# n = 7\n",
    "# for i in range(n, len(all_rick['line'])):\n",
    "#     row = []\n",
    "#     prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n",
    "#     for j in range(i, prev, -1):\n",
    "#         row.append(all_rick['line'][j])\n",
    "#     contexted.append(row)\n",
    "    \n",
    "# columns = ['response', 'context'] \n",
    "# columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "# df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "# trn_df, val_df = train_test_split(df, test_size = 0.1, random_state=42)\n",
    "# trn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why? Morty,  I defeat gagoos more powerful tha...</td>\n",
       "      <td>Rick, this really bums me out. It-It's embarra...</td>\n",
       "      <td>Uh, the adventure is the favor, Morty. Me slee...</td>\n",
       "      <td>Rick, since it's my adventure and all, could y...</td>\n",
       "      <td>Jesus... How awesome is that? I mean, they wan...</td>\n",
       "      <td>I... think the personality conflict might have...</td>\n",
       "      <td>Don't worry, Morty, they love you. Superheroes...</td>\n",
       "      <td>This article says the reason we weren't involv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah, but not heroes.</td>\n",
       "      <td>Why? Morty,  I defeat gagoos more powerful tha...</td>\n",
       "      <td>Rick, this really bums me out. It-It's embarra...</td>\n",
       "      <td>Uh, the adventure is the favor, Morty. Me slee...</td>\n",
       "      <td>Rick, since it's my adventure and all, could y...</td>\n",
       "      <td>Jesus... How awesome is that? I mean, they wan...</td>\n",
       "      <td>I... think the personality conflict might have...</td>\n",
       "      <td>Don't worry, Morty, they love you. Superheroes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh, please. They just call themselves heroes s...</td>\n",
       "      <td>Yeah, but not heroes.</td>\n",
       "      <td>Why? Morty,  I defeat gagoos more powerful tha...</td>\n",
       "      <td>Rick, this really bums me out. It-It's embarra...</td>\n",
       "      <td>Uh, the adventure is the favor, Morty. Me slee...</td>\n",
       "      <td>Rick, since it's my adventure and all, could y...</td>\n",
       "      <td>Jesus... How awesome is that? I mean, they wan...</td>\n",
       "      <td>I... think the personality conflict might have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm calling them that, Rick! They're my heroes...</td>\n",
       "      <td>Oh, please. They just call themselves heroes s...</td>\n",
       "      <td>Yeah, but not heroes.</td>\n",
       "      <td>Why? Morty,  I defeat gagoos more powerful tha...</td>\n",
       "      <td>Rick, this really bums me out. It-It's embarra...</td>\n",
       "      <td>Uh, the adventure is the favor, Morty. Me slee...</td>\n",
       "      <td>Rick, since it's my adventure and all, could y...</td>\n",
       "      <td>Jesus... How awesome is that? I mean, they wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Huh... no accounting for taste. I'm gonna go g...</td>\n",
       "      <td>I'm calling them that, Rick! They're my heroes...</td>\n",
       "      <td>Oh, please. They just call themselves heroes s...</td>\n",
       "      <td>Yeah, but not heroes.</td>\n",
       "      <td>Why? Morty,  I defeat gagoos more powerful tha...</td>\n",
       "      <td>Rick, this really bums me out. It-It's embarra...</td>\n",
       "      <td>Uh, the adventure is the favor, Morty. Me slee...</td>\n",
       "      <td>Rick, since it's my adventure and all, could y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  Why? Morty,  I defeat gagoos more powerful tha...   \n",
       "1                              Yeah, but not heroes.   \n",
       "2  Oh, please. They just call themselves heroes s...   \n",
       "3  I'm calling them that, Rick! They're my heroes...   \n",
       "4  Huh... no accounting for taste. I'm gonna go g...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Rick, this really bums me out. It-It's embarra...   \n",
       "1  Why? Morty,  I defeat gagoos more powerful tha...   \n",
       "2                              Yeah, but not heroes.   \n",
       "3  Oh, please. They just call themselves heroes s...   \n",
       "4  I'm calling them that, Rick! They're my heroes...   \n",
       "\n",
       "                                           context/0  \\\n",
       "0  Uh, the adventure is the favor, Morty. Me slee...   \n",
       "1  Rick, this really bums me out. It-It's embarra...   \n",
       "2  Why? Morty,  I defeat gagoos more powerful tha...   \n",
       "3                              Yeah, but not heroes.   \n",
       "4  Oh, please. They just call themselves heroes s...   \n",
       "\n",
       "                                           context/1  \\\n",
       "0  Rick, since it's my adventure and all, could y...   \n",
       "1  Uh, the adventure is the favor, Morty. Me slee...   \n",
       "2  Rick, this really bums me out. It-It's embarra...   \n",
       "3  Why? Morty,  I defeat gagoos more powerful tha...   \n",
       "4                              Yeah, but not heroes.   \n",
       "\n",
       "                                           context/2  \\\n",
       "0  Jesus... How awesome is that? I mean, they wan...   \n",
       "1  Rick, since it's my adventure and all, could y...   \n",
       "2  Uh, the adventure is the favor, Morty. Me slee...   \n",
       "3  Rick, this really bums me out. It-It's embarra...   \n",
       "4  Why? Morty,  I defeat gagoos more powerful tha...   \n",
       "\n",
       "                                           context/3  \\\n",
       "0  I... think the personality conflict might have...   \n",
       "1  Jesus... How awesome is that? I mean, they wan...   \n",
       "2  Rick, since it's my adventure and all, could y...   \n",
       "3  Uh, the adventure is the favor, Morty. Me slee...   \n",
       "4  Rick, this really bums me out. It-It's embarra...   \n",
       "\n",
       "                                           context/4  \\\n",
       "0  Don't worry, Morty, they love you. Superheroes...   \n",
       "1  I... think the personality conflict might have...   \n",
       "2  Jesus... How awesome is that? I mean, they wan...   \n",
       "3  Rick, since it's my adventure and all, could y...   \n",
       "4  Uh, the adventure is the favor, Morty. Me slee...   \n",
       "\n",
       "                                           context/5  \n",
       "0  This article says the reason we weren't involv...  \n",
       "1  Don't worry, Morty, they love you. Superheroes...  \n",
       "2  I... think the personality conflict might have...  \n",
       "3  Jesus... How awesome is that? I mean, they wan...  \n",
       "4  Rick, since it's my adventure and all, could y...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no data leakage\n",
    "\n",
    "# read data\n",
    "all_rick = pd.read_csv('data/rick_and_morty_conversations/RickAndMortyScripts.csv')\n",
    "all_rick.head(10)\n",
    "\n",
    "all_rick_train, all_rick_val = all_rick.iloc[:int(0.8*len(all_rick))], all_rick.iloc[int(0.8*len(all_rick)):]\n",
    "# all_rick_train, all_rick_val = train_test_split(all_rick, test_size=0.2, random_state=42)\n",
    "all_rick_train = all_rick_train.reset_index()\n",
    "all_rick_val   = all_rick_val.reset_index()\n",
    "\n",
    "\n",
    "# construct df with prompts and responses as columns\n",
    "contexted = []\n",
    "n = 7\n",
    "for i in range(n, len(all_rick_train['line'])):\n",
    "    row = []\n",
    "    prev = i - 1 - n # we additionally subtract 1, so row will contain current response and 7 previous responses  \n",
    "    for j in range(i, prev, -1):\n",
    "        row.append(all_rick_train['line'][j])\n",
    "    contexted.append(row)\n",
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "trn_df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "trn_df.head(5)\n",
    "\n",
    "contexted = []\n",
    "n = 7\n",
    "for i in range(n, len(all_rick_val['line'])):\n",
    "    row = []\n",
    "    prev = i - 1 - n # we additionally subtract 1, so row will contain current response and 7 previous responses  \n",
    "    for j in range(i, prev, -1):\n",
    "        row.append(all_rick_val['line'][j])\n",
    "    contexted.append(row)\n",
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "val_df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "val_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86F3WhnFO4H8"
   },
   "source": [
    "Now will convert our dataset in a format suitable for our model. Basically we will concatenate responses in one string for each row (additionally we will add special 'end of string' token between responses, so the model will understand end of each response in a string).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PX7jeWpYmOe_"
   },
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size # - (tokenizer.max_len - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "naaRHoXgnStq"
   },
   "outputs": [],
   "source": [
    "# Cacheing and storing of data/checkpoints\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkvMNnrnVHQw"
   },
   "source": [
    "## Training and Evaluating\n",
    "\n",
    "There will be quite a lot of code needed for training our model but donâ€™t worry, everything should work as is, the main thing is to give the model the dataset in the right format.\n",
    "\n",
    "![alt text](https://media.giphy.com/media/KetvQljQJdEMscR83K/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We gotta go, gotta get outta here, come on. Got a surprise for you Morty.<|endoftext|>What do you think of this... flying vehicle, Morty? I built it outta stuff I found in the garage.<|endoftext|>Yeah, Rick... I-it's great. Is this the surprise?<|endoftext|>Morty. I had to... I had to do it. I hadâ€” I had toâ€” I had to make a bomb, Morty. I had to create a bomb.<|endoftext|>What?! A bomb?!<|endoftext|>We're gonna drop it down there just get a whole fresh start, Morty. Create a whole fresh start.<|endoftext|>T-t-that's absolutely crazy!<|endoftext|>Come on, Morty. Just take it easy, Morty. It's gonna be good. Right now, we're gonna go pick up your little friend Jessica.<|endoftext|>\n",
      "---------------\n",
      "What do you think of this... flying vehicle, Morty? I built it outta stuff I found in the garage.<|endoftext|>Yeah, Rick... I-it's great. Is this the surprise?<|endoftext|>Morty. I had to... I had to do it. I hadâ€” I had toâ€” I had to make a bomb, Morty. I had to create a bomb.<|endoftext|>What?! A bomb?!<|endoftext|>We're gonna drop it down there just get a whole fresh start, Morty. Create a whole fresh start.<|endoftext|>T-t-that's absolutely crazy!<|endoftext|>Come on, Morty. Just take it easy, Morty. It's gonna be good. Right now, we're gonna go pick up your little friend Jessica.<|endoftext|>Jessica? From my math class?<|endoftext|>\n",
      "---------------\n",
      "What, Rick? Whatâ€™s going on?<|endoftext|>I got a surprise for you, Morty.<|endoftext|>It's the middle of the night. What are you talking about?<|endoftext|>Come on, I got a surprise for you.  Come on, hurry up.<|endoftext|>Ow! Ow! You're tugging me too hard!<|endoftext|>We gotta go, gotta get outta here, come on. Got a surprise for you Morty.<|endoftext|>What do you think of this... flying vehicle, Morty? I built it outta stuff I found in the garage.<|endoftext|>Yeah, Rick... I-it's great. Is this the surprise?<|endoftext|>\n",
      "---------------\n",
      "Come on, I got a surprise for you.  Come on, hurry up.<|endoftext|>Ow! Ow! You're tugging me too hard!<|endoftext|>We gotta go, gotta get outta here, come on. Got a surprise for you Morty.<|endoftext|>What do you think of this... flying vehicle, Morty? I built it outta stuff I found in the garage.<|endoftext|>Yeah, Rick... I-it's great. Is this the surprise?<|endoftext|>Morty. I had to... I had to do it. I hadâ€” I had toâ€” I had to make a bomb, Morty. I had to create a bomb.<|endoftext|>What?! A bomb?!<|endoftext|>We're gonna drop it down there just get a whole fresh start, Morty. Create a whole fresh start.<|endoftext|>\n",
      "---------------\n",
      "I got a surprise for you, Morty.<|endoftext|>It's the middle of the night. What are you talking about?<|endoftext|>Come on, I got a surprise for you.  Come on, hurry up.<|endoftext|>Ow! Ow! You're tugging me too hard!<|endoftext|>We gotta go, gotta get outta here, come on. Got a surprise for you Morty.<|endoftext|>What do you think of this... flying vehicle, Morty? I built it outta stuff I found in the garage.<|endoftext|>Yeah, Rick... I-it's great. Is this the surprise?<|endoftext|>Morty. I had to... I had to do it. I hadâ€” I had toâ€” I had to make a bomb, Morty. I had to create a bomb.<|endoftext|>\n",
      "---------------\n",
      "Morty! You gotta come on. Jus'... you gotta come with me.<|endoftext|>What, Rick? Whatâ€™s going on?<|endoftext|>I got a surprise for you, Morty.<|endoftext|>It's the middle of the night. What are you talking about?<|endoftext|>Come on, I got a surprise for you.  Come on, hurry up.<|endoftext|>Ow! Ow! You're tugging me too hard!<|endoftext|>We gotta go, gotta get outta here, come on. Got a surprise for you Morty.<|endoftext|>What do you think of this... flying vehicle, Morty? I built it outta stuff I found in the garage.<|endoftext|>\n",
      "---------------\n",
      "Morty. I had to... I had to do it. I hadâ€” I had toâ€” I had to make a bomb, Morty. I had to create a bomb.<|endoftext|>What?! A bomb?!<|endoftext|>We're gonna drop it down there just get a whole fresh start, Morty. Create a whole fresh start.<|endoftext|>T-t-that's absolutely crazy!<|endoftext|>Come on, Morty. Just take it easy, Morty. It's gonna be good. Right now, we're gonna go pick up your little friend Jessica.<|endoftext|>Jessica? From my math class?<|endoftext|>When I drop the bomb you know, I want you to have somebody, you know? I want you to have the thing. I'm gonna make it like a new Adam and Eve, and you're gonna be Adam.<|endoftext|>Ohh...<|endoftext|>\n",
      "---------------\n",
      "Yeah, Rick... I-it's great. Is this the surprise?<|endoftext|>Morty. I had to... I had to do it. I hadâ€” I had toâ€” I had to make a bomb, Morty. I had to create a bomb.<|endoftext|>What?! A bomb?!<|endoftext|>We're gonna drop it down there just get a whole fresh start, Morty. Create a whole fresh start.<|endoftext|>T-t-that's absolutely crazy!<|endoftext|>Come on, Morty. Just take it easy, Morty. It's gonna be good. Right now, we're gonna go pick up your little friend Jessica.<|endoftext|>Jessica? From my math class?<|endoftext|>When I drop the bomb you know, I want you to have somebody, you know? I want you to have the thing. I'm gonna make it like a new Adam and Eve, and you're gonna be Adam.<|endoftext|>\n",
      "---------------\n",
      "It's the middle of the night. What are you talking about?<|endoftext|>Come on, I got a surprise for you.  Come on, hurry up.<|endoftext|>Ow! Ow! You're tugging me too hard!<|endoftext|>We gotta go, gotta get outta here, come on. Got a surprise for you Morty.<|endoftext|>What do you think of this... flying vehicle, Morty? I built it outta stuff I found in the garage.<|endoftext|>Yeah, Rick... I-it's great. Is this the surprise?<|endoftext|>Morty. I had to... I had to do it. I hadâ€” I had toâ€” I had to make a bomb, Morty. I had to create a bomb.<|endoftext|>What?! A bomb?!<|endoftext|>\n",
      "---------------\n",
      "Ow! Ow! You're tugging me too hard!<|endoftext|>We gotta go, gotta get outta here, come on. Got a surprise for you Morty.<|endoftext|>What do you think of this... flying vehicle, Morty? I built it outta stuff I found in the garage.<|endoftext|>Yeah, Rick... I-it's great. Is this the surprise?<|endoftext|>Morty. I had to... I had to do it. I hadâ€” I had toâ€” I had to make a bomb, Morty. I had to create a bomb.<|endoftext|>What?! A bomb?!<|endoftext|>We're gonna drop it down there just get a whole fresh start, Morty. Create a whole fresh start.<|endoftext|>T-t-that's absolutely crazy!<|endoftext|>\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "\n",
    "args.n_gpu = 1\n",
    "set_seed(args)\n",
    "\n",
    "train_dataset = load_and_cache_examples(args, tokenizer, trn_df.iloc[:10], val_df, evaluate=False)\n",
    "\n",
    "def collate(examples: List[torch.Tensor]):\n",
    "    if tokenizer._pad_token is None:\n",
    "        return pad_sequence(examples, batch_first=True)\n",
    "    return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "# train_sampler = SequentialSampler(train_dataset)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=train_sampler, batch_size=1, collate_fn=collate, drop_last = True\n",
    ")\n",
    "\n",
    "eval_dataset = load_and_cache_examples(args, tokenizer, trn_df, val_df, evaluate=True)\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, sampler=eval_sampler, batch_size=1, collate_fn=collate, drop_last = True\n",
    ")\n",
    "\n",
    "# example usage\n",
    "for batch in train_dataloader:\n",
    "    example_id = 0\n",
    "    # print('input_ids:\\n',       batch[example_id])\n",
    "    # print('position_ids:\\n',    batch['position_ids'][example_id])\n",
    "    # print('token_type_ids:\\n',  batch['token_type_ids'][example_id])\n",
    "    # print('target_ids:\\n',      batch['target_ids'][example_id])\n",
    "    # print('attention_masks:\\n', batch['attention_masks'][example_id])\n",
    "\n",
    "    print(tokenizer.decode(batch[example_id]))\n",
    "    print('---------------')\n",
    "    \n",
    "    #     break\n",
    "\n",
    "# example_id = 0\n",
    "# print('input_ids:\\n',       batch[example_id])\n",
    "# # print('position_ids:\\n',    batch['position_ids'][example_id])\n",
    "# # print('token_type_ids:\\n',  batch['token_type_ids'][example_id])\n",
    "# # print('target_ids:\\n',      batch['target_ids'][example_id])\n",
    "# # print('attention_masks:\\n', batch['attention_masks'][example_id])\n",
    "\n",
    "# tokenizer.decode(batch[example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tXzKlXHeu0Mb"
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, df_trn, df_val, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "    \n",
    "    #======================================================\n",
    "    # train dataset + train dataloader\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "    \n",
    "    # set_seed(args)\n",
    "    # for batch in train_dataloader:\n",
    "    #     example_id = 0\n",
    "    #     print(tokenizer.decode(batch[example_id]))\n",
    "    #     print('---------------')\n",
    "    #======================================================\n",
    "    # initialize model, optimizer, and scheduler\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        print('loading optimizer and scheduler...')\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "    #======================================================\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # # Check if continuing training from a checkpoint\n",
    "    # if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "    #     try:\n",
    "    #         # set global_step to gobal_step of last saved checkpoint from model path\n",
    "    #         checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "    #         global_step = int(checkpoint_suffix)\n",
    "    #         epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "    #         steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "    #         logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "    #         logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "    #         logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "    #         logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "    #     except ValueError:\n",
    "    #         logger.info(\"  Starting fine-tuning.\")\n",
    "    # else:\n",
    "    #     logger.info(\"  Fine-tuning from scratch...\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    \n",
    "    # evaluate before training\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    result = evaluate(args, model, tokenizer, df_trn, df_val)\n",
    "    result = dict((k + \"_{}\".format(0), v) for k, v in result.items())\n",
    "    print('evaluate before training:', result)\n",
    "    \n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            \n",
    "            # print('example input ids: ', inputs[0], inputs[0].shape)\n",
    "            # print('example target ids: ', labels[0], labels[0].shape)\n",
    "            # print('example inputs: ', tokenizer.decode(inputs[0]))\n",
    "            # print('example targets:', tokenizer.decode(labels[0]))\n",
    "\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs.loss  #[0]  # model outputs are always tuple in transformers (see doc)\n",
    "            # print('model outputs logits:', outputs.logits)\n",
    "            # print('model loss:', loss)\n",
    "            \n",
    "            \n",
    "#             ###### DEBUG CODE ######\n",
    "#             loss.backward()\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "#             print('max grad norm:', args.max_grad_norm)\n",
    "#             print('model params after clipping:', model.parameters())\n",
    "            \n",
    "#             optimizer.step()\n",
    "#             scheduler.step()  # Update learning rate schedule\n",
    "#             print('lr after scheduler step:', scheduler.get_lr())\n",
    "#             model.zero_grad()\n",
    "#             global_step += 1\n",
    "            \n",
    "#             tr_loss += loss.item()\n",
    "            \n",
    "\n",
    "            ##### ORIGINAL CODE ######\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "\n",
    "                # gradient clipping\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "            \n",
    "        # Evaluation\n",
    "        result = evaluate(args, model, tokenizer, df_trn, df_val)\n",
    "        result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "        print('epochwise results:', result)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\", write_to_file=False) -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            print(lm_loss)\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "    \n",
    "    if write_to_file:\n",
    "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-MGD6bFXV4Z-"
   },
   "outputs": [],
   "source": [
    "# Main runner\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    # set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    print(model)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "        global_step, tr_loss = train(args, train_dataset, df_trn, df_val, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix, write_to_file=True)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZEHDzR0Vjs7"
   },
   "source": [
    "It is time to train our model!\n",
    "\n",
    "![alt text](https://media.giphy.com/media/Tia3dkakIp2m4uGoDI/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5aab8b98fdf741428fda6a2936a6c32b",
      "b581bbe998324e4198b54f1b33fa8fcb",
      "ff9f00d7a67b4a0d944299166c3943e5",
      "0bc945b69eff441db1bb7be22c0896f9",
      "475d496b15b343448e7463cdcf9463a4",
      "e34e242a73ac4f4887f3d16d651affdf",
      "b2c9710105a6405f9de235c41ed39a50",
      "62d6b4a4ad454316bcceb04ecb55821b",
      "8e1d23ba00644b80b8b4b5ee6a5abb6b",
      "4f3f630db29349da9afb5b7479f4aaf2",
      "b69f24d60bfe4bd7a50cb9465505e0dc",
      "74fad213012043dcaa370dc2e54de8bc",
      "4bf0d712c8564501b202165d1e0033d5",
      "19f2b118a7db4bc292c06d94c2760590",
      "ee6446c4023c48518fd58b833e1a594e",
      "c99fa5c01fe4478aa8e57b7d59e11c29",
      "136a63d66eb44cccbcd66bf4c0535a6b",
      "b1b239d3d3f14906934461eb66c1c0f6",
      "f326022eebe1428daab12fab05c8a075",
      "c4091ab4f5264a86a149793853dbb941",
      "a0f8864b0bf74deab8240b716dbda564",
      "3cb73064ba3545d0815cef04dba25f40",
      "c6fa8a5e40b74a4199ab785729880b39",
      "57b31670fc3a40839a6004c6aa0e41a5",
      "c2109878b39849a799187136beadf144",
      "36d501a1ff37420eadef5bcd7297ee26",
      "0a76fe2605c84745b42f9cdb5f1a52db",
      "2dfb4d0fc09645118f830e4ebc5dfdd0",
      "f5b6f22c0ea24860a93cbe6a6edfc05e",
      "69bbca8e234e48fbbc389fd44e11624f",
      "02bc54995de148238eb5ed06f2ea8d62",
      "270e2a3848324fb5996af60d6ef26366",
      "6296e303aed34c01866caffc200c0cfb",
      "60eabd58645142aea72ae027b057d65d",
      "9d9f452aa79048599819ad5cc759a5e5",
      "d4e4ccce098941459c4409187201979d",
      "d57f56580d244a59ad388adb33abcb4d",
      "37e11da84adb4832842d9a9ce54c2c2b",
      "822d24db536f4994af65dd764364695c",
      "1391f67de61c40f1aa24fd267c1f70d9"
     ]
    },
    "id": "__iqR8YFV-Ex",
    "outputId": "c14014a0-271a-452d-d282-db2bf50a6589"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 21:54:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 21:54:25 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x0000019299B10250>\n",
      "07/13/2022 21:54:25 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "07/13/2022 21:54:27 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "07/13/2022 21:54:27 - INFO - __main__ -   ***** Running training *****\n",
      "07/13/2022 21:54:27 - INFO - __main__ -     Num examples = 1517\n",
      "07/13/2022 21:54:27 - INFO - __main__ -     Num Epochs = 3\n",
      "07/13/2022 21:54:27 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
      "07/13/2022 21:54:27 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "07/13/2022 21:54:27 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "07/13/2022 21:54:27 - INFO - __main__ -     Total optimization steps = 4551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070ee4f3d8fd410faa33351abbeada8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 21:54:27 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "07/13/2022 21:54:27 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "07/13/2022 21:54:27 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/13/2022 21:54:27 - INFO - __main__ -     Num examples = 374\n",
      "07/13/2022 21:54:27 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64017f7f70a40199104cc9ddc2c84fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.7028, device='cuda:0')\n",
      "tensor(8.2763, device='cuda:0')\n",
      "tensor(7.6932, device='cuda:0')\n",
      "tensor(7.7385, device='cuda:0')\n",
      "tensor(7.6431, device='cuda:0')\n",
      "tensor(8.0051, device='cuda:0')\n",
      "tensor(6.4320, device='cuda:0')\n",
      "tensor(6.3651, device='cuda:0')\n",
      "tensor(6.2279, device='cuda:0')\n",
      "tensor(5.8319, device='cuda:0')\n",
      "tensor(6.5971, device='cuda:0')\n",
      "tensor(6.5812, device='cuda:0')\n",
      "tensor(7.2387, device='cuda:0')\n",
      "tensor(7.1878, device='cuda:0')\n",
      "tensor(8.1156, device='cuda:0')\n",
      "tensor(8.1642, device='cuda:0')\n",
      "tensor(8.3586, device='cuda:0')\n",
      "tensor(8.4157, device='cuda:0')\n",
      "tensor(8.5441, device='cuda:0')\n",
      "tensor(8.6285, device='cuda:0')\n",
      "tensor(8.6790, device='cuda:0')\n",
      "tensor(9.0097, device='cuda:0')\n",
      "tensor(7.4970, device='cuda:0')\n",
      "tensor(7.5359, device='cuda:0')\n",
      "tensor(7.3305, device='cuda:0')\n",
      "tensor(6.9391, device='cuda:0')\n",
      "tensor(6.5855, device='cuda:0')\n",
      "tensor(6.9843, device='cuda:0')\n",
      "tensor(6.9156, device='cuda:0')\n",
      "tensor(7.3290, device='cuda:0')\n",
      "tensor(6.9891, device='cuda:0')\n",
      "tensor(6.1842, device='cuda:0')\n",
      "tensor(6.0775, device='cuda:0')\n",
      "tensor(5.9435, device='cuda:0')\n",
      "tensor(6.2864, device='cuda:0')\n",
      "tensor(6.4148, device='cuda:0')\n",
      "tensor(6.4792, device='cuda:0')\n",
      "tensor(6.3779, device='cuda:0')\n",
      "tensor(6.5562, device='cuda:0')\n",
      "tensor(6.7759, device='cuda:0')\n",
      "tensor(7.0340, device='cuda:0')\n",
      "tensor(6.8807, device='cuda:0')\n",
      "tensor(6.8102, device='cuda:0')\n",
      "tensor(6.8516, device='cuda:0')\n",
      "tensor(6.6645, device='cuda:0')\n",
      "tensor(6.6685, device='cuda:0')\n",
      "tensor(7.3067, device='cuda:0')\n",
      "tensor(6.8453, device='cuda:0')\n",
      "tensor(6.8700, device='cuda:0')\n",
      "tensor(6.7199, device='cuda:0')\n",
      "tensor(6.2484, device='cuda:0')\n",
      "tensor(6.0956, device='cuda:0')\n",
      "tensor(6.6545, device='cuda:0')\n",
      "tensor(6.5379, device='cuda:0')\n",
      "tensor(5.7922, device='cuda:0')\n",
      "tensor(6.0683, device='cuda:0')\n",
      "tensor(6.0375, device='cuda:0')\n",
      "tensor(6.4384, device='cuda:0')\n",
      "tensor(6.7561, device='cuda:0')\n",
      "tensor(6.6944, device='cuda:0')\n",
      "tensor(7.7058, device='cuda:0')\n",
      "tensor(7.5982, device='cuda:0')\n",
      "tensor(8.1171, device='cuda:0')\n",
      "tensor(8.3061, device='cuda:0')\n",
      "tensor(8.3098, device='cuda:0')\n",
      "tensor(7.9565, device='cuda:0')\n",
      "tensor(8.4023, device='cuda:0')\n",
      "tensor(8.7429, device='cuda:0')\n",
      "tensor(8.7045, device='cuda:0')\n",
      "tensor(8.3057, device='cuda:0')\n",
      "tensor(7.8517, device='cuda:0')\n",
      "tensor(8.1430, device='cuda:0')\n",
      "tensor(8.3419, device='cuda:0')\n",
      "tensor(9.3535, device='cuda:0')\n",
      "tensor(9.8240, device='cuda:0')\n",
      "tensor(9.3938, device='cuda:0')\n",
      "tensor(8.7533, device='cuda:0')\n",
      "tensor(7.6803, device='cuda:0')\n",
      "tensor(7.1189, device='cuda:0')\n",
      "tensor(6.2258, device='cuda:0')\n",
      "tensor(6.5426, device='cuda:0')\n",
      "tensor(6.1309, device='cuda:0')\n",
      "tensor(5.9747, device='cuda:0')\n",
      "tensor(6.3147, device='cuda:0')\n",
      "tensor(6.2445, device='cuda:0')\n",
      "tensor(5.9420, device='cuda:0')\n",
      "tensor(6.2076, device='cuda:0')\n",
      "tensor(6.0488, device='cuda:0')\n",
      "tensor(5.7317, device='cuda:0')\n",
      "tensor(5.7582, device='cuda:0')\n",
      "tensor(6.3024, device='cuda:0')\n",
      "tensor(5.7989, device='cuda:0')\n",
      "tensor(5.9457, device='cuda:0')\n",
      "tensor(6.6459, device='cuda:0')\n",
      "tensor(7.5380, device='cuda:0')\n",
      "tensor(7.6327, device='cuda:0')\n",
      "tensor(8.1016, device='cuda:0')\n",
      "tensor(7.7958, device='cuda:0')\n",
      "tensor(7.3892, device='cuda:0')\n",
      "tensor(7.5021, device='cuda:0')\n",
      "tensor(6.9944, device='cuda:0')\n",
      "tensor(6.7527, device='cuda:0')\n",
      "tensor(7.4868, device='cuda:0')\n",
      "tensor(8.0882, device='cuda:0')\n",
      "tensor(10.2995, device='cuda:0')\n",
      "tensor(9.6000, device='cuda:0')\n",
      "tensor(9.8238, device='cuda:0')\n",
      "tensor(9.5774, device='cuda:0')\n",
      "tensor(9.5252, device='cuda:0')\n",
      "tensor(9.0809, device='cuda:0')\n",
      "tensor(8.5265, device='cuda:0')\n",
      "tensor(9.3353, device='cuda:0')\n",
      "tensor(8.5272, device='cuda:0')\n",
      "tensor(8.9430, device='cuda:0')\n",
      "tensor(9.5398, device='cuda:0')\n",
      "tensor(9.9426, device='cuda:0')\n",
      "tensor(9.1803, device='cuda:0')\n",
      "tensor(9.2394, device='cuda:0')\n",
      "tensor(8.8923, device='cuda:0')\n",
      "tensor(8.2255, device='cuda:0')\n",
      "tensor(8.1273, device='cuda:0')\n",
      "tensor(7.7267, device='cuda:0')\n",
      "tensor(8.0437, device='cuda:0')\n",
      "tensor(6.9206, device='cuda:0')\n",
      "tensor(10.6035, device='cuda:0')\n",
      "tensor(10.0762, device='cuda:0')\n",
      "tensor(9.3555, device='cuda:0')\n",
      "tensor(9.6246, device='cuda:0')\n",
      "tensor(10.0088, device='cuda:0')\n",
      "tensor(9.8185, device='cuda:0')\n",
      "tensor(9.5057, device='cuda:0')\n",
      "tensor(9.6947, device='cuda:0')\n",
      "tensor(8.1561, device='cuda:0')\n",
      "tensor(7.7572, device='cuda:0')\n",
      "tensor(8.4145, device='cuda:0')\n",
      "tensor(8.3350, device='cuda:0')\n",
      "tensor(7.8827, device='cuda:0')\n",
      "tensor(9.0400, device='cuda:0')\n",
      "tensor(9.2340, device='cuda:0')\n",
      "tensor(9.0615, device='cuda:0')\n",
      "tensor(9.2202, device='cuda:0')\n",
      "tensor(10.4461, device='cuda:0')\n",
      "tensor(9.5235, device='cuda:0')\n",
      "tensor(9.4459, device='cuda:0')\n",
      "tensor(9.3216, device='cuda:0')\n",
      "tensor(8.5644, device='cuda:0')\n",
      "tensor(8.3231, device='cuda:0')\n",
      "tensor(8.4304, device='cuda:0')\n",
      "tensor(8.4944, device='cuda:0')\n",
      "tensor(8.1421, device='cuda:0')\n",
      "tensor(8.5895, device='cuda:0')\n",
      "tensor(9.1562, device='cuda:0')\n",
      "tensor(8.9356, device='cuda:0')\n",
      "tensor(8.1676, device='cuda:0')\n",
      "tensor(8.2752, device='cuda:0')\n",
      "tensor(8.7112, device='cuda:0')\n",
      "tensor(7.6667, device='cuda:0')\n",
      "tensor(7.0444, device='cuda:0')\n",
      "tensor(6.8673, device='cuda:0')\n",
      "tensor(6.9536, device='cuda:0')\n",
      "tensor(6.9378, device='cuda:0')\n",
      "tensor(7.5199, device='cuda:0')\n",
      "tensor(7.4287, device='cuda:0')\n",
      "tensor(6.7468, device='cuda:0')\n",
      "tensor(6.7261, device='cuda:0')\n",
      "tensor(6.2967, device='cuda:0')\n",
      "tensor(6.1624, device='cuda:0')\n",
      "tensor(5.9425, device='cuda:0')\n",
      "tensor(6.3274, device='cuda:0')\n",
      "tensor(7.3028, device='cuda:0')\n",
      "tensor(7.4045, device='cuda:0')\n",
      "tensor(7.9929, device='cuda:0')\n",
      "tensor(8.2810, device='cuda:0')\n",
      "tensor(8.2518, device='cuda:0')\n",
      "tensor(8.7021, device='cuda:0')\n",
      "tensor(8.7162, device='cuda:0')\n",
      "tensor(8.7562, device='cuda:0')\n",
      "tensor(8.7862, device='cuda:0')\n",
      "tensor(9.0187, device='cuda:0')\n",
      "tensor(8.8011, device='cuda:0')\n",
      "tensor(8.0035, device='cuda:0')\n",
      "tensor(7.7829, device='cuda:0')\n",
      "tensor(7.3189, device='cuda:0')\n",
      "tensor(7.3080, device='cuda:0')\n",
      "tensor(7.1905, device='cuda:0')\n",
      "tensor(7.0222, device='cuda:0')\n",
      "tensor(6.5121, device='cuda:0')\n",
      "tensor(5.8245, device='cuda:0')\n",
      "tensor(5.7574, device='cuda:0')\n",
      "tensor(5.7039, device='cuda:0')\n",
      "tensor(5.5042, device='cuda:0')\n",
      "tensor(5.6463, device='cuda:0')\n",
      "tensor(6.2171, device='cuda:0')\n",
      "tensor(6.2439, device='cuda:0')\n",
      "tensor(6.6976, device='cuda:0')\n",
      "tensor(6.1452, device='cuda:0')\n",
      "tensor(6.4897, device='cuda:0')\n",
      "tensor(6.4997, device='cuda:0')\n",
      "tensor(6.6472, device='cuda:0')\n",
      "tensor(6.5014, device='cuda:0')\n",
      "tensor(6.1994, device='cuda:0')\n",
      "tensor(6.0793, device='cuda:0')\n",
      "tensor(6.1885, device='cuda:0')\n",
      "tensor(5.4401, device='cuda:0')\n",
      "tensor(5.9104, device='cuda:0')\n",
      "tensor(5.9443, device='cuda:0')\n",
      "tensor(6.4083, device='cuda:0')\n",
      "tensor(6.8371, device='cuda:0')\n",
      "tensor(7.5104, device='cuda:0')\n",
      "tensor(7.4323, device='cuda:0')\n",
      "tensor(7.5270, device='cuda:0')\n",
      "tensor(7.2794, device='cuda:0')\n",
      "tensor(7.2123, device='cuda:0')\n",
      "tensor(7.9276, device='cuda:0')\n",
      "tensor(7.9580, device='cuda:0')\n",
      "tensor(7.7345, device='cuda:0')\n",
      "tensor(7.8224, device='cuda:0')\n",
      "tensor(8.0079, device='cuda:0')\n",
      "tensor(8.1219, device='cuda:0')\n",
      "tensor(8.2882, device='cuda:0')\n",
      "tensor(8.5852, device='cuda:0')\n",
      "tensor(8.3153, device='cuda:0')\n",
      "tensor(8.0639, device='cuda:0')\n",
      "tensor(8.0288, device='cuda:0')\n",
      "tensor(7.4966, device='cuda:0')\n",
      "tensor(7.7090, device='cuda:0')\n",
      "tensor(8.3558, device='cuda:0')\n",
      "tensor(8.5239, device='cuda:0')\n",
      "tensor(8.5545, device='cuda:0')\n",
      "tensor(9.4493, device='cuda:0')\n",
      "tensor(10.2385, device='cuda:0')\n",
      "tensor(10.3319, device='cuda:0')\n",
      "tensor(10.9407, device='cuda:0')\n",
      "tensor(10.3575, device='cuda:0')\n",
      "tensor(8.9544, device='cuda:0')\n",
      "tensor(8.4626, device='cuda:0')\n",
      "tensor(8.6622, device='cuda:0')\n",
      "tensor(7.6890, device='cuda:0')\n",
      "tensor(7.6619, device='cuda:0')\n",
      "tensor(7.2395, device='cuda:0')\n",
      "tensor(7.8054, device='cuda:0')\n",
      "tensor(8.8750, device='cuda:0')\n",
      "tensor(10.5423, device='cuda:0')\n",
      "tensor(11.2583, device='cuda:0')\n",
      "tensor(9.4397, device='cuda:0')\n",
      "tensor(10.5594, device='cuda:0')\n",
      "tensor(9.5629, device='cuda:0')\n",
      "tensor(9.4491, device='cuda:0')\n",
      "tensor(9.3238, device='cuda:0')\n",
      "tensor(8.8321, device='cuda:0')\n",
      "tensor(8.7158, device='cuda:0')\n",
      "tensor(8.6767, device='cuda:0')\n",
      "tensor(8.2201, device='cuda:0')\n",
      "tensor(8.6752, device='cuda:0')\n",
      "tensor(8.1734, device='cuda:0')\n",
      "tensor(7.9242, device='cuda:0')\n",
      "tensor(8.0284, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.4447, device='cuda:0')\n",
      "tensor(7.7063, device='cuda:0')\n",
      "tensor(7.3068, device='cuda:0')\n",
      "tensor(7.8647, device='cuda:0')\n",
      "tensor(8.3453, device='cuda:0')\n",
      "tensor(8.0706, device='cuda:0')\n",
      "tensor(8.5225, device='cuda:0')\n",
      "tensor(8.1891, device='cuda:0')\n",
      "tensor(8.0658, device='cuda:0')\n",
      "tensor(7.8553, device='cuda:0')\n",
      "tensor(8.4951, device='cuda:0')\n",
      "tensor(8.2165, device='cuda:0')\n",
      "tensor(7.7434, device='cuda:0')\n",
      "tensor(7.7659, device='cuda:0')\n",
      "tensor(7.7829, device='cuda:0')\n",
      "tensor(7.7113, device='cuda:0')\n",
      "tensor(8.2286, device='cuda:0')\n",
      "tensor(7.7495, device='cuda:0')\n",
      "tensor(8.1875, device='cuda:0')\n",
      "tensor(8.0529, device='cuda:0')\n",
      "tensor(8.4550, device='cuda:0')\n",
      "tensor(8.9594, device='cuda:0')\n",
      "tensor(8.7042, device='cuda:0')\n",
      "tensor(8.2567, device='cuda:0')\n",
      "tensor(8.8276, device='cuda:0')\n",
      "tensor(8.8339, device='cuda:0')\n",
      "tensor(9.3432, device='cuda:0')\n",
      "tensor(9.7417, device='cuda:0')\n",
      "tensor(9.5434, device='cuda:0')\n",
      "tensor(9.2064, device='cuda:0')\n",
      "tensor(9.1488, device='cuda:0')\n",
      "tensor(8.6852, device='cuda:0')\n",
      "tensor(7.9487, device='cuda:0')\n",
      "tensor(7.9774, device='cuda:0')\n",
      "tensor(7.7065, device='cuda:0')\n",
      "tensor(7.0220, device='cuda:0')\n",
      "tensor(6.9703, device='cuda:0')\n",
      "tensor(6.6979, device='cuda:0')\n",
      "tensor(6.6319, device='cuda:0')\n",
      "tensor(6.9502, device='cuda:0')\n",
      "tensor(7.6729, device='cuda:0')\n",
      "tensor(7.5299, device='cuda:0')\n",
      "tensor(8.1144, device='cuda:0')\n",
      "tensor(8.8999, device='cuda:0')\n",
      "tensor(8.8597, device='cuda:0')\n",
      "tensor(8.4444, device='cuda:0')\n",
      "tensor(8.3724, device='cuda:0')\n",
      "tensor(8.8132, device='cuda:0')\n",
      "tensor(8.4509, device='cuda:0')\n",
      "tensor(8.1914, device='cuda:0')\n",
      "tensor(7.6805, device='cuda:0')\n",
      "tensor(6.9744, device='cuda:0')\n",
      "tensor(7.5641, device='cuda:0')\n",
      "tensor(7.0530, device='cuda:0')\n",
      "tensor(7.1338, device='cuda:0')\n",
      "tensor(6.9525, device='cuda:0')\n",
      "tensor(6.8289, device='cuda:0')\n",
      "tensor(6.6195, device='cuda:0')\n",
      "tensor(6.2024, device='cuda:0')\n",
      "tensor(6.1975, device='cuda:0')\n",
      "tensor(6.0306, device='cuda:0')\n",
      "tensor(6.1735, device='cuda:0')\n",
      "tensor(5.6880, device='cuda:0')\n",
      "tensor(5.5711, device='cuda:0')\n",
      "tensor(5.8044, device='cuda:0')\n",
      "tensor(7.5309, device='cuda:0')\n",
      "tensor(7.7455, device='cuda:0')\n",
      "tensor(7.3560, device='cuda:0')\n",
      "tensor(7.1744, device='cuda:0')\n",
      "tensor(7.5955, device='cuda:0')\n",
      "tensor(7.2435, device='cuda:0')\n",
      "tensor(7.4380, device='cuda:0')\n",
      "tensor(7.2534, device='cuda:0')\n",
      "tensor(6.4963, device='cuda:0')\n",
      "tensor(6.7404, device='cuda:0')\n",
      "tensor(6.9587, device='cuda:0')\n",
      "tensor(6.6456, device='cuda:0')\n",
      "tensor(6.3211, device='cuda:0')\n",
      "tensor(5.8569, device='cuda:0')\n",
      "tensor(5.3590, device='cuda:0')\n",
      "tensor(5.3877, device='cuda:0')\n",
      "tensor(5.2143, device='cuda:0')\n",
      "tensor(5.9844, device='cuda:0')\n",
      "tensor(6.2648, device='cuda:0')\n",
      "tensor(6.6038, device='cuda:0')\n",
      "tensor(6.5999, device='cuda:0')\n",
      "tensor(7.0089, device='cuda:0')\n",
      "tensor(8.0114, device='cuda:0')\n",
      "tensor(7.3678, device='cuda:0')\n",
      "tensor(7.4841, device='cuda:0')\n",
      "tensor(7.1297, device='cuda:0')\n",
      "tensor(6.5443, device='cuda:0')\n",
      "tensor(6.9361, device='cuda:0')\n",
      "tensor(7.0358, device='cuda:0')\n",
      "tensor(7.3342, device='cuda:0')\n",
      "tensor(6.7845, device='cuda:0')\n",
      "tensor(6.5628, device='cuda:0')\n",
      "tensor(6.3725, device='cuda:0')\n",
      "tensor(7.4570, device='cuda:0')\n",
      "tensor(10.7109, device='cuda:0')\n",
      "tensor(10.1906, device='cuda:0')\n",
      "tensor(11.7513, device='cuda:0')\n",
      "tensor(11.5170, device='cuda:0')\n",
      "tensor(12.0669, device='cuda:0')\n",
      "tensor(12.0481, device='cuda:0')\n",
      "tensor(11.2193, device='cuda:0')\n",
      "tensor(10.7633, device='cuda:0')\n",
      "tensor(7.7339, device='cuda:0')\n",
      "tensor(7.7644, device='cuda:0')\n",
      "tensor(6.8022, device='cuda:0')\n",
      "tensor(5.4870, device='cuda:0')\n",
      "tensor(6.2871, device='cuda:0')\n",
      "tensor(6.5525, device='cuda:0')\n",
      "tensor(6.6991, device='cuda:0')\n",
      "tensor(6.7074, device='cuda:0')\n",
      "tensor(7.4855, device='cuda:0')\n",
      "tensor(7.4690, device='cuda:0')\n",
      "evaluate before training: {'perplexity_0': tensor(2291.7490)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7df30188e344b03a9e2063c580801a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 21:56:38 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "07/13/2022 21:56:39 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "07/13/2022 21:56:39 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/13/2022 21:56:39 - INFO - __main__ -     Num examples = 374\n",
      "07/13/2022 21:56:39 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6189064f89e47a2a6a17b27877926ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8615, device='cuda:0')\n",
      "tensor(4.8153, device='cuda:0')\n",
      "tensor(4.6140, device='cuda:0')\n",
      "tensor(4.4550, device='cuda:0')\n",
      "tensor(4.4510, device='cuda:0')\n",
      "tensor(4.5097, device='cuda:0')\n",
      "tensor(4.4507, device='cuda:0')\n",
      "tensor(4.3221, device='cuda:0')\n",
      "tensor(3.9900, device='cuda:0')\n",
      "tensor(3.7985, device='cuda:0')\n",
      "tensor(3.6305, device='cuda:0')\n",
      "tensor(3.6562, device='cuda:0')\n",
      "tensor(3.5840, device='cuda:0')\n",
      "tensor(3.7004, device='cuda:0')\n",
      "tensor(3.4973, device='cuda:0')\n",
      "tensor(3.6191, device='cuda:0')\n",
      "tensor(3.7042, device='cuda:0')\n",
      "tensor(4.1554, device='cuda:0')\n",
      "tensor(4.3474, device='cuda:0')\n",
      "tensor(4.5268, device='cuda:0')\n",
      "tensor(4.4146, device='cuda:0')\n",
      "tensor(4.9039, device='cuda:0')\n",
      "tensor(5.0398, device='cuda:0')\n",
      "tensor(5.1164, device='cuda:0')\n",
      "tensor(4.6888, device='cuda:0')\n",
      "tensor(4.4975, device='cuda:0')\n",
      "tensor(4.5211, device='cuda:0')\n",
      "tensor(4.2342, device='cuda:0')\n",
      "tensor(4.1521, device='cuda:0')\n",
      "tensor(3.6236, device='cuda:0')\n",
      "tensor(3.5369, device='cuda:0')\n",
      "tensor(3.6848, device='cuda:0')\n",
      "tensor(3.6378, device='cuda:0')\n",
      "tensor(3.6524, device='cuda:0')\n",
      "tensor(3.7494, device='cuda:0')\n",
      "tensor(3.8932, device='cuda:0')\n",
      "tensor(4.0800, device='cuda:0')\n",
      "tensor(4.1377, device='cuda:0')\n",
      "tensor(3.9946, device='cuda:0')\n",
      "tensor(4.2412, device='cuda:0')\n",
      "tensor(4.3970, device='cuda:0')\n",
      "tensor(4.8008, device='cuda:0')\n",
      "tensor(4.9210, device='cuda:0')\n",
      "tensor(4.6664, device='cuda:0')\n",
      "tensor(4.7533, device='cuda:0')\n",
      "tensor(4.8508, device='cuda:0')\n",
      "tensor(5.1838, device='cuda:0')\n",
      "tensor(5.0443, device='cuda:0')\n",
      "tensor(4.8926, device='cuda:0')\n",
      "tensor(4.5593, device='cuda:0')\n",
      "tensor(4.4876, device='cuda:0')\n",
      "tensor(4.5742, device='cuda:0')\n",
      "tensor(4.4483, device='cuda:0')\n",
      "tensor(4.2613, device='cuda:0')\n",
      "tensor(3.9339, device='cuda:0')\n",
      "tensor(3.8698, device='cuda:0')\n",
      "tensor(3.9565, device='cuda:0')\n",
      "tensor(4.2167, device='cuda:0')\n",
      "tensor(4.2346, device='cuda:0')\n",
      "tensor(4.0495, device='cuda:0')\n",
      "tensor(3.9985, device='cuda:0')\n",
      "tensor(3.9368, device='cuda:0')\n",
      "tensor(4.0416, device='cuda:0')\n",
      "tensor(3.9637, device='cuda:0')\n",
      "tensor(3.9770, device='cuda:0')\n",
      "tensor(3.7891, device='cuda:0')\n",
      "tensor(3.7029, device='cuda:0')\n",
      "tensor(3.6771, device='cuda:0')\n",
      "tensor(3.8416, device='cuda:0')\n",
      "tensor(4.1807, device='cuda:0')\n",
      "tensor(4.1020, device='cuda:0')\n",
      "tensor(4.4276, device='cuda:0')\n",
      "tensor(4.3779, device='cuda:0')\n",
      "tensor(4.7130, device='cuda:0')\n",
      "tensor(5.0088, device='cuda:0')\n",
      "tensor(5.1942, device='cuda:0')\n",
      "tensor(5.0624, device='cuda:0')\n",
      "tensor(4.7491, device='cuda:0')\n",
      "tensor(4.4134, device='cuda:0')\n",
      "tensor(4.3279, device='cuda:0')\n",
      "tensor(4.3943, device='cuda:0')\n",
      "tensor(4.2467, device='cuda:0')\n",
      "tensor(4.1943, device='cuda:0')\n",
      "tensor(3.8334, device='cuda:0')\n",
      "tensor(3.8847, device='cuda:0')\n",
      "tensor(3.6731, device='cuda:0')\n",
      "tensor(3.8660, device='cuda:0')\n",
      "tensor(4.3144, device='cuda:0')\n",
      "tensor(4.2305, device='cuda:0')\n",
      "tensor(4.5126, device='cuda:0')\n",
      "tensor(4.7430, device='cuda:0')\n",
      "tensor(4.9902, device='cuda:0')\n",
      "tensor(5.1082, device='cuda:0')\n",
      "tensor(5.3346, device='cuda:0')\n",
      "tensor(5.4140, device='cuda:0')\n",
      "tensor(4.8056, device='cuda:0')\n",
      "tensor(4.5862, device='cuda:0')\n",
      "tensor(4.1290, device='cuda:0')\n",
      "tensor(4.2642, device='cuda:0')\n",
      "tensor(4.6035, device='cuda:0')\n",
      "tensor(4.6107, device='cuda:0')\n",
      "tensor(4.4818, device='cuda:0')\n",
      "tensor(4.6456, device='cuda:0')\n",
      "tensor(4.8320, device='cuda:0')\n",
      "tensor(4.7118, device='cuda:0')\n",
      "tensor(4.7255, device='cuda:0')\n",
      "tensor(4.5816, device='cuda:0')\n",
      "tensor(4.2865, device='cuda:0')\n",
      "tensor(4.1351, device='cuda:0')\n",
      "tensor(4.0183, device='cuda:0')\n",
      "tensor(3.9903, device='cuda:0')\n",
      "tensor(4.1534, device='cuda:0')\n",
      "tensor(4.3811, device='cuda:0')\n",
      "tensor(4.2908, device='cuda:0')\n",
      "tensor(4.3138, device='cuda:0')\n",
      "tensor(4.3921, device='cuda:0')\n",
      "tensor(4.3849, device='cuda:0')\n",
      "tensor(4.4575, device='cuda:0')\n",
      "tensor(4.5081, device='cuda:0')\n",
      "tensor(4.4074, device='cuda:0')\n",
      "tensor(4.0683, device='cuda:0')\n",
      "tensor(3.9785, device='cuda:0')\n",
      "tensor(3.8817, device='cuda:0')\n",
      "tensor(3.9423, device='cuda:0')\n",
      "tensor(3.9155, device='cuda:0')\n",
      "tensor(4.0247, device='cuda:0')\n",
      "tensor(4.9705, device='cuda:0')\n",
      "tensor(4.8728, device='cuda:0')\n",
      "tensor(4.9234, device='cuda:0')\n",
      "tensor(4.8925, device='cuda:0')\n",
      "tensor(5.1039, device='cuda:0')\n",
      "tensor(5.1264, device='cuda:0')\n",
      "tensor(5.0363, device='cuda:0')\n",
      "tensor(4.6528, device='cuda:0')\n",
      "tensor(4.6796, device='cuda:0')\n",
      "tensor(4.5985, device='cuda:0')\n",
      "tensor(4.5002, device='cuda:0')\n",
      "tensor(4.8853, device='cuda:0')\n",
      "tensor(4.7428, device='cuda:0')\n",
      "tensor(4.7122, device='cuda:0')\n",
      "tensor(4.9440, device='cuda:0')\n",
      "tensor(5.2615, device='cuda:0')\n",
      "tensor(5.2016, device='cuda:0')\n",
      "tensor(5.1254, device='cuda:0')\n",
      "tensor(5.2581, device='cuda:0')\n",
      "tensor(5.1915, device='cuda:0')\n",
      "tensor(5.1353, device='cuda:0')\n",
      "tensor(5.2024, device='cuda:0')\n",
      "tensor(5.4688, device='cuda:0')\n",
      "tensor(5.1156, device='cuda:0')\n",
      "tensor(4.9862, device='cuda:0')\n",
      "tensor(5.0188, device='cuda:0')\n",
      "tensor(4.8629, device='cuda:0')\n",
      "tensor(4.5411, device='cuda:0')\n",
      "tensor(4.6053, device='cuda:0')\n",
      "tensor(4.4100, device='cuda:0')\n",
      "tensor(4.3212, device='cuda:0')\n",
      "tensor(4.4184, device='cuda:0')\n",
      "tensor(4.4062, device='cuda:0')\n",
      "tensor(4.0336, device='cuda:0')\n",
      "tensor(4.1121, device='cuda:0')\n",
      "tensor(3.9096, device='cuda:0')\n",
      "tensor(3.8273, device='cuda:0')\n",
      "tensor(3.7103, device='cuda:0')\n",
      "tensor(3.8817, device='cuda:0')\n",
      "tensor(3.8141, device='cuda:0')\n",
      "tensor(3.8975, device='cuda:0')\n",
      "tensor(4.0737, device='cuda:0')\n",
      "tensor(4.0177, device='cuda:0')\n",
      "tensor(4.1041, device='cuda:0')\n",
      "tensor(4.1365, device='cuda:0')\n",
      "tensor(4.1533, device='cuda:0')\n",
      "tensor(4.1247, device='cuda:0')\n",
      "tensor(4.0745, device='cuda:0')\n",
      "tensor(4.1276, device='cuda:0')\n",
      "tensor(4.1821, device='cuda:0')\n",
      "tensor(4.2180, device='cuda:0')\n",
      "tensor(4.3744, device='cuda:0')\n",
      "tensor(4.4839, device='cuda:0')\n",
      "tensor(4.5177, device='cuda:0')\n",
      "tensor(4.3444, device='cuda:0')\n",
      "tensor(4.3472, device='cuda:0')\n",
      "tensor(4.1842, device='cuda:0')\n",
      "tensor(4.0149, device='cuda:0')\n",
      "tensor(3.9198, device='cuda:0')\n",
      "tensor(4.3778, device='cuda:0')\n",
      "tensor(4.3463, device='cuda:0')\n",
      "tensor(4.4343, device='cuda:0')\n",
      "tensor(4.3887, device='cuda:0')\n",
      "tensor(4.3557, device='cuda:0')\n",
      "tensor(4.4186, device='cuda:0')\n",
      "tensor(4.2940, device='cuda:0')\n",
      "tensor(4.6235, device='cuda:0')\n",
      "tensor(4.5100, device='cuda:0')\n",
      "tensor(4.7221, device='cuda:0')\n",
      "tensor(4.7387, device='cuda:0')\n",
      "tensor(4.8142, device='cuda:0')\n",
      "tensor(4.8309, device='cuda:0')\n",
      "tensor(4.8701, device='cuda:0')\n",
      "tensor(4.6461, device='cuda:0')\n",
      "tensor(4.3237, device='cuda:0')\n",
      "tensor(4.0836, device='cuda:0')\n",
      "tensor(3.8635, device='cuda:0')\n",
      "tensor(3.5777, device='cuda:0')\n",
      "tensor(3.3668, device='cuda:0')\n",
      "tensor(3.3785, device='cuda:0')\n",
      "tensor(3.4683, device='cuda:0')\n",
      "tensor(3.3206, device='cuda:0')\n",
      "tensor(3.2479, device='cuda:0')\n",
      "tensor(3.3893, device='cuda:0')\n",
      "tensor(3.4546, device='cuda:0')\n",
      "tensor(3.4920, device='cuda:0')\n",
      "tensor(3.3631, device='cuda:0')\n",
      "tensor(3.2045, device='cuda:0')\n",
      "tensor(3.7915, device='cuda:0')\n",
      "tensor(3.8167, device='cuda:0')\n",
      "tensor(4.5686, device='cuda:0')\n",
      "tensor(4.4264, device='cuda:0')\n",
      "tensor(4.3516, device='cuda:0')\n",
      "tensor(4.3611, device='cuda:0')\n",
      "tensor(4.7558, device='cuda:0')\n",
      "tensor(4.8713, device='cuda:0')\n",
      "tensor(4.5618, device='cuda:0')\n",
      "tensor(4.5423, device='cuda:0')\n",
      "tensor(4.1200, device='cuda:0')\n",
      "tensor(4.0513, device='cuda:0')\n",
      "tensor(4.1533, device='cuda:0')\n",
      "tensor(4.3890, device='cuda:0')\n",
      "tensor(4.0055, device='cuda:0')\n",
      "tensor(4.4314, device='cuda:0')\n",
      "tensor(4.5362, device='cuda:0')\n",
      "tensor(5.1920, device='cuda:0')\n",
      "tensor(5.1451, device='cuda:0')\n",
      "tensor(5.5626, device='cuda:0')\n",
      "tensor(5.2043, device='cuda:0')\n",
      "tensor(4.9436, device='cuda:0')\n",
      "tensor(4.4914, device='cuda:0')\n",
      "tensor(4.3161, device='cuda:0')\n",
      "tensor(4.3621, device='cuda:0')\n",
      "tensor(4.1949, device='cuda:0')\n",
      "tensor(4.2181, device='cuda:0')\n",
      "tensor(4.2217, device='cuda:0')\n",
      "tensor(4.6101, device='cuda:0')\n",
      "tensor(4.3304, device='cuda:0')\n",
      "tensor(5.2003, device='cuda:0')\n",
      "tensor(5.3697, device='cuda:0')\n",
      "tensor(5.3648, device='cuda:0')\n",
      "tensor(5.3038, device='cuda:0')\n",
      "tensor(4.9546, device='cuda:0')\n",
      "tensor(4.9357, device='cuda:0')\n",
      "tensor(4.6831, device='cuda:0')\n",
      "tensor(4.6245, device='cuda:0')\n",
      "tensor(4.6295, device='cuda:0')\n",
      "tensor(4.4865, device='cuda:0')\n",
      "tensor(4.0071, device='cuda:0')\n",
      "tensor(3.8815, device='cuda:0')\n",
      "tensor(4.5879, device='cuda:0')\n",
      "tensor(4.3760, device='cuda:0')\n",
      "tensor(4.7202, device='cuda:0')\n",
      "tensor(4.7559, device='cuda:0')\n",
      "tensor(4.2874, device='cuda:0')\n",
      "tensor(4.1497, device='cuda:0')\n",
      "tensor(4.2138, device='cuda:0')\n",
      "tensor(4.1734, device='cuda:0')\n",
      "tensor(3.7137, device='cuda:0')\n",
      "tensor(3.6970, device='cuda:0')\n",
      "tensor(3.5849, device='cuda:0')\n",
      "tensor(3.6080, device='cuda:0')\n",
      "tensor(3.8916, device='cuda:0')\n",
      "tensor(4.2498, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2432, device='cuda:0')\n",
      "tensor(4.2990, device='cuda:0')\n",
      "tensor(4.5107, device='cuda:0')\n",
      "tensor(4.7245, device='cuda:0')\n",
      "tensor(4.4231, device='cuda:0')\n",
      "tensor(4.2456, device='cuda:0')\n",
      "tensor(4.0126, device='cuda:0')\n",
      "tensor(3.8619, device='cuda:0')\n",
      "tensor(3.6899, device='cuda:0')\n",
      "tensor(3.7171, device='cuda:0')\n",
      "tensor(3.6016, device='cuda:0')\n",
      "tensor(3.5939, device='cuda:0')\n",
      "tensor(3.4055, device='cuda:0')\n",
      "tensor(3.5775, device='cuda:0')\n",
      "tensor(3.6358, device='cuda:0')\n",
      "tensor(3.4863, device='cuda:0')\n",
      "tensor(3.6545, device='cuda:0')\n",
      "tensor(3.6433, device='cuda:0')\n",
      "tensor(3.7271, device='cuda:0')\n",
      "tensor(3.9715, device='cuda:0')\n",
      "tensor(4.0630, device='cuda:0')\n",
      "tensor(4.2523, device='cuda:0')\n",
      "tensor(4.1224, device='cuda:0')\n",
      "tensor(3.9545, device='cuda:0')\n",
      "tensor(4.2055, device='cuda:0')\n",
      "tensor(4.1193, device='cuda:0')\n",
      "tensor(3.8432, device='cuda:0')\n",
      "tensor(3.8687, device='cuda:0')\n",
      "tensor(4.3404, device='cuda:0')\n",
      "tensor(3.8748, device='cuda:0')\n",
      "tensor(3.9291, device='cuda:0')\n",
      "tensor(3.8787, device='cuda:0')\n",
      "tensor(3.6738, device='cuda:0')\n",
      "tensor(3.4590, device='cuda:0')\n",
      "tensor(3.8740, device='cuda:0')\n",
      "tensor(4.0778, device='cuda:0')\n",
      "tensor(4.0604, device='cuda:0')\n",
      "tensor(4.5476, device='cuda:0')\n",
      "tensor(4.6254, device='cuda:0')\n",
      "tensor(5.9266, device='cuda:0')\n",
      "tensor(5.7536, device='cuda:0')\n",
      "tensor(5.3410, device='cuda:0')\n",
      "tensor(5.3625, device='cuda:0')\n",
      "tensor(5.2455, device='cuda:0')\n",
      "tensor(5.1837, device='cuda:0')\n",
      "tensor(4.8579, device='cuda:0')\n",
      "tensor(4.8309, device='cuda:0')\n",
      "tensor(4.5079, device='cuda:0')\n",
      "tensor(4.3408, device='cuda:0')\n",
      "tensor(4.0686, device='cuda:0')\n",
      "tensor(3.8614, device='cuda:0')\n",
      "tensor(3.8845, device='cuda:0')\n",
      "tensor(4.1980, device='cuda:0')\n",
      "tensor(4.3277, device='cuda:0')\n",
      "tensor(4.5697, device='cuda:0')\n",
      "tensor(4.6905, device='cuda:0')\n",
      "tensor(4.5732, device='cuda:0')\n",
      "tensor(4.4510, device='cuda:0')\n",
      "tensor(4.2982, device='cuda:0')\n",
      "tensor(4.1680, device='cuda:0')\n",
      "tensor(3.9375, device='cuda:0')\n",
      "tensor(3.8393, device='cuda:0')\n",
      "tensor(3.5065, device='cuda:0')\n",
      "tensor(3.4126, device='cuda:0')\n",
      "tensor(3.2429, device='cuda:0')\n",
      "tensor(3.3233, device='cuda:0')\n",
      "tensor(3.2593, device='cuda:0')\n",
      "tensor(3.5587, device='cuda:0')\n",
      "tensor(3.5086, device='cuda:0')\n",
      "tensor(3.7730, device='cuda:0')\n",
      "tensor(3.8675, device='cuda:0')\n",
      "tensor(3.9775, device='cuda:0')\n",
      "tensor(4.0835, device='cuda:0')\n",
      "tensor(4.1034, device='cuda:0')\n",
      "tensor(4.8473, device='cuda:0')\n",
      "tensor(4.6983, device='cuda:0')\n",
      "tensor(5.2530, device='cuda:0')\n",
      "tensor(5.4213, device='cuda:0')\n",
      "tensor(5.5302, device='cuda:0')\n",
      "tensor(5.7841, device='cuda:0')\n",
      "tensor(5.6416, device='cuda:0')\n",
      "tensor(5.5134, device='cuda:0')\n",
      "tensor(5.3050, device='cuda:0')\n",
      "tensor(5.4726, device='cuda:0')\n",
      "tensor(4.9941, device='cuda:0')\n",
      "tensor(5.1543, device='cuda:0')\n",
      "tensor(4.8107, device='cuda:0')\n",
      "tensor(4.1142, device='cuda:0')\n",
      "tensor(4.5429, device='cuda:0')\n",
      "tensor(4.6197, device='cuda:0')\n",
      "tensor(4.5916, device='cuda:0')\n",
      "tensor(4.5601, device='cuda:0')\n",
      "tensor(4.9367, device='cuda:0')\n",
      "tensor(4.9228, device='cuda:0')\n",
      "tensor(4.9098, device='cuda:0')\n",
      "tensor(4.6793, device='cuda:0')\n",
      "tensor(4.1682, device='cuda:0')\n",
      "tensor(4.2260, device='cuda:0')\n",
      "tensor(4.3354, device='cuda:0')\n",
      "tensor(4.2702, device='cuda:0')\n",
      "tensor(4.3505, device='cuda:0')\n",
      "tensor(4.2731, device='cuda:0')\n",
      "tensor(4.5315, device='cuda:0')\n",
      "tensor(4.4864, device='cuda:0')\n",
      "epochwise results: {'perplexity_1517': tensor(77.8180)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a904ce18f17f4cf589e4401d1a6083f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 21:58:48 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "07/13/2022 21:58:48 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "07/13/2022 21:58:48 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/13/2022 21:58:48 - INFO - __main__ -     Num examples = 374\n",
      "07/13/2022 21:58:48 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2941267ab94678bb88fac72d3e3a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.4024, device='cuda:0')\n",
      "tensor(6.3187, device='cuda:0')\n",
      "tensor(6.0638, device='cuda:0')\n",
      "tensor(5.9390, device='cuda:0')\n",
      "tensor(5.9281, device='cuda:0')\n",
      "tensor(5.9150, device='cuda:0')\n",
      "tensor(5.7000, device='cuda:0')\n",
      "tensor(5.4457, device='cuda:0')\n",
      "tensor(5.1507, device='cuda:0')\n",
      "tensor(4.8855, device='cuda:0')\n",
      "tensor(4.5385, device='cuda:0')\n",
      "tensor(4.5810, device='cuda:0')\n",
      "tensor(4.5072, device='cuda:0')\n",
      "tensor(4.7010, device='cuda:0')\n",
      "tensor(4.5287, device='cuda:0')\n",
      "tensor(4.6625, device='cuda:0')\n",
      "tensor(4.7267, device='cuda:0')\n",
      "tensor(5.3015, device='cuda:0')\n",
      "tensor(5.6220, device='cuda:0')\n",
      "tensor(5.8264, device='cuda:0')\n",
      "tensor(5.7183, device='cuda:0')\n",
      "tensor(6.2331, device='cuda:0')\n",
      "tensor(6.4989, device='cuda:0')\n",
      "tensor(6.4887, device='cuda:0')\n",
      "tensor(5.7510, device='cuda:0')\n",
      "tensor(5.4821, device='cuda:0')\n",
      "tensor(5.6137, device='cuda:0')\n",
      "tensor(5.1073, device='cuda:0')\n",
      "tensor(4.9939, device='cuda:0')\n",
      "tensor(4.2749, device='cuda:0')\n",
      "tensor(4.2767, device='cuda:0')\n",
      "tensor(4.5401, device='cuda:0')\n",
      "tensor(4.8033, device='cuda:0')\n",
      "tensor(4.7824, device='cuda:0')\n",
      "tensor(4.8455, device='cuda:0')\n",
      "tensor(5.0547, device='cuda:0')\n",
      "tensor(5.2578, device='cuda:0')\n",
      "tensor(5.3300, device='cuda:0')\n",
      "tensor(5.0112, device='cuda:0')\n",
      "tensor(5.2477, device='cuda:0')\n",
      "tensor(5.4230, device='cuda:0')\n",
      "tensor(5.8879, device='cuda:0')\n",
      "tensor(6.1202, device='cuda:0')\n",
      "tensor(5.6754, device='cuda:0')\n",
      "tensor(5.8589, device='cuda:0')\n",
      "tensor(6.1207, device='cuda:0')\n",
      "tensor(6.7290, device='cuda:0')\n",
      "tensor(6.4057, device='cuda:0')\n",
      "tensor(6.1098, device='cuda:0')\n",
      "tensor(5.9120, device='cuda:0')\n",
      "tensor(5.8417, device='cuda:0')\n",
      "tensor(6.0795, device='cuda:0')\n",
      "tensor(5.7119, device='cuda:0')\n",
      "tensor(5.4087, device='cuda:0')\n",
      "tensor(5.0844, device='cuda:0')\n",
      "tensor(5.0011, device='cuda:0')\n",
      "tensor(5.0053, device='cuda:0')\n",
      "tensor(5.3291, device='cuda:0')\n",
      "tensor(5.2859, device='cuda:0')\n",
      "tensor(5.0714, device='cuda:0')\n",
      "tensor(5.0121, device='cuda:0')\n",
      "tensor(4.8822, device='cuda:0')\n",
      "tensor(5.0780, device='cuda:0')\n",
      "tensor(4.9757, device='cuda:0')\n",
      "tensor(4.9716, device='cuda:0')\n",
      "tensor(4.7880, device='cuda:0')\n",
      "tensor(4.6868, device='cuda:0')\n",
      "tensor(4.6472, device='cuda:0')\n",
      "tensor(4.8902, device='cuda:0')\n",
      "tensor(5.3255, device='cuda:0')\n",
      "tensor(5.2124, device='cuda:0')\n",
      "tensor(5.7433, device='cuda:0')\n",
      "tensor(5.9305, device='cuda:0')\n",
      "tensor(6.3378, device='cuda:0')\n",
      "tensor(6.7806, device='cuda:0')\n",
      "tensor(6.9707, device='cuda:0')\n",
      "tensor(6.8786, device='cuda:0')\n",
      "tensor(6.3438, device='cuda:0')\n",
      "tensor(5.8941, device='cuda:0')\n",
      "tensor(5.7707, device='cuda:0')\n",
      "tensor(5.7930, device='cuda:0')\n",
      "tensor(5.4839, device='cuda:0')\n",
      "tensor(5.4110, device='cuda:0')\n",
      "tensor(5.0146, device='cuda:0')\n",
      "tensor(5.0476, device='cuda:0')\n",
      "tensor(4.7516, device='cuda:0')\n",
      "tensor(5.0046, device='cuda:0')\n",
      "tensor(5.8819, device='cuda:0')\n",
      "tensor(5.6923, device='cuda:0')\n",
      "tensor(6.1355, device='cuda:0')\n",
      "tensor(6.4407, device='cuda:0')\n",
      "tensor(6.7584, device='cuda:0')\n",
      "tensor(6.9330, device='cuda:0')\n",
      "tensor(7.1826, device='cuda:0')\n",
      "tensor(7.2908, device='cuda:0')\n",
      "tensor(6.1382, device='cuda:0')\n",
      "tensor(5.7786, device='cuda:0')\n",
      "tensor(5.2010, device='cuda:0')\n",
      "tensor(5.5106, device='cuda:0')\n",
      "tensor(6.0390, device='cuda:0')\n",
      "tensor(6.0646, device='cuda:0')\n",
      "tensor(5.8429, device='cuda:0')\n",
      "tensor(6.1449, device='cuda:0')\n",
      "tensor(6.4131, device='cuda:0')\n",
      "tensor(6.3282, device='cuda:0')\n",
      "tensor(6.4766, device='cuda:0')\n",
      "tensor(6.1231, device='cuda:0')\n",
      "tensor(5.6912, device='cuda:0')\n",
      "tensor(5.4687, device='cuda:0')\n",
      "tensor(5.3057, device='cuda:0')\n",
      "tensor(5.3052, device='cuda:0')\n",
      "tensor(5.4206, device='cuda:0')\n",
      "tensor(5.6368, device='cuda:0')\n",
      "tensor(5.5074, device='cuda:0')\n",
      "tensor(5.5921, device='cuda:0')\n",
      "tensor(5.6749, device='cuda:0')\n",
      "tensor(5.6513, device='cuda:0')\n",
      "tensor(5.6387, device='cuda:0')\n",
      "tensor(5.8563, device='cuda:0')\n",
      "tensor(5.4440, device='cuda:0')\n",
      "tensor(4.9325, device='cuda:0')\n",
      "tensor(4.8790, device='cuda:0')\n",
      "tensor(4.8502, device='cuda:0')\n",
      "tensor(4.8363, device='cuda:0')\n",
      "tensor(5.1617, device='cuda:0')\n",
      "tensor(5.2536, device='cuda:0')\n",
      "tensor(6.8288, device='cuda:0')\n",
      "tensor(6.6722, device='cuda:0')\n",
      "tensor(6.8048, device='cuda:0')\n",
      "tensor(6.7210, device='cuda:0')\n",
      "tensor(6.9545, device='cuda:0')\n",
      "tensor(7.0008, device='cuda:0')\n",
      "tensor(6.8160, device='cuda:0')\n",
      "tensor(6.2809, device='cuda:0')\n",
      "tensor(6.2062, device='cuda:0')\n",
      "tensor(6.0666, device='cuda:0')\n",
      "tensor(5.8578, device='cuda:0')\n",
      "tensor(6.2964, device='cuda:0')\n",
      "tensor(6.0705, device='cuda:0')\n",
      "tensor(6.0786, device='cuda:0')\n",
      "tensor(6.4009, device='cuda:0')\n",
      "tensor(6.7551, device='cuda:0')\n",
      "tensor(6.8102, device='cuda:0')\n",
      "tensor(6.7072, device='cuda:0')\n",
      "tensor(6.9129, device='cuda:0')\n",
      "tensor(6.7464, device='cuda:0')\n",
      "tensor(6.5716, device='cuda:0')\n",
      "tensor(6.4838, device='cuda:0')\n",
      "tensor(6.9694, device='cuda:0')\n",
      "tensor(6.4588, device='cuda:0')\n",
      "tensor(6.3954, device='cuda:0')\n",
      "tensor(6.3258, device='cuda:0')\n",
      "tensor(6.0656, device='cuda:0')\n",
      "tensor(5.8934, device='cuda:0')\n",
      "tensor(5.9039, device='cuda:0')\n",
      "tensor(5.6591, device='cuda:0')\n",
      "tensor(5.5318, device='cuda:0')\n",
      "tensor(5.6385, device='cuda:0')\n",
      "tensor(5.6011, device='cuda:0')\n",
      "tensor(5.1000, device='cuda:0')\n",
      "tensor(5.1929, device='cuda:0')\n",
      "tensor(5.0692, device='cuda:0')\n",
      "tensor(5.0010, device='cuda:0')\n",
      "tensor(4.9071, device='cuda:0')\n",
      "tensor(5.1399, device='cuda:0')\n",
      "tensor(5.0193, device='cuda:0')\n",
      "tensor(5.0849, device='cuda:0')\n",
      "tensor(5.5130, device='cuda:0')\n",
      "tensor(5.4077, device='cuda:0')\n",
      "tensor(5.2675, device='cuda:0')\n",
      "tensor(5.2622, device='cuda:0')\n",
      "tensor(5.0546, device='cuda:0')\n",
      "tensor(5.1218, device='cuda:0')\n",
      "tensor(5.0334, device='cuda:0')\n",
      "tensor(5.0224, device='cuda:0')\n",
      "tensor(5.0495, device='cuda:0')\n",
      "tensor(5.1143, device='cuda:0')\n",
      "tensor(5.5480, device='cuda:0')\n",
      "tensor(5.6447, device='cuda:0')\n",
      "tensor(5.6159, device='cuda:0')\n",
      "tensor(5.4365, device='cuda:0')\n",
      "tensor(5.4052, device='cuda:0')\n",
      "tensor(5.3443, device='cuda:0')\n",
      "tensor(5.0803, device='cuda:0')\n",
      "tensor(4.7895, device='cuda:0')\n",
      "tensor(5.6252, device='cuda:0')\n",
      "tensor(5.6461, device='cuda:0')\n",
      "tensor(5.9704, device='cuda:0')\n",
      "tensor(5.9132, device='cuda:0')\n",
      "tensor(5.7740, device='cuda:0')\n",
      "tensor(6.0042, device='cuda:0')\n",
      "tensor(5.8521, device='cuda:0')\n",
      "tensor(6.2040, device='cuda:0')\n",
      "tensor(6.0346, device='cuda:0')\n",
      "tensor(6.3559, device='cuda:0')\n",
      "tensor(6.3150, device='cuda:0')\n",
      "tensor(6.3783, device='cuda:0')\n",
      "tensor(6.3383, device='cuda:0')\n",
      "tensor(6.3556, device='cuda:0')\n",
      "tensor(6.0692, device='cuda:0')\n",
      "tensor(5.6708, device='cuda:0')\n",
      "tensor(5.3200, device='cuda:0')\n",
      "tensor(4.9872, device='cuda:0')\n",
      "tensor(4.6336, device='cuda:0')\n",
      "tensor(4.3046, device='cuda:0')\n",
      "tensor(4.3133, device='cuda:0')\n",
      "tensor(4.3817, device='cuda:0')\n",
      "tensor(4.2437, device='cuda:0')\n",
      "tensor(4.0994, device='cuda:0')\n",
      "tensor(4.2899, device='cuda:0')\n",
      "tensor(4.3178, device='cuda:0')\n",
      "tensor(4.3559, device='cuda:0')\n",
      "tensor(4.2725, device='cuda:0')\n",
      "tensor(4.0784, device='cuda:0')\n",
      "tensor(4.9712, device='cuda:0')\n",
      "tensor(5.0151, device='cuda:0')\n",
      "tensor(5.9817, device='cuda:0')\n",
      "tensor(5.7681, device='cuda:0')\n",
      "tensor(5.6978, device='cuda:0')\n",
      "tensor(5.6768, device='cuda:0')\n",
      "tensor(6.2217, device='cuda:0')\n",
      "tensor(6.4317, device='cuda:0')\n",
      "tensor(6.0961, device='cuda:0')\n",
      "tensor(6.0129, device='cuda:0')\n",
      "tensor(5.5356, device='cuda:0')\n",
      "tensor(5.4208, device='cuda:0')\n",
      "tensor(5.4315, device='cuda:0')\n",
      "tensor(5.7147, device='cuda:0')\n",
      "tensor(5.2992, device='cuda:0')\n",
      "tensor(5.8251, device='cuda:0')\n",
      "tensor(5.8988, device='cuda:0')\n",
      "tensor(6.8513, device='cuda:0')\n",
      "tensor(6.7545, device='cuda:0')\n",
      "tensor(7.3168, device='cuda:0')\n",
      "tensor(6.7878, device='cuda:0')\n",
      "tensor(6.4371, device='cuda:0')\n",
      "tensor(5.8263, device='cuda:0')\n",
      "tensor(5.5483, device='cuda:0')\n",
      "tensor(5.5051, device='cuda:0')\n",
      "tensor(5.2758, device='cuda:0')\n",
      "tensor(5.2503, device='cuda:0')\n",
      "tensor(5.1564, device='cuda:0')\n",
      "tensor(5.5480, device='cuda:0')\n",
      "tensor(5.1628, device='cuda:0')\n",
      "tensor(6.4867, device='cuda:0')\n",
      "tensor(6.7141, device='cuda:0')\n",
      "tensor(6.7720, device='cuda:0')\n",
      "tensor(6.7327, device='cuda:0')\n",
      "tensor(6.2719, device='cuda:0')\n",
      "tensor(6.2802, device='cuda:0')\n",
      "tensor(5.8679, device='cuda:0')\n",
      "tensor(5.8394, device='cuda:0')\n",
      "tensor(5.9552, device='cuda:0')\n",
      "tensor(5.6503, device='cuda:0')\n",
      "tensor(5.0442, device='cuda:0')\n",
      "tensor(4.8466, device='cuda:0')\n",
      "tensor(5.9290, device='cuda:0')\n",
      "tensor(5.6963, device='cuda:0')\n",
      "tensor(6.0846, device='cuda:0')\n",
      "tensor(6.1766, device='cuda:0')\n",
      "tensor(5.4642, device='cuda:0')\n",
      "tensor(5.2702, device='cuda:0')\n",
      "tensor(5.3432, device='cuda:0')\n",
      "tensor(5.2321, device='cuda:0')\n",
      "tensor(4.6300, device='cuda:0')\n",
      "tensor(4.5841, device='cuda:0')\n",
      "tensor(4.4671, device='cuda:0')\n",
      "tensor(4.4648, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8976, device='cuda:0')\n",
      "tensor(5.2679, device='cuda:0')\n",
      "tensor(5.2177, device='cuda:0')\n",
      "tensor(5.3348, device='cuda:0')\n",
      "tensor(5.6389, device='cuda:0')\n",
      "tensor(5.8371, device='cuda:0')\n",
      "tensor(5.8072, device='cuda:0')\n",
      "tensor(5.7225, device='cuda:0')\n",
      "tensor(5.4352, device='cuda:0')\n",
      "tensor(5.1144, device='cuda:0')\n",
      "tensor(4.8142, device='cuda:0')\n",
      "tensor(4.8130, device='cuda:0')\n",
      "tensor(4.6227, device='cuda:0')\n",
      "tensor(4.6154, device='cuda:0')\n",
      "tensor(4.3248, device='cuda:0')\n",
      "tensor(4.5836, device='cuda:0')\n",
      "tensor(4.6984, device='cuda:0')\n",
      "tensor(4.5508, device='cuda:0')\n",
      "tensor(4.8518, device='cuda:0')\n",
      "tensor(4.7904, device='cuda:0')\n",
      "tensor(4.9759, device='cuda:0')\n",
      "tensor(5.2466, device='cuda:0')\n",
      "tensor(5.2716, device='cuda:0')\n",
      "tensor(5.5753, device='cuda:0')\n",
      "tensor(5.5265, device='cuda:0')\n",
      "tensor(5.2860, device='cuda:0')\n",
      "tensor(5.5595, device='cuda:0')\n",
      "tensor(5.2716, device='cuda:0')\n",
      "tensor(4.8441, device='cuda:0')\n",
      "tensor(4.8588, device='cuda:0')\n",
      "tensor(5.5173, device='cuda:0')\n",
      "tensor(4.8760, device='cuda:0')\n",
      "tensor(4.9717, device='cuda:0')\n",
      "tensor(5.0594, device='cuda:0')\n",
      "tensor(4.8314, device='cuda:0')\n",
      "tensor(4.7039, device='cuda:0')\n",
      "tensor(5.2169, device='cuda:0')\n",
      "tensor(5.4416, device='cuda:0')\n",
      "tensor(5.2832, device='cuda:0')\n",
      "tensor(5.9103, device='cuda:0')\n",
      "tensor(6.0323, device='cuda:0')\n",
      "tensor(7.9656, device='cuda:0')\n",
      "tensor(7.8014, device='cuda:0')\n",
      "tensor(7.3017, device='cuda:0')\n",
      "tensor(7.2283, device='cuda:0')\n",
      "tensor(7.0964, device='cuda:0')\n",
      "tensor(6.9265, device='cuda:0')\n",
      "tensor(6.5330, device='cuda:0')\n",
      "tensor(6.4258, device='cuda:0')\n",
      "tensor(5.8894, device='cuda:0')\n",
      "tensor(5.6856, device='cuda:0')\n",
      "tensor(5.1893, device='cuda:0')\n",
      "tensor(4.9696, device='cuda:0')\n",
      "tensor(4.9629, device='cuda:0')\n",
      "tensor(5.4297, device='cuda:0')\n",
      "tensor(5.5035, device='cuda:0')\n",
      "tensor(5.9324, device='cuda:0')\n",
      "tensor(6.1381, device='cuda:0')\n",
      "tensor(5.9063, device='cuda:0')\n",
      "tensor(5.7554, device='cuda:0')\n",
      "tensor(5.5944, device='cuda:0')\n",
      "tensor(5.4675, device='cuda:0')\n",
      "tensor(5.0431, device='cuda:0')\n",
      "tensor(4.8408, device='cuda:0')\n",
      "tensor(4.3783, device='cuda:0')\n",
      "tensor(4.2190, device='cuda:0')\n",
      "tensor(3.9183, device='cuda:0')\n",
      "tensor(4.0781, device='cuda:0')\n",
      "tensor(3.9504, device='cuda:0')\n",
      "tensor(4.3360, device='cuda:0')\n",
      "tensor(4.4472, device='cuda:0')\n",
      "tensor(4.8045, device='cuda:0')\n",
      "tensor(4.9352, device='cuda:0')\n",
      "tensor(5.0939, device='cuda:0')\n",
      "tensor(5.1672, device='cuda:0')\n",
      "tensor(5.3037, device='cuda:0')\n",
      "tensor(6.6237, device='cuda:0')\n",
      "tensor(6.2163, device='cuda:0')\n",
      "tensor(7.1556, device='cuda:0')\n",
      "tensor(7.4459, device='cuda:0')\n",
      "tensor(7.6526, device='cuda:0')\n",
      "tensor(7.9722, device='cuda:0')\n",
      "tensor(7.6619, device='cuda:0')\n",
      "tensor(7.3550, device='cuda:0')\n",
      "tensor(7.2373, device='cuda:0')\n",
      "tensor(7.4678, device='cuda:0')\n",
      "tensor(6.6508, device='cuda:0')\n",
      "tensor(6.7636, device='cuda:0')\n",
      "tensor(6.1438, device='cuda:0')\n",
      "tensor(5.3701, device='cuda:0')\n",
      "tensor(5.9513, device='cuda:0')\n",
      "tensor(6.0220, device='cuda:0')\n",
      "tensor(5.9195, device='cuda:0')\n",
      "tensor(5.8755, device='cuda:0')\n",
      "tensor(6.3313, device='cuda:0')\n",
      "tensor(6.3580, device='cuda:0')\n",
      "tensor(6.4101, device='cuda:0')\n",
      "tensor(6.0050, device='cuda:0')\n",
      "tensor(5.4071, device='cuda:0')\n",
      "tensor(5.5557, device='cuda:0')\n",
      "tensor(5.5756, device='cuda:0')\n",
      "tensor(5.5581, device='cuda:0')\n",
      "tensor(5.4473, device='cuda:0')\n",
      "tensor(5.3787, device='cuda:0')\n",
      "tensor(5.7768, device='cuda:0')\n",
      "tensor(5.6722, device='cuda:0')\n",
      "epochwise results: {'perplexity_3034': tensor(279.4250)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e913cb5fd4745c793decc10f75e6a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 21:59:32 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-3500\n",
      "07/13/2022 21:59:34 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-3500\n",
      "07/13/2022 22:01:00 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "07/13/2022 22:01:00 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "07/13/2022 22:01:00 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/13/2022 22:01:00 - INFO - __main__ -     Num examples = 374\n",
      "07/13/2022 22:01:00 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6ea7ab06524bd5b0eb62847a6e794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9488, device='cuda:0')\n",
      "tensor(6.8646, device='cuda:0')\n",
      "tensor(6.6357, device='cuda:0')\n",
      "tensor(6.5281, device='cuda:0')\n",
      "tensor(6.4546, device='cuda:0')\n",
      "tensor(6.4136, device='cuda:0')\n",
      "tensor(6.2062, device='cuda:0')\n",
      "tensor(5.9349, device='cuda:0')\n",
      "tensor(5.6108, device='cuda:0')\n",
      "tensor(5.3043, device='cuda:0')\n",
      "tensor(4.9152, device='cuda:0')\n",
      "tensor(4.9561, device='cuda:0')\n",
      "tensor(4.8829, device='cuda:0')\n",
      "tensor(5.1140, device='cuda:0')\n",
      "tensor(4.9257, device='cuda:0')\n",
      "tensor(5.0910, device='cuda:0')\n",
      "tensor(5.1648, device='cuda:0')\n",
      "tensor(5.7676, device='cuda:0')\n",
      "tensor(6.0859, device='cuda:0')\n",
      "tensor(6.2453, device='cuda:0')\n",
      "tensor(6.1721, device='cuda:0')\n",
      "tensor(6.6956, device='cuda:0')\n",
      "tensor(6.9967, device='cuda:0')\n",
      "tensor(6.9664, device='cuda:0')\n",
      "tensor(6.1503, device='cuda:0')\n",
      "tensor(5.8670, device='cuda:0')\n",
      "tensor(5.9903, device='cuda:0')\n",
      "tensor(5.4878, device='cuda:0')\n",
      "tensor(5.3147, device='cuda:0')\n",
      "tensor(4.5113, device='cuda:0')\n",
      "tensor(4.5561, device='cuda:0')\n",
      "tensor(4.8665, device='cuda:0')\n",
      "tensor(5.1168, device='cuda:0')\n",
      "tensor(5.1222, device='cuda:0')\n",
      "tensor(5.2116, device='cuda:0')\n",
      "tensor(5.4294, device='cuda:0')\n",
      "tensor(5.6771, device='cuda:0')\n",
      "tensor(5.7930, device='cuda:0')\n",
      "tensor(5.4481, device='cuda:0')\n",
      "tensor(5.6856, device='cuda:0')\n",
      "tensor(5.8999, device='cuda:0')\n",
      "tensor(6.4087, device='cuda:0')\n",
      "tensor(6.6015, device='cuda:0')\n",
      "tensor(6.0891, device='cuda:0')\n",
      "tensor(6.2854, device='cuda:0')\n",
      "tensor(6.5698, device='cuda:0')\n",
      "tensor(7.3333, device='cuda:0')\n",
      "tensor(6.9829, device='cuda:0')\n",
      "tensor(6.6845, device='cuda:0')\n",
      "tensor(6.5332, device='cuda:0')\n",
      "tensor(6.4549, device='cuda:0')\n",
      "tensor(6.7272, device='cuda:0')\n",
      "tensor(6.2932, device='cuda:0')\n",
      "tensor(5.9502, device='cuda:0')\n",
      "tensor(5.5794, device='cuda:0')\n",
      "tensor(5.4739, device='cuda:0')\n",
      "tensor(5.4537, device='cuda:0')\n",
      "tensor(5.8753, device='cuda:0')\n",
      "tensor(5.8026, device='cuda:0')\n",
      "tensor(5.5112, device='cuda:0')\n",
      "tensor(5.4205, device='cuda:0')\n",
      "tensor(5.2512, device='cuda:0')\n",
      "tensor(5.4295, device='cuda:0')\n",
      "tensor(5.3398, device='cuda:0')\n",
      "tensor(5.3679, device='cuda:0')\n",
      "tensor(5.1740, device='cuda:0')\n",
      "tensor(5.1197, device='cuda:0')\n",
      "tensor(5.0718, device='cuda:0')\n",
      "tensor(5.3066, device='cuda:0')\n",
      "tensor(5.7902, device='cuda:0')\n",
      "tensor(5.6838, device='cuda:0')\n",
      "tensor(6.2685, device='cuda:0')\n",
      "tensor(6.4794, device='cuda:0')\n",
      "tensor(6.9055, device='cuda:0')\n",
      "tensor(7.4187, device='cuda:0')\n",
      "tensor(7.5928, device='cuda:0')\n",
      "tensor(7.5014, device='cuda:0')\n",
      "tensor(6.9094, device='cuda:0')\n",
      "tensor(6.4262, device='cuda:0')\n",
      "tensor(6.2456, device='cuda:0')\n",
      "tensor(6.2683, device='cuda:0')\n",
      "tensor(5.9966, device='cuda:0')\n",
      "tensor(5.9482, device='cuda:0')\n",
      "tensor(5.5438, device='cuda:0')\n",
      "tensor(5.5693, device='cuda:0')\n",
      "tensor(5.2357, device='cuda:0')\n",
      "tensor(5.4803, device='cuda:0')\n",
      "tensor(6.4230, device='cuda:0')\n",
      "tensor(6.2058, device='cuda:0')\n",
      "tensor(6.7160, device='cuda:0')\n",
      "tensor(7.0260, device='cuda:0')\n",
      "tensor(7.3640, device='cuda:0')\n",
      "tensor(7.5266, device='cuda:0')\n",
      "tensor(7.7809, device='cuda:0')\n",
      "tensor(7.9427, device='cuda:0')\n",
      "tensor(6.7101, device='cuda:0')\n",
      "tensor(6.3329, device='cuda:0')\n",
      "tensor(5.7230, device='cuda:0')\n",
      "tensor(6.0804, device='cuda:0')\n",
      "tensor(6.6166, device='cuda:0')\n",
      "tensor(6.6259, device='cuda:0')\n",
      "tensor(6.3499, device='cuda:0')\n",
      "tensor(6.6932, device='cuda:0')\n",
      "tensor(6.9933, device='cuda:0')\n",
      "tensor(6.8994, device='cuda:0')\n",
      "tensor(7.0421, device='cuda:0')\n",
      "tensor(6.6788, device='cuda:0')\n",
      "tensor(6.1818, device='cuda:0')\n",
      "tensor(5.9441, device='cuda:0')\n",
      "tensor(5.7400, device='cuda:0')\n",
      "tensor(5.7605, device='cuda:0')\n",
      "tensor(5.9167, device='cuda:0')\n",
      "tensor(6.1409, device='cuda:0')\n",
      "tensor(6.0299, device='cuda:0')\n",
      "tensor(6.1161, device='cuda:0')\n",
      "tensor(6.2028, device='cuda:0')\n",
      "tensor(6.1718, device='cuda:0')\n",
      "tensor(6.1117, device='cuda:0')\n",
      "tensor(6.4052, device='cuda:0')\n",
      "tensor(5.8559, device='cuda:0')\n",
      "tensor(5.3139, device='cuda:0')\n",
      "tensor(5.2528, device='cuda:0')\n",
      "tensor(5.2051, device='cuda:0')\n",
      "tensor(5.2097, device='cuda:0')\n",
      "tensor(5.5375, device='cuda:0')\n",
      "tensor(5.6715, device='cuda:0')\n",
      "tensor(7.5558, device='cuda:0')\n",
      "tensor(7.3645, device='cuda:0')\n",
      "tensor(7.4982, device='cuda:0')\n",
      "tensor(7.3981, device='cuda:0')\n",
      "tensor(7.6403, device='cuda:0')\n",
      "tensor(7.7290, device='cuda:0')\n",
      "tensor(7.5508, device='cuda:0')\n",
      "tensor(6.9364, device='cuda:0')\n",
      "tensor(6.8006, device='cuda:0')\n",
      "tensor(6.6558, device='cuda:0')\n",
      "tensor(6.4001, device='cuda:0')\n",
      "tensor(6.8407, device='cuda:0')\n",
      "tensor(6.5762, device='cuda:0')\n",
      "tensor(6.5733, device='cuda:0')\n",
      "tensor(6.9138, device='cuda:0')\n",
      "tensor(7.3283, device='cuda:0')\n",
      "tensor(7.3865, device='cuda:0')\n",
      "tensor(7.2518, device='cuda:0')\n",
      "tensor(7.3916, device='cuda:0')\n",
      "tensor(7.2203, device='cuda:0')\n",
      "tensor(7.0239, device='cuda:0')\n",
      "tensor(6.8886, device='cuda:0')\n",
      "tensor(7.4160, device='cuda:0')\n",
      "tensor(6.9125, device='cuda:0')\n",
      "tensor(6.8236, device='cuda:0')\n",
      "tensor(6.7624, device='cuda:0')\n",
      "tensor(6.4175, device='cuda:0')\n",
      "tensor(6.3856, device='cuda:0')\n",
      "tensor(6.3619, device='cuda:0')\n",
      "tensor(6.1112, device='cuda:0')\n",
      "tensor(5.9419, device='cuda:0')\n",
      "tensor(6.0547, device='cuda:0')\n",
      "tensor(6.0022, device='cuda:0')\n",
      "tensor(5.4705, device='cuda:0')\n",
      "tensor(5.5769, device='cuda:0')\n",
      "tensor(5.4799, device='cuda:0')\n",
      "tensor(5.3781, device='cuda:0')\n",
      "tensor(5.3034, device='cuda:0')\n",
      "tensor(5.5234, device='cuda:0')\n",
      "tensor(5.3926, device='cuda:0')\n",
      "tensor(5.5102, device='cuda:0')\n",
      "tensor(5.9983, device='cuda:0')\n",
      "tensor(5.8754, device='cuda:0')\n",
      "tensor(5.7181, device='cuda:0')\n",
      "tensor(5.6948, device='cuda:0')\n",
      "tensor(5.4328, device='cuda:0')\n",
      "tensor(5.5271, device='cuda:0')\n",
      "tensor(5.4268, device='cuda:0')\n",
      "tensor(5.3853, device='cuda:0')\n",
      "tensor(5.3987, device='cuda:0')\n",
      "tensor(5.4665, device='cuda:0')\n",
      "tensor(5.9896, device='cuda:0')\n",
      "tensor(6.1182, device='cuda:0')\n",
      "tensor(6.0713, device='cuda:0')\n",
      "tensor(5.9503, device='cuda:0')\n",
      "tensor(5.9253, device='cuda:0')\n",
      "tensor(5.8250, device='cuda:0')\n",
      "tensor(5.5462, device='cuda:0')\n",
      "tensor(5.2568, device='cuda:0')\n",
      "tensor(6.1431, device='cuda:0')\n",
      "tensor(6.1484, device='cuda:0')\n",
      "tensor(6.5127, device='cuda:0')\n",
      "tensor(6.3952, device='cuda:0')\n",
      "tensor(6.2244, device='cuda:0')\n",
      "tensor(6.4408, device='cuda:0')\n",
      "tensor(6.2719, device='cuda:0')\n",
      "tensor(6.6664, device='cuda:0')\n",
      "tensor(6.5381, device='cuda:0')\n",
      "tensor(6.9270, device='cuda:0')\n",
      "tensor(6.8980, device='cuda:0')\n",
      "tensor(6.9637, device='cuda:0')\n",
      "tensor(6.9208, device='cuda:0')\n",
      "tensor(6.9030, device='cuda:0')\n",
      "tensor(6.6031, device='cuda:0')\n",
      "tensor(6.1861, device='cuda:0')\n",
      "tensor(5.8315, device='cuda:0')\n",
      "tensor(5.4774, device='cuda:0')\n",
      "tensor(5.0873, device='cuda:0')\n",
      "tensor(4.6921, device='cuda:0')\n",
      "tensor(4.6952, device='cuda:0')\n",
      "tensor(4.8010, device='cuda:0')\n",
      "tensor(4.6463, device='cuda:0')\n",
      "tensor(4.4384, device='cuda:0')\n",
      "tensor(4.6558, device='cuda:0')\n",
      "tensor(4.7053, device='cuda:0')\n",
      "tensor(4.7385, device='cuda:0')\n",
      "tensor(4.6402, device='cuda:0')\n",
      "tensor(4.4274, device='cuda:0')\n",
      "tensor(5.4095, device='cuda:0')\n",
      "tensor(5.4518, device='cuda:0')\n",
      "tensor(6.4356, device='cuda:0')\n",
      "tensor(6.1795, device='cuda:0')\n",
      "tensor(6.1226, device='cuda:0')\n",
      "tensor(6.1154, device='cuda:0')\n",
      "tensor(6.7296, device='cuda:0')\n",
      "tensor(7.0152, device='cuda:0')\n",
      "tensor(6.6328, device='cuda:0')\n",
      "tensor(6.5566, device='cuda:0')\n",
      "tensor(6.0627, device='cuda:0')\n",
      "tensor(5.9605, device='cuda:0')\n",
      "tensor(5.9620, device='cuda:0')\n",
      "tensor(6.2225, device='cuda:0')\n",
      "tensor(5.8020, device='cuda:0')\n",
      "tensor(6.3988, device='cuda:0')\n",
      "tensor(6.4343, device='cuda:0')\n",
      "tensor(7.5036, device='cuda:0')\n",
      "tensor(7.3678, device='cuda:0')\n",
      "tensor(7.9856, device='cuda:0')\n",
      "tensor(7.4243, device='cuda:0')\n",
      "tensor(7.0209, device='cuda:0')\n",
      "tensor(6.3326, device='cuda:0')\n",
      "tensor(6.0117, device='cuda:0')\n",
      "tensor(5.9657, device='cuda:0')\n",
      "tensor(5.7397, device='cuda:0')\n",
      "tensor(5.7129, device='cuda:0')\n",
      "tensor(5.6088, device='cuda:0')\n",
      "tensor(5.9132, device='cuda:0')\n",
      "tensor(5.5162, device='cuda:0')\n",
      "tensor(6.9254, device='cuda:0')\n",
      "tensor(7.1556, device='cuda:0')\n",
      "tensor(7.3287, device='cuda:0')\n",
      "tensor(7.3477, device='cuda:0')\n",
      "tensor(6.8089, device='cuda:0')\n",
      "tensor(6.8101, device='cuda:0')\n",
      "tensor(6.3840, device='cuda:0')\n",
      "tensor(6.4052, device='cuda:0')\n",
      "tensor(6.5503, device='cuda:0')\n",
      "tensor(6.2113, device='cuda:0')\n",
      "tensor(5.5377, device='cuda:0')\n",
      "tensor(5.2968, device='cuda:0')\n",
      "tensor(6.4702, device='cuda:0')\n",
      "tensor(6.1689, device='cuda:0')\n",
      "tensor(6.6038, device='cuda:0')\n",
      "tensor(6.6827, device='cuda:0')\n",
      "tensor(5.8964, device='cuda:0')\n",
      "tensor(5.6708, device='cuda:0')\n",
      "tensor(5.7357, device='cuda:0')\n",
      "tensor(5.6134, device='cuda:0')\n",
      "tensor(5.0238, device='cuda:0')\n",
      "tensor(4.9535, device='cuda:0')\n",
      "tensor(4.8094, device='cuda:0')\n",
      "tensor(4.7714, device='cuda:0')\n",
      "tensor(5.2743, device='cuda:0')\n",
      "tensor(5.6783, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6266, device='cuda:0')\n",
      "tensor(5.8232, device='cuda:0')\n",
      "tensor(6.1573, device='cuda:0')\n",
      "tensor(6.4369, device='cuda:0')\n",
      "tensor(6.4081, device='cuda:0')\n",
      "tensor(6.3457, device='cuda:0')\n",
      "tensor(6.0349, device='cuda:0')\n",
      "tensor(5.6478, device='cuda:0')\n",
      "tensor(5.3064, device='cuda:0')\n",
      "tensor(5.2722, device='cuda:0')\n",
      "tensor(5.0650, device='cuda:0')\n",
      "tensor(5.0334, device='cuda:0')\n",
      "tensor(4.7193, device='cuda:0')\n",
      "tensor(4.9746, device='cuda:0')\n",
      "tensor(5.0968, device='cuda:0')\n",
      "tensor(4.9163, device='cuda:0')\n",
      "tensor(5.2040, device='cuda:0')\n",
      "tensor(5.1250, device='cuda:0')\n",
      "tensor(5.3451, device='cuda:0')\n",
      "tensor(5.6328, device='cuda:0')\n",
      "tensor(5.6849, device='cuda:0')\n",
      "tensor(6.0107, device='cuda:0')\n",
      "tensor(5.9516, device='cuda:0')\n",
      "tensor(5.6858, device='cuda:0')\n",
      "tensor(5.9880, device='cuda:0')\n",
      "tensor(5.6240, device='cuda:0')\n",
      "tensor(5.1612, device='cuda:0')\n",
      "tensor(5.1655, device='cuda:0')\n",
      "tensor(5.8116, device='cuda:0')\n",
      "tensor(5.1468, device='cuda:0')\n",
      "tensor(5.3583, device='cuda:0')\n",
      "tensor(5.4252, device='cuda:0')\n",
      "tensor(5.2047, device='cuda:0')\n",
      "tensor(5.0524, device='cuda:0')\n",
      "tensor(5.5976, device='cuda:0')\n",
      "tensor(5.8320, device='cuda:0')\n",
      "tensor(5.6397, device='cuda:0')\n",
      "tensor(6.3635, device='cuda:0')\n",
      "tensor(6.4767, device='cuda:0')\n",
      "tensor(8.5739, device='cuda:0')\n",
      "tensor(8.3193, device='cuda:0')\n",
      "tensor(7.9068, device='cuda:0')\n",
      "tensor(7.8419, device='cuda:0')\n",
      "tensor(7.6550, device='cuda:0')\n",
      "tensor(7.4772, device='cuda:0')\n",
      "tensor(7.0705, device='cuda:0')\n",
      "tensor(6.9686, device='cuda:0')\n",
      "tensor(6.3909, device='cuda:0')\n",
      "tensor(6.1541, device='cuda:0')\n",
      "tensor(5.6318, device='cuda:0')\n",
      "tensor(5.4480, device='cuda:0')\n",
      "tensor(5.4524, device='cuda:0')\n",
      "tensor(5.8504, device='cuda:0')\n",
      "tensor(5.9386, device='cuda:0')\n",
      "tensor(6.4448, device='cuda:0')\n",
      "tensor(6.6688, device='cuda:0')\n",
      "tensor(6.4089, device='cuda:0')\n",
      "tensor(6.2195, device='cuda:0')\n",
      "tensor(6.0701, device='cuda:0')\n",
      "tensor(5.9239, device='cuda:0')\n",
      "tensor(5.4672, device='cuda:0')\n",
      "tensor(5.2411, device='cuda:0')\n",
      "tensor(4.7380, device='cuda:0')\n",
      "tensor(4.4631, device='cuda:0')\n",
      "tensor(4.1493, device='cuda:0')\n",
      "tensor(4.3524, device='cuda:0')\n",
      "tensor(4.2750, device='cuda:0')\n",
      "tensor(4.6987, device='cuda:0')\n",
      "tensor(4.8470, device='cuda:0')\n",
      "tensor(5.2131, device='cuda:0')\n",
      "tensor(5.3890, device='cuda:0')\n",
      "tensor(5.5419, device='cuda:0')\n",
      "tensor(5.5966, device='cuda:0')\n",
      "tensor(5.7202, device='cuda:0')\n",
      "tensor(7.1568, device='cuda:0')\n",
      "tensor(6.6750, device='cuda:0')\n",
      "tensor(7.7170, device='cuda:0')\n",
      "tensor(7.9585, device='cuda:0')\n",
      "tensor(8.1912, device='cuda:0')\n",
      "tensor(8.5521, device='cuda:0')\n",
      "tensor(8.2431, device='cuda:0')\n",
      "tensor(7.9092, device='cuda:0')\n",
      "tensor(7.7970, device='cuda:0')\n",
      "tensor(8.0517, device='cuda:0')\n",
      "tensor(7.0578, device='cuda:0')\n",
      "tensor(7.1952, device='cuda:0')\n",
      "tensor(6.5730, device='cuda:0')\n",
      "tensor(5.8141, device='cuda:0')\n",
      "tensor(6.4821, device='cuda:0')\n",
      "tensor(6.6045, device='cuda:0')\n",
      "tensor(6.4856, device='cuda:0')\n",
      "tensor(6.4331, device='cuda:0')\n",
      "tensor(6.8822, device='cuda:0')\n",
      "tensor(6.8786, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 22:01:05 - INFO - __main__ -    global_step = 4551, average loss = 1.7606153168001952\n",
      "07/13/2022 22:01:05 - INFO - __main__ -   Saving model checkpoint to output-small\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9742, device='cuda:0')\n",
      "tensor(6.4884, device='cuda:0')\n",
      "tensor(5.8880, device='cuda:0')\n",
      "tensor(6.0061, device='cuda:0')\n",
      "tensor(6.0358, device='cuda:0')\n",
      "tensor(6.0315, device='cuda:0')\n",
      "tensor(5.9561, device='cuda:0')\n",
      "tensor(5.8740, device='cuda:0')\n",
      "tensor(6.2630, device='cuda:0')\n",
      "tensor(6.1247, device='cuda:0')\n",
      "epochwise results: {'perplexity_4551': tensor(449.2856)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 22:01:08 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small']\n",
      "07/13/2022 22:01:10 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "07/13/2022 22:01:10 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "07/13/2022 22:01:10 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/13/2022 22:01:10 - INFO - __main__ -     Num examples = 374\n",
      "07/13/2022 22:01:10 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b553cea7b446b48dd7efa48a5adbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9488, device='cuda:0')\n",
      "tensor(6.8646, device='cuda:0')\n",
      "tensor(6.6357, device='cuda:0')\n",
      "tensor(6.5281, device='cuda:0')\n",
      "tensor(6.4546, device='cuda:0')\n",
      "tensor(6.4136, device='cuda:0')\n",
      "tensor(6.2062, device='cuda:0')\n",
      "tensor(5.9349, device='cuda:0')\n",
      "tensor(5.6108, device='cuda:0')\n",
      "tensor(5.3043, device='cuda:0')\n",
      "tensor(4.9152, device='cuda:0')\n",
      "tensor(4.9561, device='cuda:0')\n",
      "tensor(4.8829, device='cuda:0')\n",
      "tensor(5.1140, device='cuda:0')\n",
      "tensor(4.9257, device='cuda:0')\n",
      "tensor(5.0910, device='cuda:0')\n",
      "tensor(5.1648, device='cuda:0')\n",
      "tensor(5.7676, device='cuda:0')\n",
      "tensor(6.0859, device='cuda:0')\n",
      "tensor(6.2453, device='cuda:0')\n",
      "tensor(6.1721, device='cuda:0')\n",
      "tensor(6.6956, device='cuda:0')\n",
      "tensor(6.9967, device='cuda:0')\n",
      "tensor(6.9664, device='cuda:0')\n",
      "tensor(6.1503, device='cuda:0')\n",
      "tensor(5.8670, device='cuda:0')\n",
      "tensor(5.9903, device='cuda:0')\n",
      "tensor(5.4878, device='cuda:0')\n",
      "tensor(5.3147, device='cuda:0')\n",
      "tensor(4.5113, device='cuda:0')\n",
      "tensor(4.5561, device='cuda:0')\n",
      "tensor(4.8665, device='cuda:0')\n",
      "tensor(5.1168, device='cuda:0')\n",
      "tensor(5.1222, device='cuda:0')\n",
      "tensor(5.2116, device='cuda:0')\n",
      "tensor(5.4294, device='cuda:0')\n",
      "tensor(5.6771, device='cuda:0')\n",
      "tensor(5.7930, device='cuda:0')\n",
      "tensor(5.4481, device='cuda:0')\n",
      "tensor(5.6856, device='cuda:0')\n",
      "tensor(5.8999, device='cuda:0')\n",
      "tensor(6.4087, device='cuda:0')\n",
      "tensor(6.6015, device='cuda:0')\n",
      "tensor(6.0891, device='cuda:0')\n",
      "tensor(6.2854, device='cuda:0')\n",
      "tensor(6.5698, device='cuda:0')\n",
      "tensor(7.3333, device='cuda:0')\n",
      "tensor(6.9829, device='cuda:0')\n",
      "tensor(6.6845, device='cuda:0')\n",
      "tensor(6.5332, device='cuda:0')\n",
      "tensor(6.4549, device='cuda:0')\n",
      "tensor(6.7272, device='cuda:0')\n",
      "tensor(6.2932, device='cuda:0')\n",
      "tensor(5.9502, device='cuda:0')\n",
      "tensor(5.5794, device='cuda:0')\n",
      "tensor(5.4739, device='cuda:0')\n",
      "tensor(5.4537, device='cuda:0')\n",
      "tensor(5.8753, device='cuda:0')\n",
      "tensor(5.8026, device='cuda:0')\n",
      "tensor(5.5112, device='cuda:0')\n",
      "tensor(5.4205, device='cuda:0')\n",
      "tensor(5.2512, device='cuda:0')\n",
      "tensor(5.4295, device='cuda:0')\n",
      "tensor(5.3398, device='cuda:0')\n",
      "tensor(5.3679, device='cuda:0')\n",
      "tensor(5.1740, device='cuda:0')\n",
      "tensor(5.1197, device='cuda:0')\n",
      "tensor(5.0718, device='cuda:0')\n",
      "tensor(5.3066, device='cuda:0')\n",
      "tensor(5.7902, device='cuda:0')\n",
      "tensor(5.6838, device='cuda:0')\n",
      "tensor(6.2685, device='cuda:0')\n",
      "tensor(6.4794, device='cuda:0')\n",
      "tensor(6.9055, device='cuda:0')\n",
      "tensor(7.4187, device='cuda:0')\n",
      "tensor(7.5928, device='cuda:0')\n",
      "tensor(7.5014, device='cuda:0')\n",
      "tensor(6.9094, device='cuda:0')\n",
      "tensor(6.4262, device='cuda:0')\n",
      "tensor(6.2456, device='cuda:0')\n",
      "tensor(6.2683, device='cuda:0')\n",
      "tensor(5.9966, device='cuda:0')\n",
      "tensor(5.9482, device='cuda:0')\n",
      "tensor(5.5438, device='cuda:0')\n",
      "tensor(5.5693, device='cuda:0')\n",
      "tensor(5.2357, device='cuda:0')\n",
      "tensor(5.4803, device='cuda:0')\n",
      "tensor(6.4230, device='cuda:0')\n",
      "tensor(6.2058, device='cuda:0')\n",
      "tensor(6.7160, device='cuda:0')\n",
      "tensor(7.0260, device='cuda:0')\n",
      "tensor(7.3640, device='cuda:0')\n",
      "tensor(7.5266, device='cuda:0')\n",
      "tensor(7.7809, device='cuda:0')\n",
      "tensor(7.9427, device='cuda:0')\n",
      "tensor(6.7101, device='cuda:0')\n",
      "tensor(6.3329, device='cuda:0')\n",
      "tensor(5.7230, device='cuda:0')\n",
      "tensor(6.0804, device='cuda:0')\n",
      "tensor(6.6166, device='cuda:0')\n",
      "tensor(6.6259, device='cuda:0')\n",
      "tensor(6.3499, device='cuda:0')\n",
      "tensor(6.6932, device='cuda:0')\n",
      "tensor(6.9933, device='cuda:0')\n",
      "tensor(6.8994, device='cuda:0')\n",
      "tensor(7.0421, device='cuda:0')\n",
      "tensor(6.6788, device='cuda:0')\n",
      "tensor(6.1818, device='cuda:0')\n",
      "tensor(5.9441, device='cuda:0')\n",
      "tensor(5.7400, device='cuda:0')\n",
      "tensor(5.7605, device='cuda:0')\n",
      "tensor(5.9167, device='cuda:0')\n",
      "tensor(6.1409, device='cuda:0')\n",
      "tensor(6.0299, device='cuda:0')\n",
      "tensor(6.1161, device='cuda:0')\n",
      "tensor(6.2028, device='cuda:0')\n",
      "tensor(6.1718, device='cuda:0')\n",
      "tensor(6.1117, device='cuda:0')\n",
      "tensor(6.4052, device='cuda:0')\n",
      "tensor(5.8559, device='cuda:0')\n",
      "tensor(5.3139, device='cuda:0')\n",
      "tensor(5.2528, device='cuda:0')\n",
      "tensor(5.2051, device='cuda:0')\n",
      "tensor(5.2097, device='cuda:0')\n",
      "tensor(5.5375, device='cuda:0')\n",
      "tensor(5.6715, device='cuda:0')\n",
      "tensor(7.5558, device='cuda:0')\n",
      "tensor(7.3645, device='cuda:0')\n",
      "tensor(7.4982, device='cuda:0')\n",
      "tensor(7.3981, device='cuda:0')\n",
      "tensor(7.6403, device='cuda:0')\n",
      "tensor(7.7290, device='cuda:0')\n",
      "tensor(7.5508, device='cuda:0')\n",
      "tensor(6.9364, device='cuda:0')\n",
      "tensor(6.8006, device='cuda:0')\n",
      "tensor(6.6558, device='cuda:0')\n",
      "tensor(6.4001, device='cuda:0')\n",
      "tensor(6.8407, device='cuda:0')\n",
      "tensor(6.5762, device='cuda:0')\n",
      "tensor(6.5733, device='cuda:0')\n",
      "tensor(6.9138, device='cuda:0')\n",
      "tensor(7.3283, device='cuda:0')\n",
      "tensor(7.3865, device='cuda:0')\n",
      "tensor(7.2518, device='cuda:0')\n",
      "tensor(7.3916, device='cuda:0')\n",
      "tensor(7.2203, device='cuda:0')\n",
      "tensor(7.0239, device='cuda:0')\n",
      "tensor(6.8886, device='cuda:0')\n",
      "tensor(7.4160, device='cuda:0')\n",
      "tensor(6.9125, device='cuda:0')\n",
      "tensor(6.8236, device='cuda:0')\n",
      "tensor(6.7624, device='cuda:0')\n",
      "tensor(6.4175, device='cuda:0')\n",
      "tensor(6.3856, device='cuda:0')\n",
      "tensor(6.3619, device='cuda:0')\n",
      "tensor(6.1112, device='cuda:0')\n",
      "tensor(5.9419, device='cuda:0')\n",
      "tensor(6.0547, device='cuda:0')\n",
      "tensor(6.0022, device='cuda:0')\n",
      "tensor(5.4705, device='cuda:0')\n",
      "tensor(5.5769, device='cuda:0')\n",
      "tensor(5.4799, device='cuda:0')\n",
      "tensor(5.3781, device='cuda:0')\n",
      "tensor(5.3034, device='cuda:0')\n",
      "tensor(5.5234, device='cuda:0')\n",
      "tensor(5.3926, device='cuda:0')\n",
      "tensor(5.5102, device='cuda:0')\n",
      "tensor(5.9983, device='cuda:0')\n",
      "tensor(5.8754, device='cuda:0')\n",
      "tensor(5.7181, device='cuda:0')\n",
      "tensor(5.6948, device='cuda:0')\n",
      "tensor(5.4328, device='cuda:0')\n",
      "tensor(5.5271, device='cuda:0')\n",
      "tensor(5.4268, device='cuda:0')\n",
      "tensor(5.3853, device='cuda:0')\n",
      "tensor(5.3987, device='cuda:0')\n",
      "tensor(5.4665, device='cuda:0')\n",
      "tensor(5.9896, device='cuda:0')\n",
      "tensor(6.1182, device='cuda:0')\n",
      "tensor(6.0713, device='cuda:0')\n",
      "tensor(5.9503, device='cuda:0')\n",
      "tensor(5.9253, device='cuda:0')\n",
      "tensor(5.8250, device='cuda:0')\n",
      "tensor(5.5462, device='cuda:0')\n",
      "tensor(5.2568, device='cuda:0')\n",
      "tensor(6.1431, device='cuda:0')\n",
      "tensor(6.1484, device='cuda:0')\n",
      "tensor(6.5127, device='cuda:0')\n",
      "tensor(6.3952, device='cuda:0')\n",
      "tensor(6.2244, device='cuda:0')\n",
      "tensor(6.4408, device='cuda:0')\n",
      "tensor(6.2719, device='cuda:0')\n",
      "tensor(6.6664, device='cuda:0')\n",
      "tensor(6.5381, device='cuda:0')\n",
      "tensor(6.9270, device='cuda:0')\n",
      "tensor(6.8980, device='cuda:0')\n",
      "tensor(6.9637, device='cuda:0')\n",
      "tensor(6.9208, device='cuda:0')\n",
      "tensor(6.9030, device='cuda:0')\n",
      "tensor(6.6031, device='cuda:0')\n",
      "tensor(6.1861, device='cuda:0')\n",
      "tensor(5.8315, device='cuda:0')\n",
      "tensor(5.4774, device='cuda:0')\n",
      "tensor(5.0873, device='cuda:0')\n",
      "tensor(4.6921, device='cuda:0')\n",
      "tensor(4.6952, device='cuda:0')\n",
      "tensor(4.8010, device='cuda:0')\n",
      "tensor(4.6463, device='cuda:0')\n",
      "tensor(4.4384, device='cuda:0')\n",
      "tensor(4.6558, device='cuda:0')\n",
      "tensor(4.7053, device='cuda:0')\n",
      "tensor(4.7385, device='cuda:0')\n",
      "tensor(4.6402, device='cuda:0')\n",
      "tensor(4.4274, device='cuda:0')\n",
      "tensor(5.4095, device='cuda:0')\n",
      "tensor(5.4518, device='cuda:0')\n",
      "tensor(6.4356, device='cuda:0')\n",
      "tensor(6.1795, device='cuda:0')\n",
      "tensor(6.1226, device='cuda:0')\n",
      "tensor(6.1154, device='cuda:0')\n",
      "tensor(6.7296, device='cuda:0')\n",
      "tensor(7.0152, device='cuda:0')\n",
      "tensor(6.6328, device='cuda:0')\n",
      "tensor(6.5566, device='cuda:0')\n",
      "tensor(6.0627, device='cuda:0')\n",
      "tensor(5.9605, device='cuda:0')\n",
      "tensor(5.9620, device='cuda:0')\n",
      "tensor(6.2225, device='cuda:0')\n",
      "tensor(5.8020, device='cuda:0')\n",
      "tensor(6.3988, device='cuda:0')\n",
      "tensor(6.4343, device='cuda:0')\n",
      "tensor(7.5036, device='cuda:0')\n",
      "tensor(7.3678, device='cuda:0')\n",
      "tensor(7.9856, device='cuda:0')\n",
      "tensor(7.4243, device='cuda:0')\n",
      "tensor(7.0209, device='cuda:0')\n",
      "tensor(6.3326, device='cuda:0')\n",
      "tensor(6.0117, device='cuda:0')\n",
      "tensor(5.9657, device='cuda:0')\n",
      "tensor(5.7397, device='cuda:0')\n",
      "tensor(5.7129, device='cuda:0')\n",
      "tensor(5.6088, device='cuda:0')\n",
      "tensor(5.9132, device='cuda:0')\n",
      "tensor(5.5162, device='cuda:0')\n",
      "tensor(6.9254, device='cuda:0')\n",
      "tensor(7.1556, device='cuda:0')\n",
      "tensor(7.3287, device='cuda:0')\n",
      "tensor(7.3477, device='cuda:0')\n",
      "tensor(6.8089, device='cuda:0')\n",
      "tensor(6.8101, device='cuda:0')\n",
      "tensor(6.3840, device='cuda:0')\n",
      "tensor(6.4052, device='cuda:0')\n",
      "tensor(6.5503, device='cuda:0')\n",
      "tensor(6.2113, device='cuda:0')\n",
      "tensor(5.5377, device='cuda:0')\n",
      "tensor(5.2968, device='cuda:0')\n",
      "tensor(6.4702, device='cuda:0')\n",
      "tensor(6.1689, device='cuda:0')\n",
      "tensor(6.6038, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6827, device='cuda:0')\n",
      "tensor(5.8964, device='cuda:0')\n",
      "tensor(5.6708, device='cuda:0')\n",
      "tensor(5.7357, device='cuda:0')\n",
      "tensor(5.6134, device='cuda:0')\n",
      "tensor(5.0238, device='cuda:0')\n",
      "tensor(4.9535, device='cuda:0')\n",
      "tensor(4.8094, device='cuda:0')\n",
      "tensor(4.7714, device='cuda:0')\n",
      "tensor(5.2743, device='cuda:0')\n",
      "tensor(5.6783, device='cuda:0')\n",
      "tensor(5.6266, device='cuda:0')\n",
      "tensor(5.8232, device='cuda:0')\n",
      "tensor(6.1573, device='cuda:0')\n",
      "tensor(6.4369, device='cuda:0')\n",
      "tensor(6.4081, device='cuda:0')\n",
      "tensor(6.3457, device='cuda:0')\n",
      "tensor(6.0349, device='cuda:0')\n",
      "tensor(5.6478, device='cuda:0')\n",
      "tensor(5.3064, device='cuda:0')\n",
      "tensor(5.2722, device='cuda:0')\n",
      "tensor(5.0650, device='cuda:0')\n",
      "tensor(5.0334, device='cuda:0')\n",
      "tensor(4.7193, device='cuda:0')\n",
      "tensor(4.9746, device='cuda:0')\n",
      "tensor(5.0968, device='cuda:0')\n",
      "tensor(4.9163, device='cuda:0')\n",
      "tensor(5.2040, device='cuda:0')\n",
      "tensor(5.1250, device='cuda:0')\n",
      "tensor(5.3451, device='cuda:0')\n",
      "tensor(5.6328, device='cuda:0')\n",
      "tensor(5.6849, device='cuda:0')\n",
      "tensor(6.0107, device='cuda:0')\n",
      "tensor(5.9516, device='cuda:0')\n",
      "tensor(5.6858, device='cuda:0')\n",
      "tensor(5.9880, device='cuda:0')\n",
      "tensor(5.6240, device='cuda:0')\n",
      "tensor(5.1612, device='cuda:0')\n",
      "tensor(5.1655, device='cuda:0')\n",
      "tensor(5.8116, device='cuda:0')\n",
      "tensor(5.1468, device='cuda:0')\n",
      "tensor(5.3583, device='cuda:0')\n",
      "tensor(5.4252, device='cuda:0')\n",
      "tensor(5.2047, device='cuda:0')\n",
      "tensor(5.0524, device='cuda:0')\n",
      "tensor(5.5976, device='cuda:0')\n",
      "tensor(5.8320, device='cuda:0')\n",
      "tensor(5.6397, device='cuda:0')\n",
      "tensor(6.3635, device='cuda:0')\n",
      "tensor(6.4767, device='cuda:0')\n",
      "tensor(8.5739, device='cuda:0')\n",
      "tensor(8.3193, device='cuda:0')\n",
      "tensor(7.9068, device='cuda:0')\n",
      "tensor(7.8419, device='cuda:0')\n",
      "tensor(7.6550, device='cuda:0')\n",
      "tensor(7.4772, device='cuda:0')\n",
      "tensor(7.0705, device='cuda:0')\n",
      "tensor(6.9686, device='cuda:0')\n",
      "tensor(6.3909, device='cuda:0')\n",
      "tensor(6.1541, device='cuda:0')\n",
      "tensor(5.6318, device='cuda:0')\n",
      "tensor(5.4480, device='cuda:0')\n",
      "tensor(5.4524, device='cuda:0')\n",
      "tensor(5.8504, device='cuda:0')\n",
      "tensor(5.9386, device='cuda:0')\n",
      "tensor(6.4448, device='cuda:0')\n",
      "tensor(6.6688, device='cuda:0')\n",
      "tensor(6.4089, device='cuda:0')\n",
      "tensor(6.2195, device='cuda:0')\n",
      "tensor(6.0701, device='cuda:0')\n",
      "tensor(5.9239, device='cuda:0')\n",
      "tensor(5.4672, device='cuda:0')\n",
      "tensor(5.2411, device='cuda:0')\n",
      "tensor(4.7380, device='cuda:0')\n",
      "tensor(4.4631, device='cuda:0')\n",
      "tensor(4.1493, device='cuda:0')\n",
      "tensor(4.3524, device='cuda:0')\n",
      "tensor(4.2750, device='cuda:0')\n",
      "tensor(4.6987, device='cuda:0')\n",
      "tensor(4.8470, device='cuda:0')\n",
      "tensor(5.2131, device='cuda:0')\n",
      "tensor(5.3890, device='cuda:0')\n",
      "tensor(5.5419, device='cuda:0')\n",
      "tensor(5.5966, device='cuda:0')\n",
      "tensor(5.7202, device='cuda:0')\n",
      "tensor(7.1568, device='cuda:0')\n",
      "tensor(6.6750, device='cuda:0')\n",
      "tensor(7.7170, device='cuda:0')\n",
      "tensor(7.9585, device='cuda:0')\n",
      "tensor(8.1912, device='cuda:0')\n",
      "tensor(8.5521, device='cuda:0')\n",
      "tensor(8.2431, device='cuda:0')\n",
      "tensor(7.9092, device='cuda:0')\n",
      "tensor(7.7970, device='cuda:0')\n",
      "tensor(8.0517, device='cuda:0')\n",
      "tensor(7.0578, device='cuda:0')\n",
      "tensor(7.1952, device='cuda:0')\n",
      "tensor(6.5730, device='cuda:0')\n",
      "tensor(5.8141, device='cuda:0')\n",
      "tensor(6.4821, device='cuda:0')\n",
      "tensor(6.6045, device='cuda:0')\n",
      "tensor(6.4856, device='cuda:0')\n",
      "tensor(6.4331, device='cuda:0')\n",
      "tensor(6.8822, device='cuda:0')\n",
      "tensor(6.8786, device='cuda:0')\n",
      "tensor(6.9742, device='cuda:0')\n",
      "tensor(6.4884, device='cuda:0')\n",
      "tensor(5.8880, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/13/2022 22:01:15 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/13/2022 22:01:15 - INFO - __main__ -     perplexity = tensor(449.2856)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0061, device='cuda:0')\n",
      "tensor(6.0358, device='cuda:0')\n",
      "tensor(6.0315, device='cuda:0')\n",
      "tensor(5.9561, device='cuda:0')\n",
      "tensor(5.8740, device='cuda:0')\n",
      "tensor(6.2630, device='cuda:0')\n",
      "tensor(6.1247, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(449.2856)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main(trn_df.iloc[:10], val_df)  # debug\n",
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eDkPEuvbD47"
   },
   "source": [
    "## Chatting with  Rick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjVqotI05gOS"
   },
   "source": [
    "The model is ready, so it's time to chat with Rick. But don't forget that Rick can be rude, I warned you.\n",
    "\n",
    "A variety of methods can be used in responces generation. You can find more details about these methods by this [link](https://huggingface.co/blog/how-to-generate). \n",
    "\n",
    "![alt text](https://media.giphy.com/media/ftl8NiWz2C3tuXOn3w/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nIeqMwZktv7N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:How are you, Rick?\n",
      "RickBot: I'm fine. How are you?\n",
      ">> User:Good, thanks\n",
      "RickBot: Rick!\n",
      ">> User:Where's Morty?\n",
      "RickBot: On my work bench, Morty.\n",
      ">> User:What's he doing there?\n",
      "RickBot: Saving our lives. After he takes me out he's gonna be coming for you!\n",
      ">> User:What? Me?\n",
      "RickBot: Ooh, oh, oh! Oh!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('output-small')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zq2wJFtss1e"
   },
   "source": [
    "**User**:How are you, Rick? <br>\n",
    "**RickBot**: I'm fine. <br>\n",
    "**User**:Where is Morty? <br>\n",
    "**RickBot**: He's in a hospital. <br>\n",
    "**User**:Why? <br>\n",
    "**RickBot**: Well, that depends on who breaks first -- me or the hospital. I mean, you know, the one that keeps giving me pills. <br>\n",
    "**User**:I don't understand <br>\n",
    "**RickBot**: Whoa! Whoa! <br>\n",
    "**User**:I see, you took your pills <br>\n",
    "**RickBot**: Hey, I-I'm not proud of this. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvxnAOkTBTIJ"
   },
   "source": [
    "I will give an example of a few more dialogues to show how Ricky is our Rick now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1gX23iKkd0g"
   },
   "source": [
    "![alt text](https://media.giphy.com/media/eNpXWzGIMRjIo4lXT8/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Who are you?\n",
      "RickBot: The guy with the mega seeds in his mouth.\n",
      ">> User:What seeds?\n",
      "RickBot: I have seeds. Come on, motherfucker!\n",
      ">> User:Must be some very valuable seeds\n",
      "RickBot: For those of us that are not ready to give in.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8496/241632387.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mnew_user_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User:\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# print(new_user_input_ids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('output-small')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User**:Who are you? <br>\n",
    "**RickBot**: A musician, I'm not a musician, but I'm pretty close to you. <br>\n",
    "**User**:What kind of musician are you? <br>\n",
    "**RickBot**: Who cares? I'm a musician. <br>\n",
    "**User**:Ok, play a song for me <br>\n",
    "**RickBot**: Shut up and listen to me, Morty. Listen to me. Listen! I'm the leader of a bunch of different death worlds, and I'm just trying to assemble a band. I'm gonna make this one-piece, and you're gonna be part of it. <br>\n",
    "**User**:ok <br>\n",
    "**RickBot**: Whoa, whoa, hey, who's this? <br>\n",
    "**User**:This is Morty <br>\n",
    "**RickBot**: Geez!!! <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Rick what's going on in your lab?\n",
      "RickBot: It's a device, Morty, that when you put it in your ear, you can enter people's dreams, Morty. It's like that movie that you keep crowing about.\n",
      ">> User:Inception?\n",
      "RickBot: That's right, Morty! I'm coming for you now.\n",
      ">> User:No, don't use it on me!!\n",
      "RickBot: What?!\n",
      ">> User:Don't use that device on me\n",
      "RickBot: Morty, you fucking moron!\n",
      ">> User:What?\n",
      "RickBot: You're a serious fuckin' idiot, Morty -- you're a full season behind.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('output-small')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:What do you think about Elon Musk?\n",
      "RickBot: I think about him a lot. He's a realist.\n",
      ">> User:But what exactly do you think?\n",
      "RickBot: That he's a good speaker, Jerry?\n",
      ">> User:Do you think he's smart or not?\n",
      "RickBot: Who cares? We got a pickle in the booth with a pickles!\n",
      ">> User:That's certainly a lot of pickles\n",
      "RickBot: You don't have to rub it in.\n",
      ">> User:But I will!!\n",
      "RickBot: I'm not gonna take my chances giving my son the middle name, Summer, because he's not a pickLE!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('output-small')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDiz2J0LBqaq"
   },
   "source": [
    "**User**:What do you think about Elon Musk? <br>\n",
    "**RickBot**: I think about him a lot. <br>\n",
    "**User**:But what exactly do you think? <br>\n",
    "**RickBot**: He's not a bad guy. He's just... not a very charismatic one. <br>\n",
    "**User**:I think he is charismatic <br>\n",
    "**RickBot**: He is. <br>\n",
    "**User**:What else you can say about him? <br>\n",
    "**RickBot**: You don't have to say anything. He is the leader of your class. <br>\n",
    "**User**:ok, thank you <br>\n",
    "**RickBot**: !!!\n",
    "\n",
    "![alt text](https://i.kym-cdn.com/news/posts/desktop/000/000/247/Screenshot_21.jpg)\n",
    "\n",
    "Image from [news.knowyourmeme.com](https://news.knowyourmeme.com/news/elon-musks-rick-and-morty-cameo-is-a-meme-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezq6Qtl_3NtD"
   },
   "source": [
    "![alt text](https://media.giphy.com/media/YSTmV6MWEkgW0HblZQ/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)\n",
    "\n",
    "**User**:hey, go get me a burger <br>\n",
    "RickBot: Whoa, whoa, hold on a second, let's not get crazy. This is gonna be good. <br>\n",
    "**User**:I'm hungry! c'mon, buy me a burger or better two <br>\n",
    "**RickBot**: Holy crap. <br>\n",
    "**User**:no crappy burger, a good one! <br>\n",
    "**RickBot**: What the hell? <br>\n",
    "**User**:don't be so bad, feed me <br>\n",
    "**RickBot**: I'll be back as fast as I can, Morty. <br>\n",
    "**User**:good boy, and don't forget ketchup and fries <br>\n",
    "**RickBot**: !!!!!! <br>\n",
    "**User**:what a hell? can you reply with words? <br>\n",
    "**RickBot**: Yes!! I will!! We will go out on a date. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Woi683Eyvttc"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZmkXDkv4BAl"
   },
   "source": [
    "![alt text](https://media.giphy.com/media/YQBWwGLJtFDQIATck5/giphy.gif)\n",
    "\n",
    "Image from [Giphy](https://giphy.com/)\n",
    "\n",
    "Congratulations! Our virtual Rick is alive (almost)! With the help of fine-tuning our model on a small dataset, we were able to create a virtual character with whom we can conduct interesting dialogs. \n",
    "\n",
    "Using the proposed approach you can create many interesting virtual characters based on an arbitrary dialogs dataset (just a csv file with replicas, one replica per line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jAMH0XPuE-f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjd1pCZxukWh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofg7iJNFyjBs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GPTRick.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02bc54995de148238eb5ed06f2ea8d62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07e561e746054cd59ef9b6c99c4c0b76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a76fe2605c84745b42f9cdb5f1a52db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69bbca8e234e48fbbc389fd44e11624f",
      "max": 427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f5b6f22c0ea24860a93cbe6a6edfc05e",
      "value": 427
     }
    },
    "0bc945b69eff441db1bb7be22c0896f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62d6b4a4ad454316bcceb04ecb55821b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b2c9710105a6405f9de235c41ed39a50",
      "value": " 3/3 [11:37&lt;00:00, 232.56s/it]"
     }
    },
    "136a63d66eb44cccbcd66bf4c0535a6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f326022eebe1428daab12fab05c8a075",
       "IPY_MODEL_c4091ab4f5264a86a149793853dbb941"
      ],
      "layout": "IPY_MODEL_b1b239d3d3f14906934461eb66c1c0f6"
     }
    },
    "1391f67de61c40f1aa24fd267c1f70d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1431b3f013104ab99ae7f9a5a576a487": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "167f7f54e2384c3d863bfff043dd3dec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16b59273a5cb45ea92b2406eda686f2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "19f2b118a7db4bc292c06d94c2760590": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "270e2a3848324fb5996af60d6ef26366": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27953c96d08846f9a919115a2d85006e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4021fb5d92c4211b369d5a5fe44d624",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_317bc13466ac499ea0c696a153188e96",
      "value": 456318
     }
    },
    "27bbbc46cfad4733b7ede6b9242e73d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_27953c96d08846f9a919115a2d85006e",
       "IPY_MODEL_c0292a6aba0e483a8388311f249ab35f"
      ],
      "layout": "IPY_MODEL_992dde81bdad49b9a03abb43a22caf3f"
     }
    },
    "2dfb4d0fc09645118f830e4ebc5dfdd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_270e2a3848324fb5996af60d6ef26366",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_02bc54995de148238eb5ed06f2ea8d62",
      "value": " 427/427 [03:53&lt;00:00,  1.83it/s]"
     }
    },
    "2f18e981fde04466b609e54f50abff48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "317bc13466ac499ea0c696a153188e96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "33d3beeaedeb469a92403db541551342": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36d501a1ff37420eadef5bcd7297ee26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37e11da84adb4832842d9a9ce54c2c2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cb73064ba3545d0815cef04dba25f40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d8165ae581c4b1dacd1f7f55c7cab77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6b866565b99437c80f887584b21f0d7",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6957ad8de954698ada26fd880ccd711",
      "value": 1042301
     }
    },
    "475d496b15b343448e7463cdcf9463a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4bf0d712c8564501b202165d1e0033d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4f3f630db29349da9afb5b7479f4aaf2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57b31670fc3a40839a6004c6aa0e41a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5aab8b98fdf741428fda6a2936a6c32b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff9f00d7a67b4a0d944299166c3943e5",
       "IPY_MODEL_0bc945b69eff441db1bb7be22c0896f9"
      ],
      "layout": "IPY_MODEL_b581bbe998324e4198b54f1b33fa8fcb"
     }
    },
    "60eabd58645142aea72ae027b057d65d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6296e303aed34c01866caffc200c0cfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d9f452aa79048599819ad5cc759a5e5",
       "IPY_MODEL_d4e4ccce098941459c4409187201979d"
      ],
      "layout": "IPY_MODEL_60eabd58645142aea72ae027b057d65d"
     }
    },
    "62d6b4a4ad454316bcceb04ecb55821b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6819f91aad5b4930ac1b43a74a57d852": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69bbca8e234e48fbbc389fd44e11624f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bb27f2bf6cd4463b47bf92338a24c67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_acfb2630da884e03809a47c581e5ea86",
       "IPY_MODEL_7442f2a6fd734b88a67e7da5003f3e0a"
      ],
      "layout": "IPY_MODEL_b0bf7a14389241fe8dd662ad2650d2ef"
     }
    },
    "7442f2a6fd734b88a67e7da5003f3e0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea1abfd0d2c841499355edb67589b69d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_928b31a5d3c041feab1f1abfbe83d386",
      "value": " 351M/351M [50:42&lt;00:00, 115kB/s]"
     }
    },
    "74fad213012043dcaa370dc2e54de8bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c99fa5c01fe4478aa8e57b7d59e11c29",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ee6446c4023c48518fd58b833e1a594e",
      "value": " 427/427 [11:37&lt;00:00,  1.63s/it]"
     }
    },
    "822d24db536f4994af65dd764364695c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88cc0f856ed54dffbed94cc2d22fab1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e1d23ba00644b80b8b4b5ee6a5abb6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b69f24d60bfe4bd7a50cb9465505e0dc",
       "IPY_MODEL_74fad213012043dcaa370dc2e54de8bc"
      ],
      "layout": "IPY_MODEL_4f3f630db29349da9afb5b7479f4aaf2"
     }
    },
    "928b31a5d3c041feab1f1abfbe83d386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95ed0f145f494915b5870739603e7085": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbcfb9b4822e4dc9b9238bd8fe98bebf",
       "IPY_MODEL_a6a5fdd6d4024dfc876270eb6a3d17e9"
      ],
      "layout": "IPY_MODEL_6819f91aad5b4930ac1b43a74a57d852"
     }
    },
    "992dde81bdad49b9a03abb43a22caf3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d9f452aa79048599819ad5cc759a5e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Evaluating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37e11da84adb4832842d9a9ce54c2c2b",
      "max": 47,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d57f56580d244a59ad388adb33abcb4d",
      "value": 47
     }
    },
    "a055d6391b5e4c9cb7fe3a88482d7d55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d8165ae581c4b1dacd1f7f55c7cab77",
       "IPY_MODEL_aa29d0cf3f834e7c85527584958f8b0c"
      ],
      "layout": "IPY_MODEL_fb73201b47dc446ca66cdefa01b81f7f"
     }
    },
    "a0f8864b0bf74deab8240b716dbda564": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a6a5fdd6d4024dfc876270eb6a3d17e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1431b3f013104ab99ae7f9a5a576a487",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a9798066c3d14283aee0876deb309ae6",
      "value": " 554/554 [1:37:32&lt;00:00, 10.6s/B]"
     }
    },
    "a9798066c3d14283aee0876deb309ae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa29d0cf3f834e7c85527584958f8b0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33d3beeaedeb469a92403db541551342",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_88cc0f856ed54dffbed94cc2d22fab1f",
      "value": " 1.04M/1.04M [00:03&lt;00:00, 344kB/s]"
     }
    },
    "acfb2630da884e03809a47c581e5ea86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6777b193dc6464687b4b6975f72cc71",
      "max": 351265583,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16b59273a5cb45ea92b2406eda686f2b",
      "value": 351265583
     }
    },
    "b0bf7a14389241fe8dd662ad2650d2ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1b239d3d3f14906934461eb66c1c0f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2c9710105a6405f9de235c41ed39a50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b581bbe998324e4198b54f1b33fa8fcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6777b193dc6464687b4b6975f72cc71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b69f24d60bfe4bd7a50cb9465505e0dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19f2b118a7db4bc292c06d94c2760590",
      "max": 427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bf0d712c8564501b202165d1e0033d5",
      "value": 427
     }
    },
    "bbcfb9b4822e4dc9b9238bd8fe98bebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07e561e746054cd59ef9b6c99c4c0b76",
      "max": 554,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2f18e981fde04466b609e54f50abff48",
      "value": 554
     }
    },
    "c0292a6aba0e483a8388311f249ab35f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb46b6aa2a6f4533a1f79a419ed17f14",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_167f7f54e2384c3d863bfff043dd3dec",
      "value": " 456k/456k [00:01&lt;00:00, 268kB/s]"
     }
    },
    "c2109878b39849a799187136beadf144": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a76fe2605c84745b42f9cdb5f1a52db",
       "IPY_MODEL_2dfb4d0fc09645118f830e4ebc5dfdd0"
      ],
      "layout": "IPY_MODEL_36d501a1ff37420eadef5bcd7297ee26"
     }
    },
    "c4021fb5d92c4211b369d5a5fe44d624": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4091ab4f5264a86a149793853dbb941": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57b31670fc3a40839a6004c6aa0e41a5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c6fa8a5e40b74a4199ab785729880b39",
      "value": " 427/427 [06:20&lt;00:00,  1.12it/s]"
     }
    },
    "c6b866565b99437c80f887584b21f0d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6fa8a5e40b74a4199ab785729880b39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c99fa5c01fe4478aa8e57b7d59e11c29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4e4ccce098941459c4409187201979d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1391f67de61c40f1aa24fd267c1f70d9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_822d24db536f4994af65dd764364695c",
      "value": " 47/47 [10:08&lt;00:00, 12.96s/it]"
     }
    },
    "d57f56580d244a59ad388adb33abcb4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e34e242a73ac4f4887f3d16d651affdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea1abfd0d2c841499355edb67589b69d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee6446c4023c48518fd58b833e1a594e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f326022eebe1428daab12fab05c8a075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cb73064ba3545d0815cef04dba25f40",
      "max": 427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0f8864b0bf74deab8240b716dbda564",
      "value": 427
     }
    },
    "f5b6f22c0ea24860a93cbe6a6edfc05e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f6957ad8de954698ada26fd880ccd711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fb46b6aa2a6f4533a1f79a419ed17f14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb73201b47dc446ca66cdefa01b81f7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff9f00d7a67b4a0d944299166c3943e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e34e242a73ac4f4887f3d16d651affdf",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_475d496b15b343448e7463cdcf9463a4",
      "value": 3
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
