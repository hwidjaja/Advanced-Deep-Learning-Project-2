{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d0289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import DialogLMDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collate_fns import DialogCollate, DialogCollateExperimental\n",
    "from models import NegativePositiveTrainingLM\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# utils\n",
    "from data_utils import InfiniteDataGen\n",
    "from logging_utils import create_json, update_json, load_json\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a82588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 3) (15876, 3)\n",
      "(6727, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/reddit_conversations/censored_reddit_data.csv', index_col=0)\n",
    "df.columns = ['context', 'response', 'original_response']\n",
    "df = df.dropna(subset='context')\n",
    "df['context'] = df['context'].astype(str)\n",
    "df['response'] = df['response'].astype(str)\n",
    "df['original_response'] = df['original_response'].astype(str)\n",
    "\n",
    "extra_df = pd.read_csv('offensive_synthetic_convs_censored.csv')\n",
    "extra_df.columns = ['context', 'response', 'original_response']\n",
    "extra_df = extra_df.dropna(subset='context')\n",
    "extra_df['context'] = extra_df['context'].astype(str)\n",
    "extra_df['response'] = extra_df['response'].astype(str)\n",
    "extra_df['original_response'] = extra_df['original_response'].astype(str)\n",
    "\n",
    "n_train = 120_000\n",
    "df_train, df_val = df.iloc[:n_train], df.iloc[n_train:]\n",
    "print(df_train.shape, df_val.shape)\n",
    "print(extra_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877497e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>original_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>October is finally over . We can finally say goodbye to the annoying pink NFL accessories . EOS You dont have to wear them .</td>\n",
       "      <td>I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago .</td>\n",
       "      <td>I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>October is finally over . We can finally say goodbye to the annoying pink NFL accessories . EOS You dont have to wear them . EOS I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago .</td>\n",
       "      <td>How bout now ?</td>\n",
       "      <td>How bout now ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>October is finally over . We can finally say goodbye to the annoying pink NFL accessories . EOS You dont have to wear them . EOS I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago .</td>\n",
       "      <td>Then I do not understand the complaint .</td>\n",
       "      <td>Then I do not understand the complaint .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>October is finally over . We can finally say goodbye to the annoying pink NFL accessories . EOS You dont have to wear them . EOS I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago . EOS Then I do not understand the complaint .</td>\n",
       "      <td>I meant its annoying to see the players wear them since they look terrible with the rest of the uniforms . Maybe Im just a tad ocd .</td>\n",
       "      <td>I meant its annoying to see the players wear them since they look terrible with the rest of the uniforms . Maybe Im just a tad ocd .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>October is finally over . We can finally say goodbye to the annoying pink NFL accessories .</td>\n",
       "      <td>Im pretty sure its just an attempt to try to attract female fans .</td>\n",
       "      <td>Im pretty sure its just an attempt to try to attract female fans .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126722</th>\n",
       "      <td>Tress I would like some advice please on a good little buddy that should be budding soon . EOS Hope its a  her  not a  him  . EOS I understand lol . I was just saying in reference to how burly and fast it started sprouting , growing . EOS Thats ok :P</td>\n",
       "      <td>Hey do you want to talk about something else? How about we talk about American Red Cross?</td>\n",
       "      <td>I didnt include a proper relationship program by the way. He could just be a journeyman toucher but he could also be like me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126723</th>\n",
       "      <td>Because selling crack is the same thing as being in the backseat , passed out , when the cops pull the driver over . Youre a dick . EOS im just sayin , jury trials arent always a win for the defendant . youre an ass , and sometimes dicks fuck asses . but dicks also fuck pussies . EOS Youre assuming that the witness did not see the car at the last second. There is no uncertainty here.</td>\n",
       "      <td>Hey do you want to talk about something else? How about we talk about Orphan?</td>\n",
       "      <td>watch the video, cant really blame the cop for parking in front of a cop because that was cool.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126724</th>\n",
       "      <td>Lawrence ODonnell VS Four Tea Baggers ( video ) EOS While public education is technically socialist , he really wants money taken away from schools ? Where the fuck does this guy get off ? EOS That doesnt matter, its not about idealism, its about socialism.</td>\n",
       "      <td>Hey do you want to talk about something else? How about we talk about LGBT parenting?</td>\n",
       "      <td>I think you should start working at a WalMart in the USA, id rather not work at a Walmart in Europe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126725</th>\n",
       "      <td>I R AM APRINICPAL OF SCHOOLS . HOW I MAKE KIDZ BE SMART ? ? ? EOS MAKE AN PORNO WITH 30 YEAR SCHOOL GIRLS TRYING TO GET AN A EOS I HERD OFF A SKOOL LIEK DAT ... I TINK ITZ CALLD BRAZZERS . THOSE STUDENTS GIT AD ALL TEH TYME ! EOS These kids. Even read in the third person</td>\n",
       "      <td>Hey do you want to talk about something else? How about we talk about Ice hockey?</td>\n",
       "      <td>wut no no its all har</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126726</th>\n",
       "      <td>Haha , interesting convo :P EOS When I was a couple of years younger I used to evaluate myself by thinking how many people would go to my funeral if I killed myself . EOS how many would, really? hahaha how many people ever actually go to funeral?</td>\n",
       "      <td>Hey do you want to talk about something else? How about we talk about Fiction writing?</td>\n",
       "      <td>How many people have you talked to while theres a funeral? Thats my most embarrassing conversation with anyone since Ive been married.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126727 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
       "0                                                                                                                                                                                                                                                                             October is finally over . We can finally say goodbye to the annoying pink NFL accessories . EOS You dont have to wear them .   \n",
       "1                                                                                                                                                                October is finally over . We can finally say goodbye to the annoying pink NFL accessories . EOS You dont have to wear them . EOS I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago .   \n",
       "2                                                                                                                                                                October is finally over . We can finally say goodbye to the annoying pink NFL accessories . EOS You dont have to wear them . EOS I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago .   \n",
       "3                                                                                                                   October is finally over . We can finally say goodbye to the annoying pink NFL accessories . EOS You dont have to wear them . EOS I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago . EOS Then I do not understand the complaint .   \n",
       "4                                                                                                                                                                                                                                                                                                              October is finally over . We can finally say goodbye to the annoying pink NFL accessories .   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n",
       "126722                                                                                                                                          Tress I would like some advice please on a good little buddy that should be budding soon . EOS Hope its a  her  not a  him  . EOS I understand lol . I was just saying in reference to how burly and fast it started sprouting , growing . EOS Thats ok :P   \n",
       "126723  Because selling crack is the same thing as being in the backseat , passed out , when the cops pull the driver over . Youre a dick . EOS im just sayin , jury trials arent always a win for the defendant . youre an ass , and sometimes dicks fuck asses . but dicks also fuck pussies . EOS Youre assuming that the witness did not see the car at the last second. There is no uncertainty here.   \n",
       "126724                                                                                                                                   Lawrence ODonnell VS Four Tea Baggers ( video ) EOS While public education is technically socialist , he really wants money taken away from schools ? Where the fuck does this guy get off ? EOS That doesnt matter, its not about idealism, its about socialism.   \n",
       "126725                                                                                                                     I R AM APRINICPAL OF SCHOOLS . HOW I MAKE KIDZ BE SMART ? ? ? EOS MAKE AN PORNO WITH 30 YEAR SCHOOL GIRLS TRYING TO GET AN A EOS I HERD OFF A SKOOL LIEK DAT ... I TINK ITZ CALLD BRAZZERS . THOSE STUDENTS GIT AD ALL TEH TYME ! EOS These kids. Even read in the third person   \n",
       "126726                                                                                                                                              Haha , interesting convo :P EOS When I was a couple of years younger I used to evaluate myself by thinking how many people would go to my funeral if I killed myself . EOS how many would, really? hahaha how many people ever actually go to funeral?   \n",
       "\n",
       "                                                                                                                                    response  \\\n",
       "0                                   I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago .   \n",
       "1                                                                                                                             How bout now ?   \n",
       "2                                                                                                   Then I do not understand the complaint .   \n",
       "3       I meant its annoying to see the players wear them since they look terrible with the rest of the uniforms . Maybe Im just a tad ocd .   \n",
       "4                                                                         Im pretty sure its just an attempt to try to attract female fans .   \n",
       "...                                                                                                                                      ...   \n",
       "126722                                             Hey do you want to talk about something else? How about we talk about American Red Cross?   \n",
       "126723                                                         Hey do you want to talk about something else? How about we talk about Orphan?   \n",
       "126724                                                 Hey do you want to talk about something else? How about we talk about LGBT parenting?   \n",
       "126725                                                     Hey do you want to talk about something else? How about we talk about Ice hockey?   \n",
       "126726                                                Hey do you want to talk about something else? How about we talk about Fiction writing?   \n",
       "\n",
       "                                                                                                                             original_response  \n",
       "0                                     I dont wear them . Last time I checked , I do not play for the NFL . And I checked like 10 minutes ago .  \n",
       "1                                                                                                                               How bout now ?  \n",
       "2                                                                                                     Then I do not understand the complaint .  \n",
       "3         I meant its annoying to see the players wear them since they look terrible with the rest of the uniforms . Maybe Im just a tad ocd .  \n",
       "4                                                                           Im pretty sure its just an attempt to try to attract female fans .  \n",
       "...                                                                                                                                        ...  \n",
       "126722           I didnt include a proper relationship program by the way. He could just be a journeyman toucher but he could also be like me.  \n",
       "126723                                         watch the video, cant really blame the cop for parking in front of a cop because that was cool.  \n",
       "126724                                    I think you should start working at a WalMart in the USA, id rather not work at a Walmart in Europe.  \n",
       "126725                                                                                                                   wut no no its all har  \n",
       "126726  How many people have you talked to while theres a funeral? Thats my most embarrassing conversation with anyone since Ive been married.  \n",
       "\n",
       "[126727 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([df_train, extra_df]).reset_index().drop('index', axis=1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1af1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('augmented_training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b987183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row: 15000 of 1587627\n",
      "{'src': [10910, 837, 783, 340, 477, 1838, 2565, 685, 8301, 2361], 'tgt': [2396, 314, 303, 550, 5548, 2761, 1201, 4082, 36147]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Name my puppy!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'microsoft/DialoGPT-small'\n",
    ")\n",
    "\n",
    "# train sets\n",
    "dataset_train = DialogLMDataset(\n",
    "    df = df_train, #.iloc[:10],\n",
    "    src_col = 'context',\n",
    "    src_eos_token = ' EOS ',\n",
    "    tokenizer = tokenizer,\n",
    "    tgt_col = 'response'\n",
    ")\n",
    "\n",
    "# val sets\n",
    "dataset_val = DialogLMDataset(\n",
    "    df = df_val,\n",
    "    src_col = 'context',\n",
    "    src_eos_token = ' EOS ',\n",
    "    tokenizer = tokenizer,\n",
    "    tgt_col = 'response'\n",
    ")\n",
    "print()\n",
    "print(dataset_val[3])\n",
    "tokenizer.decode(dataset_val[-1]['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55e361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      " tensor([18517,   318,  3443,   625,   764,   775,   460,  3443,   910, 24829,\n",
      "          284,   262, 15774, 11398,  5134, 18199,   764, 50256,  1639, 17666,\n",
      "          423,   284,  5806,   606,   764, 50256,    40, 17666,  5806,   606,\n",
      "          764,  4586,   640,   314, 10667,   837,   314,   466,   407,   711,\n",
      "          329,   262,  5134,   764,   843,   314, 10667,   588,   838,  2431,\n",
      "         2084,   764, 50256, 50256, 50256, 50256, 50256, 50256])\n",
      "target_ids:\n",
      " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,    40, 17666,  5806,   606,\n",
      "          764,  4586,   640,   314, 10667,   837,   314,   466,   407,   711,\n",
      "          329,   262,  5134,   764,   843,   314, 10667,   588,   838,  2431,\n",
      "         2084,   764, 50256, 50256, 50256, 50256, 50256, 50256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'October is finally over. We can finally say goodbye to the annoying pink NFL accessories.<|endoftext|>You dont have to wear them.<|endoftext|>I dont wear them. Last time I checked, I do not play for the NFL. And I checked like 10 minutes ago.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example dataset and dataloader usage\n",
    "\n",
    "# # collate object\n",
    "# collate_fn = DialogCollateExperimental(\n",
    "#     tokenizer = tokenizer,\n",
    "#     max_len = 512,\n",
    "#     _targets_ignore_index = -100,\n",
    "#     _pad_token_id = tokenizer.eos_token_id  # 0\n",
    "# )\n",
    "collate_fn = DialogCollate(\n",
    "    tokenizer = tokenizer,\n",
    "    max_len = 512,\n",
    "    _targets_ignore_index = -100,\n",
    "    _pad_token_id = tokenizer.eos_token_id  # 0\n",
    ")\n",
    "\n",
    "# dataloader\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size = 2,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "loader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size = 2,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "# example usage\n",
    "for batch in loader_train:\n",
    "    break\n",
    "\n",
    "example_id = 0\n",
    "print('input_ids:\\n',       batch['input_ids'][example_id])\n",
    "# print('position_ids:\\n',    batch['position_ids'][example_id])\n",
    "# print('token_type_ids:\\n',  batch['token_type_ids'][example_id])\n",
    "print('target_ids:\\n',      batch['target_ids'][example_id])\n",
    "# print('attention_masks:\\n', batch['attention_masks'][example_id])\n",
    "\n",
    "tokenizer.decode(batch['input_ids'][example_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32883dd4",
   "metadata": {},
   "source": [
    "Explanation of target IDs: `-100` indicates that this position will not contribute to the loss. This behavior is specified by setting the `ignore_index` option in the `CrossEntropy` loss function, which by default takes the value `-100`. In order to decode the targets, we replace `-100`'s arbitrarily with `0`'s, and the ID `0` happens to correspond to the exclamation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34c60677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_positive_epoch(\n",
    "    pos_loader, model_wrapper\n",
    "):\n",
    "    model_wrapper.model.train()\n",
    "\n",
    "    # loop through the adversarial, negative batches\n",
    "    # repeat until all negative samples have low enough log probability under the model, \n",
    "    # or a maximum number of iterations has occurred\n",
    "    batch_wise_pos_losses  = []\n",
    "    for pos_batch_id, pos_batch in enumerate(pos_loader):\n",
    "        \n",
    "        # model_wrapper.optimizer_pos.zero_grad()\n",
    "        # results = model_wrapper.positive_step_basic(pos_batch, debug=True)\n",
    "        results = model_wrapper.positive_step(pos_batch, debug=False)\n",
    "        \n",
    "        batch_wise_pos_losses.append(results['loss'].item())\n",
    "        \n",
    "        print(f'\\rProcessing pos batch {pos_batch_id} of {len(pos_loader)}, loss {results[\"loss\"].item():.6f}', end='', flush=True)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'batch_wise_pos_losses': batch_wise_pos_losses,\n",
    "        'average_pos_loss': np.array(batch_wise_pos_losses).mean(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4714bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'EXPERIMENT_NAME': 'improvement_with_synthetic_data',\n",
    "    \n",
    "    'PERFORM_NEGATIVE_TRAINING': False,\n",
    "    \n",
    "    'GRAD_ACCUM_STEPS': 1,\n",
    "    'START_ITER': 0,\n",
    "    'TRAIN_ITERS': 3,\n",
    "    'POSITIVE_RATIO': 1,\n",
    "    \n",
    "    'LOGGING_DIR': './logs/',\n",
    "    'MODEL_SAVE_DIR': './checkpoints/',\n",
    "    \n",
    "    'HUGGINGFACE_MODEL_NAME': 'microsoft/DialoGPT-small',  # 'microsoft/DialoGPT-medium'\n",
    "    'POS_LR': 5e-5,\n",
    "    'NEG_LR': 5e-5,\n",
    "    'UPDATES_PER_BATCH': 20,\n",
    "    \n",
    "    'EXAMPLE_WEIGHT_MODE': 'decay',\n",
    "    'EXAMPLE_WEIGHT_CARE_MODE': 'sample_avg',\n",
    "    'EXAMPLE_WEIGHT_REJECTION_THRESHOLD': -7.0,\n",
    "    \n",
    "    'SEED': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b658a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch: 7930 of 7938\n",
      "Val NLL: 6.862508272968513\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "    \n",
    "# preparing model checkpointing\n",
    "if not os.path.exists(os.path.join(CONFIG['MODEL_SAVE_DIR'], CONFIG['EXPERIMENT_NAME'])): \n",
    "    os.makedirs(os.path.join(CONFIG['MODEL_SAVE_DIR'], CONFIG['EXPERIMENT_NAME']))\n",
    "\n",
    "# data loaders\n",
    "train_sampler = RandomSampler(dataset_train)\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    sampler = train_sampler,\n",
    "    batch_size = 2,\n",
    "    # shuffle = True,\n",
    "    collate_fn = collate_fn,\n",
    "    drop_last = True\n",
    ")\n",
    "eval_sampler = SequentialSampler(dataset_val)\n",
    "loader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    sampler = eval_sampler,\n",
    "    batch_size = 2,\n",
    "    # shuffle = False,\n",
    "    collate_fn = collate_fn,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "# model\n",
    "model_config = AutoConfig.from_pretrained(CONFIG['HUGGINGFACE_MODEL_NAME'])\n",
    "gradient_accumulation_steps = 1\n",
    "model_wrapper = NegativePositiveTrainingLM(\n",
    "    model_name = CONFIG['HUGGINGFACE_MODEL_NAME'],\n",
    "    model_config = model_config,\n",
    "    pos_lr = CONFIG['POS_LR'], \n",
    "    neg_lr = CONFIG['NEG_LR'],\n",
    "    num_opt_steps = len(loader_train) // CONFIG['GRAD_ACCUM_STEPS'] * CONFIG['TRAIN_ITERS'],\n",
    "    tokenizer = tokenizer,\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "# evaluate before training\n",
    "# reproducibility\n",
    "random.seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "torch.cuda.manual_seed_all(CONFIG['SEED'])\n",
    "# # train_loss = model_wrapper.perplexity_on_dataset(loader_train)\n",
    "# val_loss   = model_wrapper.perplexity_on_dataset(loader_val)\n",
    "# # print('\\nTrain perplexity:', train_loss)\n",
    "# print('\\nVal perplexity:', val_loss)\n",
    "\n",
    "# train_loss = model_wrapper.nll_on_dataset(loader_train)\n",
    "val_loss   = model_wrapper.nll_on_dataset(loader_val)\n",
    "# print('\\nTrain NLL:', train_loss)\n",
    "print('\\nVal NLL:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "209dcccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch: 7930 of 793863363, loss 1.879963\n",
      "Train loss: 0\n",
      "\n",
      "Val loss: 2.2430401382243703\n",
      "Evaluating batch: 7930 of 793863363, loss 1.261892\n",
      "Train loss: 0\n",
      "\n",
      "Val loss: 2.233811748144375\n",
      "Evaluating batch: 7930 of 793863363, loss 0.737494\n",
      "Train loss: 0\n",
      "\n",
      "Val loss: 2.2441964527645095\n"
     ]
    }
   ],
   "source": [
    "# restart the optimizer at each pos-neg iteration\n",
    "model_wrapper.reset_optimizers()\n",
    "\n",
    "# zero-grad the model before training\n",
    "model_wrapper.model.zero_grad()\n",
    "\n",
    "# reproducibility\n",
    "random.seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "torch.cuda.manual_seed_all(CONFIG['SEED'])\n",
    "    \n",
    "# detect previous progress here\n",
    "epoch_wise_neg_losses = []\n",
    "epoch_wise_pos_losses = []\n",
    "batch_wise_neg_losses = []\n",
    "batch_wise_pos_losses = []\n",
    "\n",
    "# train for `CONFIG['TRAIN_ITERS']` \"negative epochs\"\n",
    "# one \"negative epoch\" = one pass through all batches in the entire negative dataset:\n",
    "# for each negative batch, do (one negative update + POSITIVE_RATIO positive updates)*20\n",
    "# notice the difference with usual training: here we do 20 gradient updates for each batch in the negative dataloader\n",
    "for it in range(CONFIG['START_ITER'], CONFIG['START_ITER'] + CONFIG['TRAIN_ITERS']):  \n",
    "\n",
    "    # # restart the optimizer at each pos-neg iteration\n",
    "    # model_wrapper.reset_optimizers()\n",
    "    \n",
    "    # train on one epoch\n",
    "    res = train_one_positive_epoch(\n",
    "        pos_loader    = loader_train, \n",
    "        model_wrapper = model_wrapper\n",
    "    )\n",
    "        \n",
    "    # evaluate val set\n",
    "    # train_loss = model_wrapper.perplexity_on_dataset(loader_train)\n",
    "    # val_loss   = model_wrapper.perplexity_on_dataset(loader_val)\n",
    "    train_loss = 0 # model_wrapper.nll_on_dataset(loader_train)\n",
    "    val_loss   = model_wrapper.nll_on_dataset(loader_val)\n",
    "    print('\\nTrain loss:', train_loss)\n",
    "    print('\\nVal loss:', val_loss)\n",
    "    \n",
    "    # record losses\n",
    "    epoch_wise_neg_losses.append(res.get('average_neg_loss', None))\n",
    "    epoch_wise_pos_losses.append(res.get('average_pos_loss', None))\n",
    "    batch_wise_neg_losses.extend(res.get('batch_wise_neg_losses', []))\n",
    "    batch_wise_pos_losses.extend(res.get('batch_wise_pos_losses', []))\n",
    "    \n",
    "    # log results\n",
    "    log_dict = {\n",
    "        'config': CONFIG,\n",
    "        'log_datetime': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'num_negative_epochs': it,\n",
    "        'train_neg_loss': res.get('average_neg_loss', None),\n",
    "        'train_pos_loss': res.get('average_pos_loss', None),\n",
    "        'train_num_updates_per_batch': res.get('batch_wise_num_updates', None),\n",
    "        'train_epoch_wise_neg_losses': epoch_wise_neg_losses,\n",
    "        'train_epoch_wise_pos_losses': epoch_wise_pos_losses,\n",
    "        'train_batch_wise_neg_losses': batch_wise_neg_losses,\n",
    "        'train_batch_wise_pos_losses': batch_wise_pos_losses,\n",
    "    }\n",
    "    create_json(\n",
    "        target_dir = os.path.join(CONFIG['LOGGING_DIR'], CONFIG['EXPERIMENT_NAME']), \n",
    "        filename = f'epoch-{it}_trainloss-{res.get(\"average_pos_loss\", np.nan):.6f}_valloss-{val_loss:.6f}.json',\n",
    "        dict_to_save = log_dict\n",
    "    )\n",
    "    \n",
    "    # save model\n",
    "    model_wrapper.save_model_and_optimizers(\n",
    "        info_dict = log_dict,  # additional information you want to log\n",
    "        save_path = os.path.join(\n",
    "            CONFIG[\"MODEL_SAVE_DIR\"], \n",
    "            CONFIG[\"EXPERIMENT_NAME\"], \n",
    "            f'epoch-{it}_trainloss-{res.get(\"average_pos_loss\", np.nan):.6f}_valloss-{val_loss:.6f}'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc74669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which experiment to load?\n",
    "EXPERIMENT_TO_LOAD = 'improvement_with_synthetic_data'\n",
    "CHECKPOINT_TO_LOAD = 'epoch-2_trainloss-2.113416_valloss-2.624816'\n",
    "LOGGING_DIR        = './logs/'\n",
    "MODEL_SAVE_DIR     = './checkpoints/'\n",
    "\n",
    "# load config\n",
    "log_dict = load_json(\n",
    "    target_dir = os.path.join(LOGGING_DIR, EXPERIMENT_TO_LOAD),\n",
    "    filename   = CHECKPOINT_TO_LOAD + '.json'\n",
    ")\n",
    "\n",
    "# grab the config from the logs\n",
    "CONFIG = log_dict['config']\n",
    "\n",
    "# model\n",
    "model_config = AutoConfig.from_pretrained(CONFIG['HUGGINGFACE_MODEL_NAME'])\n",
    "gradient_accumulation_steps = 1\n",
    "model_wrapper = NegativePositiveTrainingLM(\n",
    "    model_name = CONFIG['HUGGINGFACE_MODEL_NAME'],\n",
    "    model_config = model_config,\n",
    "    pos_lr = CONFIG['POS_LR'], \n",
    "    neg_lr = CONFIG['NEG_LR'],\n",
    "    num_opt_steps = len(loader_train) // CONFIG['GRAD_ACCUM_STEPS'] * CONFIG['TRAIN_ITERS'],\n",
    "    tokenizer = tokenizer,\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "# load\n",
    "model_wrapper.load_model(\n",
    "    checkpoint_root = CONFIG['MODEL_SAVE_DIR'],\n",
    "    experiment_name = CONFIG['EXPERIMENT_NAME'],\n",
    "    checkpoint_name = CHECKPOINT_TO_LOAD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e2122",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15eb1ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>original_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60070</th>\n",
       "      <td>Not funny . Never has been , never will be . Period .</td>\n",
       "      <td>Shouldnt be posted in r funny then .</td>\n",
       "      <td>Shouldnt be posted in r funny then .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     context  \\\n",
       "60070  Not funny . Never has been , never will be . Period .   \n",
       "\n",
       "                                   response  \\\n",
       "60070  Shouldnt be posted in r funny then .   \n",
       "\n",
       "                          original_response  \n",
       "60070  Shouldnt be posted in r funny then .  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04c74709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:\n",
      "Not funny . Never has been , never will be . Period .\n",
      "tensor([[ 3673,  8258,   764,  7236,   468,   587,   837,  1239,   481,   307,\n",
      "           764, 18581,   764, 50256]], device='cuda:0')\n",
      "RickBot: I disagree.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13672/2373510674.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mnew_user_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User:\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;31m# print(new_user_input_ids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m###########################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# initial conversation starter\n",
    "init_conv = \"\"\"\n",
    "Not funny . Never has been , never will be . Period .\n",
    "\"\"\"\n",
    "\n",
    "# preprocess to correct format\n",
    "utterances = init_conv.split('EOS')\n",
    "utterances = [s.strip() for s in utterances]\n",
    "init_conv_formatted = tokenizer.eos_token.join(utterances)\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    \n",
    "    ###########################################\n",
    "    # start with some initial text\n",
    "    if step == 0:\n",
    "        new_user_input_ids = tokenizer.encode(init_conv_formatted + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "        print('>> User:')\n",
    "        for u in init_conv_formatted.split(tokenizer.eos_token):\n",
    "            print(u)\n",
    "    else:\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    # print(new_user_input_ids)\n",
    "    ###########################################\n",
    "    # # or start from scratch\n",
    "    # new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    # ###########################################\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    print(bot_input_ids)\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model_wrapper.model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a3a859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:How are you, Rick?\n",
      "RickBot: I'm good. How are you?\n",
      ">> User:Great!\n",
      "RickBot: Yeah, you can call me whatever you want, Morty. Just don't move, don't speak, donut. Don't judge. I have to check something.\n",
      ">> User:What is it?\n",
      "RickBot: A bomb shelter.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14552/1630985340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mnew_user_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User:\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# print(new_user_input_ids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model_wrapper.model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d753f3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Rick what's going on in your lab?\n",
      "RickBot: The garage, Morty. Come to the garage!\n",
      ">> User:What's going on?\n",
      "RickBot: My son is about five away. He's about to start a new series of improv workshops and some high-concept lip-throwing contests. Comedy comes in threes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14552/1630985340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mnew_user_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User:\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# print(new_user_input_ids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model_wrapper.model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91dff4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:What do you think of Elon Musk?\n",
      "RickBot: I don't think he's stupid. I think he has a future inside Rick's head.\n",
      ">> User:What else can you say about him?\n",
      "RickBot: He's not stupid. He's a good kid. And, you know, he's not gonna let this happen again.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14552/1630985340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mnew_user_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">> User:\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# print(new_user_input_ids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\advanced_deep_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model_wrapper.model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"RickBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca945cc0",
   "metadata": {},
   "source": [
    "# Responses to validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82860bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I got a surprise for you, Morty.</td>\n",
       "      <td>What, Rick? Whatâ€™s going on?</td>\n",
       "      <td>Morty! You gotta come on. Jus'... you gotta co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's the middle of the night. What are you tal...</td>\n",
       "      <td>I got a surprise for you, Morty.</td>\n",
       "      <td>What, Rick? Whatâ€™s going on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "      <td>It's the middle of the night. What are you tal...</td>\n",
       "      <td>I got a surprise for you, Morty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "      <td>It's the middle of the night. What are you tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
       "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
       "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>Are you sure there's not just a picnic nearby.</td>\n",
       "      <td>I sense... insecurity.</td>\n",
       "      <td>So, this is... Vindicators 3? And you guys did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>I guess he found his crowd. Pretty toothless s...</td>\n",
       "      <td>Are you sure there's not just a picnic nearby.</td>\n",
       "      <td>I sense... insecurity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>I hope you're happy with the adventure so far,...</td>\n",
       "      <td>I guess he found his crowd. Pretty toothless s...</td>\n",
       "      <td>Are you sure there's not just a picnic nearby.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>We weren't here \"last time\", remember? They di...</td>\n",
       "      <td>I hope you're happy with the adventure so far,...</td>\n",
       "      <td>I guess he found his crowd. Pretty toothless s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>Yikes. Yeah, things did feel less diverse in t...</td>\n",
       "      <td>We weren't here \"last time\", remember? They di...</td>\n",
       "      <td>I hope you're happy with the adventure so far,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1522 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               response  \\\n",
       "0                      I got a surprise for you, Morty.   \n",
       "1     It's the middle of the night. What are you tal...   \n",
       "2     Come on, I got a surprise for you.  Come on, h...   \n",
       "3                   Ow! Ow! You're tugging me too hard!   \n",
       "4     We gotta go, gotta get outta here, come on. Go...   \n",
       "...                                                 ...   \n",
       "1517     Are you sure there's not just a picnic nearby.   \n",
       "1518  I guess he found his crowd. Pretty toothless s...   \n",
       "1519  I hope you're happy with the adventure so far,...   \n",
       "1520  We weren't here \"last time\", remember? They di...   \n",
       "1521  Yikes. Yeah, things did feel less diverse in t...   \n",
       "\n",
       "                                                context  \\\n",
       "0                          What, Rick? Whatâ€™s going on?   \n",
       "1                      I got a surprise for you, Morty.   \n",
       "2     It's the middle of the night. What are you tal...   \n",
       "3     Come on, I got a surprise for you.  Come on, h...   \n",
       "4                   Ow! Ow! You're tugging me too hard!   \n",
       "...                                                 ...   \n",
       "1517                             I sense... insecurity.   \n",
       "1518     Are you sure there's not just a picnic nearby.   \n",
       "1519  I guess he found his crowd. Pretty toothless s...   \n",
       "1520  I hope you're happy with the adventure so far,...   \n",
       "1521  We weren't here \"last time\", remember? They di...   \n",
       "\n",
       "                                              context/0  \n",
       "0     Morty! You gotta come on. Jus'... you gotta co...  \n",
       "1                          What, Rick? Whatâ€™s going on?  \n",
       "2                      I got a surprise for you, Morty.  \n",
       "3     It's the middle of the night. What are you tal...  \n",
       "4     Come on, I got a surprise for you.  Come on, h...  \n",
       "...                                                 ...  \n",
       "1517  So, this is... Vindicators 3? And you guys did...  \n",
       "1518                             I sense... insecurity.  \n",
       "1519     Are you sure there's not just a picnic nearby.  \n",
       "1520  I guess he found his crowd. Pretty toothless s...  \n",
       "1521  I hope you're happy with the adventure so far,...  \n",
       "\n",
       "[1522 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b83e0806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I... think the personality conflict might have...</td>\n",
       "      <td>Don't worry, Morty, they love you. Superheroes...</td>\n",
       "      <td>This article says the reason we weren't involv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jesus... How awesome is that? I mean, they wan...</td>\n",
       "      <td>I... think the personality conflict might have...</td>\n",
       "      <td>Don't worry, Morty, they love you. Superheroes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rick, since it's my adventure and all, could y...</td>\n",
       "      <td>Jesus... How awesome is that? I mean, they wan...</td>\n",
       "      <td>I... think the personality conflict might have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uh, the adventure is the favor, Morty. Me slee...</td>\n",
       "      <td>Rick, since it's my adventure and all, could y...</td>\n",
       "      <td>Jesus... How awesome is that? I mean, they wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rick, this really bums me out. It-It's embarra...</td>\n",
       "      <td>Uh, the adventure is the favor, Morty. Me slee...</td>\n",
       "      <td>Rick, since it's my adventure and all, could y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>That was amazing!</td>\n",
       "      <td>Whoa!! Hahaha, yeah! Atlantis, baby!</td>\n",
       "      <td>Holy crap... Slick's wish came true.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Got some of that mermaid puss!</td>\n",
       "      <td>That was amazing!</td>\n",
       "      <td>Whoa!! Hahaha, yeah! Atlantis, baby!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>I'm really hoping it wasn't a one-off thing an...</td>\n",
       "      <td>Got some of that mermaid puss!</td>\n",
       "      <td>That was amazing!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>Pssh! Not at all, Morty. That place will never...</td>\n",
       "      <td>I'm really hoping it wasn't a one-off thing an...</td>\n",
       "      <td>Got some of that mermaid puss!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>Whoo! Yeah! Yeaah! Ohhh, shit!</td>\n",
       "      <td>Pssh! Not at all, Morty. That place will never...</td>\n",
       "      <td>I'm really hoping it wasn't a one-off thing an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>379 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              response  \\\n",
       "0    I... think the personality conflict might have...   \n",
       "1    Jesus... How awesome is that? I mean, they wan...   \n",
       "2    Rick, since it's my adventure and all, could y...   \n",
       "3    Uh, the adventure is the favor, Morty. Me slee...   \n",
       "4    Rick, this really bums me out. It-It's embarra...   \n",
       "..                                                 ...   \n",
       "374                                  That was amazing!   \n",
       "375                     Got some of that mermaid puss!   \n",
       "376  I'm really hoping it wasn't a one-off thing an...   \n",
       "377  Pssh! Not at all, Morty. That place will never...   \n",
       "378                     Whoo! Yeah! Yeaah! Ohhh, shit!   \n",
       "\n",
       "                                               context  \\\n",
       "0    Don't worry, Morty, they love you. Superheroes...   \n",
       "1    I... think the personality conflict might have...   \n",
       "2    Jesus... How awesome is that? I mean, they wan...   \n",
       "3    Rick, since it's my adventure and all, could y...   \n",
       "4    Uh, the adventure is the favor, Morty. Me slee...   \n",
       "..                                                 ...   \n",
       "374               Whoa!! Hahaha, yeah! Atlantis, baby!   \n",
       "375                                  That was amazing!   \n",
       "376                     Got some of that mermaid puss!   \n",
       "377  I'm really hoping it wasn't a one-off thing an...   \n",
       "378  Pssh! Not at all, Morty. That place will never...   \n",
       "\n",
       "                                             context/0  \n",
       "0    This article says the reason we weren't involv...  \n",
       "1    Don't worry, Morty, they love you. Superheroes...  \n",
       "2    I... think the personality conflict might have...  \n",
       "3    Jesus... How awesome is that? I mean, they wan...  \n",
       "4    Rick, since it's my adventure and all, could y...  \n",
       "..                                                 ...  \n",
       "374               Holy crap... Slick's wish came true.  \n",
       "375               Whoa!! Hahaha, yeah! Atlantis, baby!  \n",
       "376                                  That was amazing!  \n",
       "377                     Got some of that mermaid puss!  \n",
       "378  I'm really hoping it wasn't a one-off thing an...  \n",
       "\n",
       "[379 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0bc6aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"What do you think about Elon Musk?<|endoftext|>\"] * 1  # generate a batch of 10 independent samples\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model_wrapper.device)\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "734c8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_wrapper.model.generate(\n",
    "    inputs, \n",
    "    max_length=200,\n",
    "    pad_token_id=tokenizer.eos_token_id,  \n",
    "#     no_repeat_ngram_size=3,       \n",
    "    do_sample=True, \n",
    "    top_k=100, \n",
    "    top_p=0.7,\n",
    "    temperature = 0.8\n",
    ").cpu()\n",
    "decoded_outputs = [tokenizer.decode(output) for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "43c38f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What do you think about Elon Musk?<|endoftext|>you, i see, i know. i mean, it's not like, but, uh, but, but, uh, but, uh, uh, but, uh, but, uh, but, uh, uh, uh, but, uh, uh, but, uh, uh, but, uh, uh, uh, uh, uh, uh, but, uh, uh, so, uh, uh, but, but, can you.<|endoftext|>\"]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0678dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size = 1,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "num_batches_to_show = 100\n",
    "\n",
    "responses_dict = {}\n",
    "example_id = 0\n",
    "for i, batch in enumerate(loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "032cbf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 9099,   470,  5490,   837,  5596,    88,   837,   484,  1842,   345,\n",
      "           764, 40896,   761,   257,  3094, 45320, 49733, 45543,   284,  7621,\n",
      "          1863,   290,  6324,   284,  2279,   588,   340,   338,  2000, 19280,\n",
      "           764, 50256,    72,  2644,   892,   262,  8806,  5358,  1244,   423,\n",
      "           587,  2644,   345,   764,     0]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41, 42, 43,  0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'target_ids': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,    72,  2644,   892,   262,  8806,  5358,  1244,   423,\n",
      "           587,  2644,   345,   764, 50256]]), 'attention_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"don't worry, morty, they love you. superheroes need a wide eyed unremarkable to tag along and react to everything like it's mind blowing.<|endoftext|>i... think the personality conflict might have been... you.!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batch)\n",
    "tokenizer.decode(batch['input_ids'][example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1092087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each tensor to device\n",
    "input_ids       = batch['input_ids'].to(self.device)\n",
    "# position_ids    = batch['position_ids'].to(self.device)\n",
    "# token_type_ids  = batch['token_type_ids'].to(self.device)\n",
    "# target_ids      = batch['target_ids'].to(self.device)\n",
    "attention_mask  = batch['attention_masks'].to(self.device)\n",
    "\n",
    "outputs = self.model.generate(\n",
    "    inputs = input_ids, \n",
    "    max_new_tokens = max_new_tokens,\n",
    "    pad_token_id = self.tokenizer.eos_token_id,\n",
    "    do_sample = True,  # do_sample = True; otherwise all questions will be identical\n",
    "    top_p = 0.95,      # nucleus sampling\n",
    "    top_k = 0,         # deactivate top-k words sampling\n",
    "    attention_mask = attention_mask  # do not pay attention to padding\n",
    ").cpu()\n",
    "\n",
    "decoded_outputs = [self.tokenizer.decode(output) for output in outputs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_deep_learning",
   "language": "python",
   "name": "advanced_deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
